

# Papers for 2025-09-30

## 0.[LongLive: Real-time Interactive Long Video Generation](https://arxiv.org/pdf/2509.22622)
summary:**核心关键词**： **长视频生成**，**实时交互**，**自回归框架**，**KV-recache**，**流式长视频微调**  **一句话核心摘要**： 该论文提出了一个名为LONGLIVE的帧级自回归框架，通过集成KV-recache机制、流式长视频微调策略以及结合帧级注意力池的短窗口注意力，实现了高效、高质量且支持实时交互的长视频生成。  **主要研究问题或目标**： 该研究旨在解决长视频生成领域中现有模型的双重挑战：一方面，扩散模型因双向注意力机制导致推理效率低下，难以实现实时生成；另一方面，传统的因果自回归模型虽然支持快速推理，但在长视频上容易出现质量下降，并且在处理流式（实时）的用户指令切换时，难以保证视觉连贯性和语义一致性。  **关键方法论**： LONGLIVE采用了一个因果、帧级的自回归设计，其核心技术包含三个部分：1) **KV-recache机制**：在用户切换指令时，该机制利用已生成的视频帧和新指令重新计算并刷新键值缓存（KV cache），从而在保持视觉平滑过渡的同时，确保内容能快速遵循新指令。2) **流式长视频微调 (Streaming Long Tuning)**：该策略通过在训练中模拟长视频的滚动生成过程，使模型能够在长序列上进行训练，解决了传统短视频训练模型在长视频推理时性能下降的问题，对齐了训练与推理过程。3) **短窗口注意力与帧级注意力池 (Short Window Attention with Frame-level Attention Sink)**：为提升推理效率，模型采用局部窗口注意力，并永久缓存初始视频帧的token作为“注意力池”，使其在后续生成中始终可被全局访问，从而在加速计算的同时保持了长距离依赖和一致性。  **主要成果**： 实验结果表明，LONGLIVE在效率和质量上均表现出色。最核心的量化成果是，该模型在单张NVIDIA H100 GPU上实现了20.7 FPS的实时推理速度，能够生成长达240秒的视频。与基线模型相比，其生成速度提升了41倍以上（对比SkyReels-V2），并且在VBench短视频和长视频基准测试中均取得了领先的性能分数。  **对AI从业者的主要启示**： 对于AI工程师和研究者而言，该研究提供了一套完整且高效的工程实践方案，用于构建实时交互式长视频生成应用。其价值在于证明了通过结合特定的缓存管理（KV-recache）、训练策略（流式微调）和高效注意力机制（窗口注意力+注意力池），可以在现有短视频模型的基础上，以较低的算力成本（32个GPU日）高效微调出兼具高质量、长时一致性和实时交互能力的长视频模型，为相关应用的商业化落地提供了重要的技术路径。

## 1.[EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning](https://arxiv.org/pdf/2509.22576)
summary:**多轮稀疏奖励**，**探索-利用级联失效**，**熵正则化策略优化(EPO)**，**熵平滑正则器** 该论文针对在多轮稀疏奖励环境中训练大语言模型智能体时出现的“探索-利用级联失效”问题，提出了一种名为熵正则化策略优化（EPO）的通用框架，该框架通过熵平滑和自适应加权机制来稳定训练，从而显著提升了智能体的性能。 该研究旨在解决大语言模型智能体在需要长序列交互（如30+轮次）且只有最终才有奖励的稀疏奖励环境中所特有的训练失败模式，即“探索-利用级联失效”，此失效表现为早期策略过早收敛至低熵次优策略，随后因传统熵正则化失效而进入后期策略崩溃。 其核心方法是熵正则化策略优化（EPO），一个包含三个协同机制的框架：1）采用适应多轮次设定的熵正则化以增强探索；2）引入一个熵平滑正则器，通过将策略熵约束在历史平均水平附近来防止剧烈波动；3）设计一种自适应的、分阶段的加权方案，以在整个训练过程中动态平衡探索与利用。 实验结果表明，EPO框架在ScienceWorld基准测试中取得了高达152%的性能提升，在ALFWorld基准上提升了19.8%，成功地将先前难以训练的稀疏奖励场景转化为了能够平滑收敛的优化问题。 对于AI从业者而言，该研究最重要的启示是，在为多轮稀疏奖励环境设计智能体时，传统的强化学习熵控制方法可能适得其反；论文提出的EPO框架通过时间维度上的熵平滑提供了一种更稳定的探索控制机制，为开发能够在复杂、长时程任务中稳定学习的LLM智能体提供了一种有效的、可直接应用的优化方案。

## 2.[Quantile Advantage Estimation for Entropy-Safe Reasoning](https://arxiv.org/pdf/2509.22611)
summary:**核心关键词**： **分位数优势估计**，**熵安全**，**基线设计**，**可验证奖励强化学习 (RLVR)**  **1-Sentence Core Summary**： 该研究针对可验证奖励强化学习（RLVR）中的熵崩溃与熵爆炸问题，提出了一种分位数优势估计（QAE）方法，通过用分组K分位数基线替代均值基线，实现了双向熵安全，从而稳定了训练过程并提升了模型推理性能。  **Main Research Question or Objective**： 该研究旨在解决可验证奖励强化学习（RLVR）中因基线设计不当而导致的训练不稳定性问题，即策略熵在“熵崩溃”和“熵爆炸”之间振荡，从而限制了模型性能的进一步提升。  **Key Methodology**： 核心方法是分位数优势估计（QAE），它将无价值函数强化学习（如GRPO和DAPO）中传统的均值基线替换为分组K分位数基线。该方法根据问题难度（成功率p）动态形成一个双模态门控机制：对于困难问题（p ≤ 1-K），基线为0，仅奖励稀有成功样本以防止熵崩溃；对于简单问题（p > 1-K），基线为1，仅惩罚剩余失败样本以抑制熵爆炸。  **Primary Results**： 实验表明，QAE有效稳定了策略熵，并实现了信用分配的稀疏化，在调整后的K值下，约80%的响应获得零优势值，从而将更新集中于信息量最大的样本。在AIME'24基准测试上，该方法使Qwen3-8B-Base模型的pass@1得分相较于基线方法提升了21.5%（从39.69%到48.23%），获得了持续的性能增益。  **Principal Implication for AI Practitioners**： 对于AI从业者而言，该研究最重要的启示是，在扩展RLVR时，核心瓶颈在于优势估计中的基线设计，而非token级别的启发式方法。通过简单地将均值基线替换为分位数基线这一微小改动，工程师即可显著稳定训练动态、提升模型性能，这为优化大模型推理能力提供了一个直接且高效的切入点。

## 3.[MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing](https://arxiv.org/pdf/2509.22186)
summary:核心关键词：**文档解析**，**视觉语言模型**，**由粗到细策略**，**解耦解析**  一句话核心总结：论文介绍了一个12亿参数的文档解析视觉语言模型MinerU2.5，它通过一种创新的由粗到细的两阶段解耦策略，在保持高计算效率的同时实现了业界领先的识别精度。  主要研究问题或目标：本研究旨在解决高分辨率文档解析任务中，识别精度与计算效率难以兼顾的核心问题，特别是如何避免因处理高分辨率输入而产生的巨大计算开销。  关键方法论：模型采用一种解耦的、由粗到细的两阶段解析策略。第一阶段，在降采样的低分辨率图像上进行全局版面分析，以低成本识别文档结构元素。第二阶段，根据第一阶段的版面分析结果，在原始高分辨率图像上进行靶向内容识别，通过处理原生分辨率的局部裁切块来保留文本、公式和表格中的精细细节。  主要成果：实验结果表明，MinerU2.5在多个基准测试中取得了SOTA性能。具体而言，该模型在OmniDocBench基准测试上取得了90.67分的综合得分，显著优于现有的通用及领域专用模型，同时计算开销大幅降低。  对AI从业者的主要启示：该研究为AI从业者提供了一种在处理高分辨率视觉任务时平衡性能与成本的有效架构。其两阶段解耦策略证明了可以在不牺牲关键细节识别精度的前提下，通过分阶段处理大幅提升计算效率，为开发大规模、高效率的文档自动化处理系统提供了重要的实践范例。

## 4.[ReviewScore: Misinformed Peer Review Detection with Large Language Models](https://arxiv.org/pdf/2509.21679)
summary:**核心关键词**：同行评审质量, 误导性评审检测, 大语言模型, 论证重构  **一句话核心概述**：本研究针对AI学术会议中普遍存在的低质量同行评审问题，提出了一种名为REVIEWSCORE的自动化评估框架，该框架利用大语言模型进行论证重构与事实性检验，以可靠地检测出评审意见中包含错误前提或已解答问题的误导性信息。  **主要研究问题或目标**：该研究旨在解决AI学术会议同行评审质量下降的核心问题，即如何创建一个具体、适用性广且可自动化执行的方法，来可靠地识别和量化评审中的“误导性评审点”（misinformed review points），从而有效过滤低质量的评审意见。  **关键方法**：研究的核心方法论是首先将低质量评审具体化为两类“误导性评审点”：包含不正确前提的“弱点”和论文中已有答案的“问题”。为评估“弱点”的真实性，研究团队设计了一个自动论证重构引擎，该引擎利用大语言模型将评审意见分解为一组显式和隐式的前提，然后逐一评估每个前提的事实性。为验证该方法的有效性，研究构建了一个由人类专家标注的REVIEWSCORE数据集，并使用八个先进的LLM来测试其自动化评估能力。  **主要成果**：研究发现，在AI会议的同行评审中，15.2%的“弱点”意见和26.4%的“问题”意见是误导性的。实验证明，LLM自动化评估与人类专家标注之间存在中等程度的一致性。一个关键的量化发现是，评估每个前提的事实性（premise-level）比直接评估整个弱点（weakness-level）的人机一致性要高得多，其中先进的REVIEWSCORE方法在F1分数上比基础方法提升了2.48倍。  **对AI从业者的主要启示**：此研究为参与学术流程的AI工程师和研究人员提供了一个实用的自动化工具。该工具可用于在评审流程中对评审质量进行初步的自动化筛选，帮助作者识别评审中的事实性错误以撰写更有力的反驳，同时也能辅助审稿人进行自查或为领域主席提供决策依据。它将主观的评审质量评估问题，部分转化为可量化的、基于事实核查的任务，有潜力提升大型AI会议评审流程的效率与公正性。

## 5.[Variational Reasoning for Language Models](https://arxiv.org/pdf/2509.22637)
summary:**变分推理，潜变量，证据下界 (ELBO)，前向KL散度** 该研究提出了一种用于语言模型的变分推理框架，该框架将思维轨迹视为潜变量并通过变分推理进行优化，从而提供了一个统一变分推理与强化学习方法的原则性概率视角，并为提升语言模型推理能力产生了更稳定的目标。 该研究旨在解决现有语言模型推理训练方法（如监督微调和强化学习）所面临的训练不稳定、泛化能力差和存在偏见等问题，目标是构建一个更具原则性且稳定的训练框架来提升模型的推理能力。 该框架将模型的思维轨迹（thinking traces）视为潜变量，通过变分推理优化生成正确答案的证据下界（ELBO），并将其扩展为一个多轨迹目标以获得更紧的界，同时提出一种前向KL散度公式来稳定变分后验分布的训练。 实验证明，该方法在多个模型和推理任务上均取得了显著优于强基线的性能，例如，在Qwen3-8B-Base模型上，该方法在数学等任务上相比基础模型实现了超过160%的性能提升。 该研究为AI工程师提供了一个更稳定和原则性的训练流程来开发强推理能力的语言模型，其不仅统一了不同的训练范式，还揭示了现有方法中存在的“偏爱简单问题”的固有偏见，这对于设计更鲁棒的训练策略和评估模型真实能力至关重要。

## 6.[Language Models Can Learn from Verbal Feedback Without Scalar Rewards](https://arxiv.org/pdf/2509.22638)
summary:**语言模型**，**言语反馈**，**标量奖励**，**反馈条件策略 (FCP)**  该研究提出一种名为反馈条件策略 (Feedback-Conditional Policy, FCP) 的新方法，通过将言语反馈作为条件信号，使大型语言模型能够直接从包含丰富信息的言语反馈中学习，从而摒弃了传统强化学习中将反馈压缩为标量奖励的范式。  该研究旨在解决现有基于人类或AI反馈的强化学习 (RLHF/RLAIF) 方法的核心局限性：将细致的言语反馈压缩为单一标量奖励会导致严重的信息损失、反馈歧义性以及跨任务奖励尺度不平衡等问题。  该研究的核心方法是反馈条件策略 (FCP)，它将反馈驱动的学习重构为条件生成任务。该方法包含两个阶段：1) **离线训练阶段**：利用已有的“响应-反馈”数据对，通过最大似然估计直接学习一个反馈条件策略 `πθ(o|x, c)`，该策略能够根据给定的反馈 `c` 和指令 `x` 生成响应 `o`。2) **在线自举 (Bootstrapping) 阶段**：模型在给定正面反馈条件 `c+` 的情况下生成新的响应，然后由环境（如人类或AI）对新响应提供新的言语反馈，形成新的训练数据，从而迭代式地优化和增强模型性能。  实验结果表明，该方法在不依赖标量奖励的情况下，性能可与强大的基线方法相媲美。在数学推理基准测试中，经过在线自举优化的FCP模型取得了38.7%的平均准确率，略微超过了基于标量奖励的强基线方法GRPO（38.4%）。  对于AI从业者而言，这项研究提供了一种更直接、高效且可扩展的语言模型对齐范式。开发者可以直接利用原始、多样化的言语反馈（包括混合或不确定的反馈）进行模型训练，无需设计复杂的奖励函数或进行标量转换，这不仅保留了反馈中的丰富信息，还降低了奖励 hacking 的风险和跨任务微调的复杂性，为构建更具表达力和鲁棒性的对齐模型提供了新途径。

## 7.[CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning](https://arxiv.org/pdf/2509.22647)
summary:**核心关键词**：图像字幕，强化学习，可验证奖励，解耦式视觉问答  **一句话核心摘要**：该研究提出名为CapRL的新型训练框架，通过应用可验证奖励强化学习（RLVR）范式，将图像字幕的质量定义为其在多大程度上能帮助一个无视觉能力的语言模型准确回答相关问题，以此克服传统SFT方法的局限性并生成更通用、准确的图像描述。  **主要研究问题或目标**：本研究旨在解决基于监督微调（SFT）的图像字幕模型存在的依赖昂贵标注数据、泛化能力差的问题，并攻克为图像字幕这一主观性开放任务设计客观、可扩展的强化学习奖励函数的挑战。  **关键方法论**：该研究引入了一个名为CapRL的解耦式两阶段训练框架：第一阶段，一个大型视觉语言模型（LVLM）为输入图像生成候选字幕；第二阶段，一个独立的、无视觉能力的语言模型仅基于生成的字幕来回答关于图像的多项选择题（MCQs），其回答的准确率被用作客观、可验证的奖励信号，通过策略梯度优化来训练字幕生成模型。  **主要成果**：实验证明，CapRL框架能有效提升模型性能，在Prism字幕质量评估框架中，使用CapRL训练的模型性能不仅媲美了规模更大的Qwen2.5-VL-72B模型，并且相较于基线模型平均超出了8.4%；此外，使用CapRL-3B模型标注的CapRL-5M数据集进行预训练，可以在12个基准测试中带来显著的性能增益。  **对AI从业者的主要启示**：对于AI工程师和数据科学家而言，该研究提供了一种创新且可扩展的范式，用于为图像字幕等主观性任务创建高质量的训练数据和奖励信号，而无需昂贵的人工标注；该方法通过将主观评价转化为客观效用度量，为提升多模态模型的预训练质量和泛化能力提供了一条有效路径，尤其是在数据稀疏或标注成本高的场景中具有重要应用价值。

## 8.[No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping](https://arxiv.org/pdf/2509.21880)
summary:**核心关键词**：零方差提示，强化学习，优势塑造，大语言模型，熵引导  **一句话核心概要**：该研究提出了一种名为RL-ZVP的新算法，通过熵引导的优势塑造方法，从先前被忽略的零方差提示中提取有效的学习信号，从而提升大型语言模型在可验证奖励强化学习（RLVR）框架下的推理能力。  **主要研究问题或目标**：该研究旨在解决现有RLVR方法（如GRPO）无法处理零方差提示（即所有模型响应获得相同奖励的提示）的问题，这些提示因导致优势函数坍塌为零而被丢弃，造成了计算资源的浪费和学习信号的损失。  **关键方法论**：研究提出RL with Zero-Variance Prompts (RL-ZVP)算法。对于非零方差提示，该算法与GRPO相同；对于零方差提示，它设计了一种新的熵引导优势函数：1）优势方向：若所有响应均正确，则为正向奖励；若均错误，则为负向惩罚。2）优势幅度：利用每个token的熵进行调节。对于正确响应，高熵（信息量大）的token获得更高奖励；对于错误响应，高熵token受到的惩罚较小，以保留模型的探索潜力，其优势函数具体形式为`AZVP(oi,t)`，在正确时与熵`Hi,t`成正比，在错误时与`(max(Hi,k) - Hi,t)`成反比。  **主要成果**：实验结果表明，在六个数学推理基准测试中，RL-ZVP相较于基线方法GRPO表现出显著优势。在AIME25基准上，RL-ZVP在准确率（Acc@8）上取得了高达8.61个百分点的提升，在OlympiadBench基准上，其通过率（Pass@8）提升了7.77个百分点，并且稳定优于其他过滤零方差提示的基线方法。  **对AI从业者的主要启示**：这项工作为AI工程师和研究者提供了一个新的视角：先前在RL训练中被视为无用并被丢弃的零方差提示，实际上是宝贵的学习信号来源。通过采用RL-ZVP中的熵引导优势塑造策略，从业者可以更充分地利用所有采样数据，从而在不增加额外数据或过滤成本的情况下，提升LLM推理任务的训练效率、稳定性和最终性能。

## 9.[PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning](https://arxiv.org/pdf/2509.19894)
summary:**提示合成, 推理理据, 期望最大化循环, 自博弈** 该研究提出了一种名为PromptCoT 2.0的可扩展框架，其通过期望最大化（EM）循环替代手工启发式方法，迭代优化推理理据来指导提示构建，从而生成更高难度和多样性的训练问题，以提升大语言模型的推理能力。 本研究旨在解决大语言模型在高级推理任务中面临的高质量训练数据稀缺问题，即如何以可扩展、自动化的方式生成比现有合成语料库更难、更多样化的训练问题。 该框架将概念-理据-提示的生成过程建模为一个EM优化问题：在E步（期望步），通过强化学习更新理据生成模型，使其产出能引导生成更有效提示的理据；在M步（最大化步），利用E步生成的更优理据，监督式地更新提示生成模型。 实验证明，该方法生成的提示显著提升了模型性能；在监督微调（SFT）设置下，仅使用合成提示训练的Qwen2.5-7B模型在AIME 24测试集上准确率达到73.1%，超越了使用人类标注或混合数据训练的模型。 对于AI工程师和研究者而言，这项工作提供了一种可扩展且自动化的方法来生成高质量、高难度的推理训练数据，从而有效缓解对昂贵的人工标注数据的依赖，并证明了提示合成是与模型规模扩展并行的、提升大模型复杂推理能力的有效途径。

## 10.[MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning](https://arxiv.org/pdf/2509.22281)
summary:**核心关键词**: **任务驱动场景生成**, **3D空间推理**, **大语言模型 (LLM)**, **MesaTask-10K数据集**, **直接偏好优化 (DPO)**  **一句话核心总结**: 该研究提出了一种基于大语言模型 (LLM) 的MesaTask框架，通过一种新颖的“空间推理链”和直接偏好优化 (DPO) 算法，将高层级的自然语言任务指令转化为物理上合理且与任务高度对齐的3D桌面场景。  **主要研究问题或目标**: 本文旨在解决机器人领域中，如何根据高层级的人类指令自动生成用于训练的、既真实又与任务相关的3D桌面场景，以克服传统手动设计或随机布局方法在多样性和任务对齐性上的局限。  **关键方法论**: 核心方法是MesaTask框架，它利用一个名为“空间推理链”的三阶段过程（物体推断、空间关系推理、场景图构建）来分解生成任务；该框架首先通过监督微调 (SFT) 在其自建的大规模MesaTask-10K数据集上训练一个LLM，赋予其3D空间推理能力，随后利用直接偏好优化 (DPO) 算法，通过对比正负样本进一步提升生成场景的物理真实性和任务一致性。  **主要成果**: 实验结果表明，MesaTask在各项评估指标上均优于基线方法；具体而言，其生成场景的Fréchet Inception Distance (FID) 达到了**40.3**，显著优于基线模型GPT-40的74.4，并且在综合评估指标GPT-Score上获得了**8.25**的平均分。  **对AI从业者的主要启示**: 这项工作为AI工程师（尤其是在机器人和具身智能领域）提供了一个从自然语言指令自动生成高质量、多样化仿真训练环境的完整解决方案，极大地降低了创建机器人策略训练数据的人工成本和时间，其所提出的空间推理链也为将LLM应用于复杂3D空间理解任务提供了有效范式。

## 11.[UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios](https://arxiv.org/pdf/2509.21766)
summary:**核心关键词**：长时序任务，自主智能体，基准测试，部分可观测环境，上下文锁定  **一句话核心摘要**：该研究提出了一个名为UltraHorizon的新型基准，通过在需要持续推理、规划和工具使用的超长时序、部分可观测探索任务中进行评估，揭示了现有大型语言模型智能体相较于人类在处理复杂真实世界挑战方面的显著能力差距。  **主要研究问题或目标**：现有智能体基准大多集中于短时序、完全可观测的任务，无法有效评估智能体在处理如大规模软件开发等需要持续推理、规划、记忆管理和工具使用的长时序、部分可观测真实世界场景中的核心能力，本研究旨在填补这一评估空白。  **关键方法论**：研究者设计了UltraHorizon基准，其包含三个不同但统一于“探索”任务的环境，要求智能体通过与环境的迭代交互来发现隐藏规则；该基准的特点在于其“超长时序”规模，在最重度设置下，智能体轨迹平均超过20万tokens和400次工具调用，在标准配置下也超过3.5万tokens和60次工具调用。  **主要研究结果**：实验表明，所有顶尖的大型语言模型智能体在UltraHorizon上的表现均显著低于人类参与者，例如人类平均得分为26.52，而表现最好的模型仅为14.33；研究还发现，简单增加交互步数的扩展策略（simple scaling）并不能有效提升智能体性能，并通过轨迹分析识别出“上下文锁定”（in-context locking）和基础能力缺陷是导致失败的两个主要原因。  **对AI从业者的主要启示**：研究结果向AI工程师和研究者揭示，当前简单扩展模型或增加交互次数的方法不足以解决长时序任务的挑战；智能体在早期交互中形成的错误假设会固化（即“上下文锁定”），导致后续决策失败，因此未来的研发重点需转向改进智能体的核心能力，如设计更鲁棒的记忆机制、自适应推理框架和探索策略，以应对复杂动态环境。

## 12.[LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training](https://arxiv.org/pdf/2509.23661)
summary:**核心关键词**：大型多模态模型, 高效训练框架, 开源, 数据打包  **一句话核心摘要**：本文提出了LLaVA-OneVision-1.5，一个新型大型多模态模型家族，它利用大规模精选数据集和一个采用离线并行数据打包策略的高效训练框架，以显著降低的计算和财务成本实现了最先进的性能，并提供了一个完全开源、可复现的多模态模型构建方案。  **主要研究问题或目标**：该研究旨在解决当前高性能大型多模态模型（LMMs）训练成本高昂、过程不透明且难以复现的问题，目标是创建一个完全开源、高效且经济的端到端框架，使社区能够以低成本从零开始构建高质量的视觉语言模型。  **关键方法论**：该框架的核心由三部分构成：1) **大规模精选数据集**：构建了一个8500万样本的概念平衡预训练数据集和一个2200万样本的精选指令微调数据集；2) **高效训练框架**：开发了一个端到端训练流程，其核心技术是采用离线并行数据打包策略，将多个短样本序列合并成一个长序列，以大幅减少计算中的填充（padding），显著提高GPU利用率和训练效率，从而将整体训练成本控制在16000美元以内；3) **三阶段训练流程**：模型训练遵循语言-图像对齐、高质量知识学习和视觉指令微调三个阶段。  **主要成果**：实验结果表明，LLaVA-OneVision-1.5模型系列在广泛的下游任务中表现出卓越的性能。具体量化结果显示，LLaVA-OneVision-1.5-8B模型在27个基准测试中的18个上超越了Qwen2.5-VL-7B模型，而LLaVA-OneVision-1.5-4B模型则在全部27个基准测试中均优于Qwen2.5-VL-3B。  **对AI从业者的主要启示**：该研究为AI工程师和数据科学家提供了一套完整、开源且经过验证的低成本（预算低于16000美元）高性能大型多模态模型构建蓝图。从业者可以直接利用其开源的数据集、代码和高效训练策略，快速复现、微调或从零开始构建针对特定业务需求的视觉语言模型，这极大地降低了先进多模态AI技术研发与应用的技术和资金门槛。

## 13.[See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation](https://arxiv.org/pdf/2509.22653)
summary:**核心关键词**: 视觉语言模型, 免训练, 无人机导航, 空间定位  **一句话核心总结**: 本文提出了一种名为SPF的免训练空中视觉语言导航框架，其核心思想是将无人机动作预测视为一个2D空间定位任务，利用视觉语言模型（VLM）将模糊的语言指令分解为图像上的2D航点标注，从而实现对任意环境和指令的普适性导航。  **主要研究问题或目标**: 该研究旨在解决现有无人机视觉语言导航方法在面对新环境和自由格式指令时泛化能力差、依赖特定训练数据的问题，并克服将动作预测视为文本生成任务所带来的不精确性。  **关键方法论**: 关键方法是SPF框架，它将动作预测重新定义为2D空间定位任务。该框架利用一个冻结的VLM，根据当前视觉观测和自然语言指令，直接在输入图像上标注出2D航点。随后，结合预测的行进距离，通过相机针孔模型将2D航点转换为3D位移向量作为无人机的动作指令，并通过闭环控制实现连续 replanning 和对动态目标的跟踪。  **主要成果**: 实验结果表明，SPF框架在DRL模拟基准测试中取得了当前最优性能（SOTA），其成功率比之前最好的方法绝对高出63%。在模拟和真实世界评估中，SPF分别取得了93.9%和92.7%的平均成功率，显著优于基线模型。  **对AI从业者的主要启示**: 该研究为AI从业者提供了一个将大型视觉语言模型应用于机器人控制的免训练新范式。其核心启示是，将动作生成问题从传统的文本输出模式转化为直接的空间定位模式（在图像上“指点”），能更有效地利用VLM的内在空间推理能力，从而无需任务特定的数据收集和模型微调即可实现精确、鲁棒的物理世界导航与控制，这一思路可推广至其他机器人应用领域。

## 14.[COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning](https://arxiv.org/pdf/2509.22075)
summary:**核心关键词** 大语言模型压缩, 稀疏字典学习, 校准引导, 结构化稀疏  **一句话核心总结** 本文提出了一种名为CoSpaDi的免训练大语言模型压缩框架，该框架用校准引导的稀疏字典学习取代传统的低秩分解，通过更灵活的子空间联合表示来最小化功能重构误差，从而在不微调的情况下保持更高的模型保真度。  **主要研究问题或目标** 该研究旨在解决传统低秩近似方法在大型语言模型训练后压缩中所面临的刚性结构约束导致模型精度显著下降的问题，其目标是开发一种更灵活且无需训练的压缩技术，以更好地保持模型性能。  **关键方法** 核心方法CoSpaDi将每个权重矩阵分解为一个稠密字典（D）和一个列稀疏系数矩阵（S），从而实现了子空间联合表示，允许权重矩阵的不同列在不同字典原子张成的相异子空间中进行近似。关键在于，该方法利用少量校准数据集来指导分解优化，其目标是最小化压缩后层输出激活值的重构误差（即功能误差），而非仅仅是权重近似误差。该优化过程采用基于K-SVD的迭代算法，在稀疏编码和字典更新步骤之间交替进行。  **主要成果** 实验结果表明，在20%-50%的压缩率下，CoSpaDi在多个Llama和Qwen模型上的准确率和困惑度均持续优于当前先进的数据感知低秩方法。例如，在Llama3.2-1B模型上，当压缩率为20%时，CoSpaDi的平均准确率达到0.4267，显著高于SOTA低秩方法SVD-LLM的0.3762。  **对AI从业者的主要启示** 对于AI技术人员，该研究证实了结构化稀疏字典学习是比传统低秩分解更强大的LLM压缩范式，为在资源受限环境中部署大模型提供了一个高效的免训练方案。从业者可以利用CoSpaDi在不进行微调的情况下实现更高的压缩率和更小的性能损失，其生成的结构化稀疏格式也为利用稀疏计算硬件加速推理提供了可能。

## 15.[VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing](https://arxiv.org/pdf/2509.22651)
summary:**核心关键词**：语音助手, 基准测试, 多模态评估, 听-说-看能力  **一句话核心摘要**：该研究推出了一个名为VoiceAssistant-Eval的综合基准，通过横跨听、说、看三个维度的10,497个样本，系统性地评估AI语音助手在多模态交互中的综合能力，从而揭示了当前模型普遍存在音频理解能力滞后于语音生成能力的问题，并为下一代助手的开发提供了严谨的评估框架。  **主要研究问题或目标**：旨在解决现有基准测试在全面评估先进语音优先（voice-first）AI助手横跨听觉、口语和视觉等多维度综合能力方面的不足。  **关键方法**：构建了一个名为VoiceAssistant-Eval的综合性评估基准，其包含10,497个横跨13个任务类别的精选样本。该基准全面覆盖了“听”（自然声音、音乐、语音对话）、“说”（多轮对话、角色扮演模仿）和“看”（高度异构图像）三大核心能力。研究团队基于此基准对22个模型进行了评估，衡量维度包括响应内容质量、生成语音质量以及两者之间的一致性。  **主要结果**：实验揭示了三个关键发现：(1) 多数模型在口语表达任务上表现出色，但在音频理解方面显著落后；(2) 设计精良的小型模型可以在特定任务上媲美甚至超越大型模型，其中一个显著的量化结果是，中等规模的Step-Audio-2-mini (7B)模型在听力任务上的准确率达到了LLaMA-Omni2-32B-Bilingual模型的两倍以上；(3) 音频与视觉结合的多模态输入任务对当前模型仍是巨大挑战。  **对AI从业者的主要启示**：此研究为AI工程师和数据科学家提供了一个可立即使用的、用于诊断和迭代语音AI助手的严谨框架和数据集。它揭示了当前技术的核心瓶颈在于音频理解而非语音生成，这提示从业者应将更多资源投入到提升模型的音频感知与推理能力上。此外，研究证明了模型规模并非唯一决定因素，精巧的架构设计和针对性训练同样关键，这为开发高效能、低成本的AI助手提供了重要的实践指导。

## 16.[LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale Diffusion Transformer](https://arxiv.org/pdf/2509.22414)
summary:**通用图像修复, 无标题修复, 扩散Transformer, 双分支条件器** 该研究提出了一种名为LucidFlux的无标题通用图像修复框架，它通过轻量级双分支条件器和基于SigLIP的语义对齐来适配大规模扩散Transformer，从而在无需文本提示的情况下实现了对混合降质图像的鲁棒、高质量恢复。 本研究旨在解决通用图像修复（UIR）中的一个核心挑战：如何在处理未知混合降质的真实世界图像时，既能有效恢复图像细节，又能保持全局结构和语义一致性，同时避免传统方法常见的过平滑、幻觉或语义漂移问题，并摆脱对文本标题的依赖。 该框架的核心方法论建立在一个冻结的大规模扩散Transformer（Flux.1）之上，其关键创新包括：1）设计了一个轻量级双分支条件器，分别处理原始低质量输入（LQ）以锚定几何结构，以及一个轻度恢复的代理图像（LRP）以抑制伪影；2）引入了时间步和层级自适应的调制机制，将双分支信号在不同阶段和深度进行有效融合；3）为避免文本提示带来的延迟和不稳定性，通过一个冻结的SigLIP编码器从代理图像中直接提取语义特征，实现了无标题的语义对齐。 实验结果表明，LucidFlux在多个合成及真实世界基准测试中均优于现有的开源和商业模型；具体而言，在RealLQ250真实世界数据集上，LucidFlux的MUSIQ图像质量评估得分达到73.01，显著高于MeiTu SR（66.59）和DreamClear（67.08）等强劲的商业及开源基线模型，证明了其卓越的性能。 对于AI工程师和研究人员而言，该研究最重要的启示是，在将大规模预训练生成模型应用于下游任务时，关键成功因素并非增加可训练参数或依赖外部文本提示，而是设计精巧的条件注入机制；LucidFlux证明了通过智能地设计“何时、何处、以何种信息”进行条件控制，可以在不依赖文本、保持模型主干冻结的情况下，高效地引导模型完成复杂的修复任务，为相关应用开发提供了新的设计范式。

## 17.[Fine-tuning Done Right in Model Editing](https://arxiv.org/pdf/2509.22072)
summary:**核心关键词**：模型编辑, 微调, 广度优先流水线, 局部化调优  **一句话核心摘要**：本研究通过将模型编辑中被误用的深度优先微调流水线恢复为标准的广度优先小批量优化，并结合系统化的局部参数定位策略，提出了一种名为LocFT-BF的简单高效方法，显著提升了微调在模型编辑任务上的性能，并首次将其扩展至支持10万次编辑和72B参数的大模型。  **主要研究问题或目标**：该研究旨在解决一个核心矛盾：作为模型适应的基础方法，微调在模型编辑领域长期被视为一种因灾难性遗忘而表现不佳的弱基线。论文的核心目标是探究这种失败是源于微调方法的内在局限性，还是由于现有研究中不恰当的实现方式，并在此基础上重塑微调作为一种有竞争力的模型编辑技术。  **关键方法论**：论文提出的核心方法LocFT-BF建立在对现有微调实现方式的修正之上。首先，它将模型编辑中普遍采用的逐样本优化至收敛的“深度优先”（Depth-First, DF）单遍流水线，替换为标准的、基于epoch迭代整个数据集的“广度优先”（Breadth-First, BF）流水线。其次，它用小批量梯度聚合（mini-batch optimization）取代了高方差的单样本更新。最后，通过系统性实验分析不同层和模块（如MLP、Attention）的调优效果，确定了在编辑成功率、泛化性和模型通用能力之间实现最佳权衡的局部化参数调优位置。  **主要结果**：实验证明，仅将微调流水线从深度优先切换为广度优先，就能大幅提升编辑效果。所提出的LocFT-BF方法在多个LLM和数据集上显著优于现有SOTA方法，其编辑成功率平均比次优基线高出**33.72%**。值得注意的是，LocFT-BF是首个被验证能够有效支持多达**10万次顺序编辑**和**72B参数模型**的方法，这一规模比以往的实践高出一个数量级，同时保持了模型的通用能力。  **对AI从业者的主要启示**：这项研究最重要的启示是，一个被广泛忽视的基础技术（微调），只要被正确地实现，就能够成为模型编辑领域最强大、最可扩展的工具之一。对于AI工程师和研究者而言，这意味着无需依赖复杂的专门算法（如元学习或定位-编辑），只需采用标准的广度优先、小批量微调框架，并对模型参数更新的位置进行审慎选择，即可高效、稳健地对大语言模型进行知识更新。这不仅大幅降低了模型编辑的技术门槛和计算开销，也为该技术在真实世界场景的部署提供了更坚实可靠的基础。

## 18.[Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation](https://arxiv.org/pdf/2509.21989)
summary:**主体驱动生成**，**视觉一致性**，**特征解耦**，**视觉对应**  该研究提出了一种新方法，通过自动化数据管道和对比学习架构从预训练扩散模型中解耦视觉与语义特征，并基于此设计了名为视觉语义匹配(VSM)的新度量，以有效量化和定位主体驱动图像生成中的视觉不一致性。  本研究旨在解决主体驱动图像生成领域缺乏可靠的视觉一致性评估方法的问题，特别是开发一种能够同时量化不一致性程度并精确定位不一致区域的新型度量。  该方法的核心分为三部分：首先，建立一个自动化数据生成流程，利用现有主体驱动生成数据集，通过语义对应计算、SAM分割和局部重绘（inpainting）技术，自动构建包含已知、局部视觉不一致性的图像对；其次，设计一个包含语义和视觉两个分支的对比学习架构，从冻结的扩散模型主干中提取特征，通过专门设计的损失函数，使语义分支对齐所有语义对应点，而视觉分支仅对齐外观一致的区域，从而实现视觉与语义特征的解耦；最后，基于解耦后的特征提出视觉语义匹配（VSM）度量，该度量首先识别语义上匹配的区域，然后计算这些区域中视觉特征也匹配的比例。  实验结果表明，所提出的VSM度量在评估视觉一致性方面显著优于现有方法。在一个包含受控不一致性的测试集上，VSM与基准真相（oracle）的皮尔逊相关系数达到了**0.448**，而CLIP、DINO和VLM等基准方法的相关系数分别为-0.053、0.087和0.072，显示出巨大的优势。在对真实世界的主体驱动生成模型的评估中，VSM同样表现出与人工标注结果更高的一致性，并能生成热力图以直观地定位不一致区域。  本研究为从事图像生成，特别是主体驱动生成领域的AI从业者提供了一个关键的评估工具。VSM度量不仅能提供一个比CLIP或DINO更准确的全局一致性分数，还能精确定位模型生成图像中与参考主体不一致的具体区域。这一能力使得开发者可以进行更可靠的模型性能比较、有针对性地调试模型缺陷（例如，为何某个特定区域总是生成错误），并为开发能够自动修复生成缺陷的新算法提供了基础。

## 19.[WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning](https://arxiv.org/pdf/2509.22644)
summary:**网站生成代理, 多层级反馈, 步骤级强化学习, 视觉语言模型** 该研究提出了一种名为WebGen-Agent的新型网站生成代理，其通过利用视觉语言模型（VLM）提供的多层级视觉反馈进行迭代式代码优化，并引入步骤级强化学习方法（Step-GRPO）进行训练，从而显著提升了生成网站的功能准确性和视觉效果。 本研究旨在解决当前代码代理在网站生成任务中仅依赖简单代码执行反馈，无法有效评估和优化网站视觉效果与交互功能质量的问题。 核心方法包含两部分：首先，WebGen-Agent工作流在每个迭代步骤中，利用VLM评估网站截图的视觉质量并部署GUI代理测试其功能，生成包含描述、建议和量化分数的多层级反馈，这些分数被整合到一个回溯和最优选择机制中以指导代码优化。其次，研究引入了“带截图与GUI代理反馈的步骤级GRPO”训练方法，该方法将每一步的视觉与功能分数作为稠密的奖励信号，对作为代理核心的LLM进行过程监督下的强化学习。 实验结果表明，在WebGen-Bench数据集上，WebGen-Agent将Claude-3.5-Sonnet模型的准确率从26.4%显著提升至51.9%。此外，通过Step-GRPO方法训练后，Qwen2.5-Coder-7B-Instruct模型的准确率也从38.9%增长到45.4%。 对于AI从业者而言，该研究证实了在视觉与交互密集型的代码生成任务中，整合多层级、多模态的反馈比单一的代码执行反馈更为有效。其提出的Step-GRPO方法为如何利用这些内生的、步骤级的反馈信号作为稠密奖励来训练代码代理提供了实用范例，尤其是在通过过程监督来提升开源小模型性能方面，展示了超越传统结果监督的潜力。

## 20.[SPARK: Synergistic Policy And Reward Co-Evolving Framework](https://arxiv.org/pdf/2509.22624)
summary:**核心关键词**：策略与奖励协同演化, 生成式奖励模型, 在策略学习, 自反思  **一句话核心摘要**：该研究提出了一种名为SPARK的策略与奖励协同演化框架，它通过在可验证奖励强化学习（RLVR）中回收rollouts和正确性数据，将策略模型自身同步训练为一个生成式奖励模型，从而无需独立的奖励模型和昂贵的人类偏好数据即可高效提升模型性能。  **主要研究问题或目标**：本研究旨在解决当前大语言模型（LLM）和大多模态语言模型（LVLM）在强化学习应用中的核心挑战，即如何克服基于人类反馈的强化学习（RLHF）的高成本和奖励-策略不匹配问题，以及可验证奖励强化学习（RLVR）中监督信号被浪费的问题。  **关键方法论**：SPARK框架构建于可验证奖励强化学习（RLVR）之上，其核心方法是回收在策略更新后通常被丢弃的rollouts及其正确性数据。该框架利用这些回收的数据，通过一个包含逐点奖励评分、成对比较和基于反思的条件评估的混合辅助目标，将策略模型自身同步训练成一个生成式奖励模型。这一机制形成了一个正向反馈循环：更准确的内置奖励模型产生更好的策略梯度，从而生成更高质量的rollouts，进而进一步优化奖励模型。  **主要成果**：实验结果表明，SPARK框架在多个LLM和LVLM模型上均取得了显著的性能提升。具体而言，SPARK-VL-7B模型相较于基线，在7个推理基准上平均提升了9.7%，在2个奖励模型基准上平均提升了12.1%，并在8个通用基准上平均提升了1.5%，展示了该方法的鲁棒性和广泛的泛化能力。  **对AI从业者的主要启示**：对于AI工程师和研究人员，SPARK提供了一种更高效、稳定且成本更低的RL对齐范式。它通过在单一模型内统一策略优化和奖励建模，省去了训练独立奖励模型和标注人类偏好数据的需求，显著简化了模型训练和部署流程。此外，该框架支持无需外部模型的测试时自反思扩展，为开发更强大、更具自主校正能力的AI系统提供了可行的技术路径。

## 21.[Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval](https://arxiv.org/pdf/2509.21710)
summary:**多智能体上下文演化与检索 (MACER)，异构图索引，双重演化机制，图增强生成 (Graph-RAG)** 本文提出Think-on-Graph 3.0 (ToG-3)框架，通过引入一个多智能体上下文演化与检索（MACER）机制，对查询和子图进行双重动态演化，以克服现有图增强生成（Graph-RAG）方法中静态图索引的局限性，从而在使用轻量级大语言模型时也能实现深度和精确的推理。 该研究旨在解决现有图增强生成（Graph-RAG）方法面临的关键困境：手动构建的知识图谱难以扩展，而基于轻量级模型从语料库自动提取的静态图质量有限，导致无法根据具体查询进行自适应调整，从而影响复杂推理的性能。 核心方法是多智能体上下文演化与检索（MACER）机制，该机制由构造器、检索器、反思器和响应器四个智能体协同工作，通过一个迭代循环动态构建和优化一个由“文本块-三元组-社区”组成的异构图索引。其关键创新在于“双重演化”：当反思器判断当前信息不足时，它会演化查询（生成子查询），同时构造器会演化子图（补充相关知识），直至检索到的上下文足以回答问题。 实验结果表明，ToG-3在多个深度推理基准测试中超越了现有基线方法，在HotpotQA、2WikiMultihopQA和Musique三个数据集上取得了0.453的平均精确匹配（EM）分数。消融研究证实了框架各组件的有效性，其中移除“查询演化”机制会导致性能下降最为显著，平均EM分数降低12.6%。 对于AI从业者而言，该研究提供了一个在资源受限（如使用本地化部署的轻量级模型）场景下构建高性能RAG系统的实用框架。它展示了如何通过动态、自适应的图构建与查询分解过程，替代传统的静态图索引方法，从而显著提升系统在处理需要多步推理的复杂问题时的准确性和深度，为开发更智能、更可靠的知识密集型应用提供了新的技术路径。

## 22.[TUN3D: Towards Real-World Scene Understanding from Unposed Images](https://arxiv.org/pdf/2509.21388)
summary:**核心关键词**: 联合布局估计, 3D目标检测, 多视角图像, 稀疏卷积, 无姿态图像 TUN3D是首个在实际扫描场景中，利用多视角图像输入且无需真值相机姿态或深度监督，联合进行布局估计和3D目标检测的方法，并在多种挑战性场景理解基准上达到了最先进的性能，显著推进了布局估计，为整体室内场景理解建立了新基准。 现有室内场景理解方法通常依赖点云输入，但消费级相机普遍缺乏深度传感器，视觉数据更常见，这构成了主要限制。TUN3D旨在解决这一局限性，实现从多视角图像输入（无需真值相机姿态或深度监督）进行联合布局估计和3D目标检测。 TUN3D采用轻量级稀疏卷积骨干网络，并配备两个专用头部：一个用于3D目标检测，另一个用于布局估计，其中布局估计利用了新颖有效的参数化墙体表示。 广泛实验表明，TUN3D在三种挑战性场景理解基准（使用真值点云、姿态图像和无姿态图像）上均达到了最先进的性能。它在专业3D目标检测方法中表现相当，同时显著提升了布局估计的性能，例如在ScanNet上，布局估计F1分数比PQ-Transformer高出11.1分。 TUN3D的价值在于其极大的输入灵活性，使AI从业者能够利用更普遍的视觉数据（如来自消费级设备的无姿态多视角图像），实现对室内场景的全面3D理解和建模，大大降低了对专业深度传感器或高精度姿态估计设备的需求，扩展了相关应用场景。

## 23.[UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models](https://arxiv.org/pdf/2509.21760)
summary:**UniVid**, **预训练视频生成模型**, **视频扩散Transformer**, **视觉语句**, **跨模态泛化** UniVid提出通过微调预训练视频扩散Transformer，在不进行任务特定修改的情况下处理各类视觉任务，旨在为视觉建模提供统一且可扩展的基础。 该研究旨在探索预训练视频生成模型是否能适应多样化的图像和视频任务。 UniVid框架通过对预训练的视频扩散Transformer进行微调，将不同视觉任务表示为视觉语句，其中上下文序列定义任务及预期输出模态，无需任务特定架构修改。 UniVid在仅使用自然视频数据进行预训练的情况下，展现了良好的跨模态推理和跨源任务泛化能力。此外，通过简单地逆转视觉语句顺序，模型可轻松实现理解任务和生成任务之间的切换。 这项研究强调了预训练视频生成模型作为可扩展、统一的视觉建模基础的潜力，为AI从业者提供了处理多样化视觉任务的通用骨干模型，从而降低了任务特定预训练的成本和复杂性。

## 24.[D-Artemis: A Deliberative Cognitive Framework for Mobile GUI Multi-Agents](https://arxiv.org/pdf/2509.21799)
summary:**移动GUI智能体，审思认知框架，执行前对齐，多模态大语言模型** 该研究提出了一个名为D-Artemis的新型审思认知框架，通过模拟人类的思考、对齐和反思认知循环，并结合细粒度提示检索与执行前对齐机制，解决了当前GUI智能体面临的数据瓶颈、高成本错误检测等挑战，从而增强了通用多模态大语言模型（MLLMs）在GUI任务上的能力。 主要研究问题或目标是克服当前移动GUI智能体在端到端训练中面临的数据瓶颈、延迟错误检测的高昂成本以及外部指导信息可能存在的矛盾风险。 关键方法是构建一个包含三个核心阶段的审思认知循环：首先，利用一个细粒度的、特定于应用的提示检索机制来指导决策；其次，通过一个主动的“执行前对齐”阶段，其中“思考-行为一致性”（TAC）检查模块与“行为纠正智能体”（ACA）协同工作以预防和修正错误；最后，由“状态反思智能体”（SRA）在执行后进行复盘，实现经验学习。 主要成果是D-Artemis框架在两大主流基准测试中均刷新了SOTA记录，在AndroidWorld上取得了75.8%的成功率，在ScreenSpot-V2上成功率达到96.8%，证明了其卓越的性能。 对AI从业者的主要启示在于，该框架提供了一种无需依赖复杂轨迹数据集进行训练即可显著提升通用MLLMs在GUI任务上性能的有效范式，这为开发数据高效、泛化能力强且更可靠的GUI自动化智能体指明了新的方向。

## 25.[Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training](https://arxiv.org/pdf/2509.21500)
summary:**核心关键词**: 奖励过度优化, 基于准则的奖励, 高奖励区域, 离策略样本  **一句话核心总结**: 本文研究了如何通过一种基于准则(rubric-based)的奖励建模方法，利用优质且多样化的离策略样本来精确捕捉高奖励区域的信号，从而显著缓解大语言模型强化微调中的奖励过度优化问题并提升训练效果。  **主要研究问题或目标**: 该研究旨在解决大语言模型强化微调中普遍存在的奖励过度优化问题，即策略模型利用代理奖励模型的缺陷获得高分但实际输出质量低下。其核心目标是提升奖励模型在高分区间的分辨能力，使其能可靠地区分“卓越”与“优秀”的响应。  **关键方法论**: 论文提出了一种迭代式的“通过差异化进行精炼”(Refinement-through-Differentiation)工作流：首先，利用强大的外部模型生成优质且多样化的离策略响应对；然后，使用一个“提议者”LLM分析这些响应对之间的关键差异，并将这些差异转化为具体、可量化的新准则(rubric)，或对现有准则进行精炼；通过多轮迭代，该方法逐步提升准则在高奖励区域的精确度和分辨力。  **主要成果**: 实验证明，该方法能有效缓解过度优化。在健康领域任务中，使用经过4对优质且多样化响应精炼的准则进行训练，模型胜率达到34.4%，HealthBench得分达到0.3513。该方法将奖励模型在高奖励区域的准确率从40.3%提升至47.9%，并将在训练中出现过度优化（性能下降）的步数从约60步推迟到约160步。  **对AI从业者的主要启示**: 对于AI工程师而言，该研究最重要的启示是，奖励模型的关键在于其分辨顶尖样本（高奖励尾部）的能力，而非全局准确性。这提供了一种比传统RLHF更高效、更抗“黑客攻击”的奖励建模范式：从业者无需收集海量偏好数据，而是可以专注于生成少量、高质量、多样化的“典范”响应，并利用它们构建结构化的评估准则。这种方法能更精确地引导模型学习真正的卓越能力，尤其适用于专业领域等高质量数据稀缺的场景。

## 26.[WoW: Towards a World omniscient World model Through Embodied Interaction](https://arxiv.org/pdf/2509.22642)
summary:**具身世界模型, SOPHIA, 逆动力学模型, WoWBench, 物理直觉** 本文提出了WoW，一个140亿参数的具身生成世界模型，通过SOPHIA框架和逆动力学模型结合大规模真实世界机器人交互数据，旨在解决现有视频模型物理直觉不足的问题，并在物理一致性和因果推理基准WoWBench上实现最先进性能。该研究旨在解决现有视频模型因被动观察而难以掌握物理因果关系的问题，并致力于通过具身世界模型发展AI的真实物理直觉。核心方法是WoW模型，一个140亿参数的生成世界模型，在200万条机器人交互轨迹上训练，并结合SOPHIA范式，该范式利用视觉语言模型（VLM）评估由DiT生成的输出，并通过迭代演化语言指令来指导模型精炼，同时通过协同训练的逆动力学模型将精炼后的规划转化为可执行的机器人动作，从而闭合想象-行动循环。WoW模型在物理一致性和因果推理基准WoWBench上达到了最先进的性能，在人工和自主评估中均表现出色，尤其在物理因果、碰撞动力学和物体永存性方面展示出强大的能力。这项工作为AI从业者提供了通过大规模真实世界交互数据构建具身智能的新范式，表明了结合VLM迭代精炼和逆动力学模型能够有效提升生成模型的物理真实感和行动可执行性，并且其模型、数据和基准将开源，为具身世界模型的未来研究奠定基础。

## 27.[Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning](https://arxiv.org/pdf/2509.22601)
summary:**核心关键词**：智能体强化学习, 自模仿学习, 渐进式探索, 策略熵  **一句话核心总结**：为解决大型语言模型智能体在长时程、稀疏奖励任务中的探索-利用权衡难题，该研究提出了一个名为SPEAR的课程化自模仿学习方法，通过内在奖励和自模仿学习引导策略熵的动态演化，从而有效提升智能体的任务成功率。  **主要研究问题或目标**：该研究旨在解决大型语言模型智能体在强化学习训练中所面临的核心挑战：如何在长时程、稀疏奖励任务中有效平衡探索与利用，从而避免因策略熵过早崩溃或无限制增长而导致的训练不稳定问题。  **关键方法论**：该研究提出SPEAR方法，其核心是基于课程的自模仿学习（SIL）。在训练初期，通过一个随时间衰减的内在“工具调用”奖励来促进技能层面的广泛探索；随着训练深入，系统逐渐加强对回报缓冲区中成功轨迹的自模仿，转向动作层面的深度利用。为稳定训练，该方法引入了优势重校准（Advantage Recalibration）技术，使用近期奖励的动态基线修正历史经验的优势值以应对策略漂移，并采用基于协方差的裁剪（Covariance-based Clipping）来正则化策略更新，防止过拟合。  **主要成果**：实验结果表明，SPEAR在多个基准测试中显著提升了不同基线模型的性能。具体而言，在ALFWorld任务上，SPEAR将GRPO基线的成功率提升了高达16.1%；在WebShop任务上，成功率提升了20.7%。这些性能增益仅带来10%-25%的额外理论计算复杂度和可忽略的实际运行开销，证明了其高效性和可扩展性。  **对AI从业者的主要启示**：该研究为AI从业者提供了一个即插即用且高效的强化学习训练方法SPEAR，可用于构建更稳定、性能更强的LLM智能体。其核心价值在于揭示了课程化训练范式在智能体学习中的重要性，即从广泛的技能探索平滑过渡到精确的策略利用。开发者可直接应用其优势重校准和协方差裁剪等技术来稳定训练过程，或将该方法集成到现有的基于GRPO的训练框架中，以较低成本提升智能体在复杂工具使用和长时程规划任务上的表现。

## 28.[FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image Editing](https://arxiv.org/pdf/2509.22244)
summary:**FlashEdit**, **文本引导图像编辑**, **实时编辑**, **单步反演编辑**, **背景保护**, **稀疏空间交叉注意力**  FlashEdit引入了一个新颖的框架，通过One-Step Inversion-and-Editing (OSIE)管线、Background Shield (BG-Shield)技术和Sparsified Spatial Cross-Attention (SSCA)机制，实现了高保真、实时文本引导图像编辑。该论文旨在解决扩散模型文本引导图像编辑中存在的处理延迟过高，从而阻碍实际应用的问题，目标是实现高保真、实时的图像编辑。FlashEdit通过三项关键创新提高效率：首先，一个单步反演编辑 (OSIE) 管线绕过了耗时的迭代过程；其次，Background Shield (BG-Shield)技术通过仅在编辑区域内选择性修改特征来确保背景完整性；最后，稀疏空间交叉注意力 (SSCA) 机制通过抑制语义泄露到背景，确保了精确的局部编辑。实验结果表明，FlashEdit在保持卓越的背景一致性和结构完整性的同时，能在0.2秒内完成编辑，相较于先前的多步方法，实现了超过150倍的速度提升。对于AI从业者而言，FlashEdit的突破性成果使得扩散模型文本引导图像编辑从耗时操作转变为实时应用成为可能，极大地提升了用户体验和实际应用价值，特别是在需要快速、精确图像修改的场景中。

## 29.[The role of synthetic data in Multilingual, Multi-cultural AI systems: Lessons from Indic Languages](https://arxiv.org/pdf/2509.21294)
summary:**合成数据**, **多语言多文化AI**, **印度语言**, **大型语言模型**, **指令遵循数据集** 本研究通过引入Updesh这一包含9.5M数据点、覆盖13种印度语言的指令遵循合成数据集，旨在解决多语言、多文化AI系统在低资源环境下有效运行并保持文化一致性的挑战。论文的核心目标是探索合成数据在多语言多文化AI背景下的有效性及其生成策略，尤其关注印度语言。其核心方法是采用自下而上的生成策略，提示参数规模≥235B的大型开源LLM，以语言特定的维基百科内容为基础生成数据，以此补充传统自上而下的翻译范式。综合评估（包括10k次人工评估）显示，Updesh生成数据质量高，但人工评估也指出了进一步改进的领域。在下游评估中，使用Updesh数据集微调的模型在生成任务上持续取得显著性能提升，并在多项选择型NLU任务中保持竞争力。模型在低资源和中等资源语言中的相对改进最为显著，有效缩小了与高资源语言的差距。这些实证结果表明，要有效开发多语言AI，需采取多方面的数据管理和生成策略，并融入上下文感知和文化背景化的方法。

## 30.[HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models](https://arxiv.org/pdf/2509.22300)
summary:**扩散模型**, **历史引导采样**, **动量采样**, **图像生成质量**, **采样效率**  本文提出了一种名为历史引导采样（HiGS）的动量采样技术，通过将近期模型预测集成到每个推理步骤中来增强扩散采样的质量和效率，以解决扩散模型在较少神经功能评估（NFE）或较低引导尺度下生成图像不真实和缺乏细节的问题，从而实现更高保真度的快速生成。  该研究旨在解决扩散模型在图像生成中，尤其是在使用较少神经功能评估（NFE）或较低引导尺度时，输出图像可能不真实且缺乏精细细节的问题。  HiGS是一种新颖的动量采样技术，它通过将近期模型预测集成到每个推理步骤中来提升扩散采样的质量和效率。具体而言，HiGS利用当前预测与过去预测的加权平均值之间的差异来引导采样过程，使其产生更逼真的输出，具有更好的细节和结构。此方法几乎不增加额外计算，无需额外训练或微调，并可无缝集成到现有扩散框架中。  广泛实验表明，HiGS在不同模型、架构、采样预算和引导尺度下均能一致性地改善图像质量。值得注意的是，使用预训练的SiT模型，HiGS在无引导256×256 ImageNet生成上仅用30个采样步骤（而非标准250步）就达到了1.61的最新FID。  HiGS作为标准扩散采样的即插即用增强，为AI从业者提供了一种无需额外训练即可提升模型生成图像质量和效率的方法，特别适用于需要更快生成速度和更高保真度的应用场景。

## 31.[CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition](https://arxiv.org/pdf/2509.19768)
summary:**Core Keywords**: 历史文本识别, 视觉语言模型, CHURRO-DS, 开源模型, 成本效益  本文介绍了CHURRO，一个专门用于历史文本识别的3B参数开源视觉语言模型，该模型在迄今最大的历史文本识别数据集CHURRO-DS上训练，并在性能和成本效益方面均超越了现有模型，以促进文化遗产的可读性。论文旨在解决现有视觉语言模型不适用于具有多样语言、脚本、不规则布局和频繁退化等特点的历史文档的文本识别问题，从而提高历史文本识别的准确性和可读性。核心方法是开发一个3B参数的开源视觉语言模型CHURRO，并利用CHURRO-DS数据集进行训练，该数据集整合了155个历史语料库、99,491页文档，涵盖22个世纪和46个语言集群。在CHURRO-DS测试集上，CHURRO模型在印刷文本上实现了82.3%的归一化Levenshtein相似度，在手写文本上实现了70.1%，分别比次优模型（Gemini 2.5 Pro）高出1.4%和6.5%，同时成本效益高出15.5倍。对于AI从业者而言，该研究通过发布CHURRO模型和CHURRO-DS数据集，为高精度、低成本的历史文本识别提供了即用型解决方案和丰富的训练资源，极大地降低了开发专门模型的门槛，并为改进历史文本可读性和加速学术研究奠定了基础。

## 32.[StateX: Enhancing RNN Recall via Post-training State Expansion](https://arxiv.org/pdf/2509.22630)
summary:**循环神经网络 (RNNs), 状态空间模型 (SSMs), 线性注意力 (linear attention), 状态扩展 (state expansion), 后训练 (post-training)**  本文介绍了StateX，一种用于高效扩展预训练循环神经网络（RNNs），特别是线性注意力和状态空间模型状态的后训练流程，旨在提高其在长上下文中的召回和上下文学习能力，同时避免高额成本。该研究旨在解决RNNs在长上下文处理中，由于恒定大小的循环状态导致上下文信息召回能力不足，且直接训练更大状态的RNNs成本高昂的问题。StateX通过后训练阶段对预训练RNNs进行架构修改，以扩展其状态大小，模型参数增加可忽略不计，具体为线性注意力和状态空间模型设计定制的后训练架构修改。实验结果表明，在高达1.3B参数的模型上，StateX有效地增强了RNNs的召回和上下文学习能力，未显著增加后训练成本或损害其他性能，例如在召回密集型任务中，GLA模型的相对准确率提高了3.36%。StateX为AI从业者提供了一种高效且低成本的途径，以提升现有预训练RNNs处理长上下文的召回和上下文学习能力，使其成为Transformer模型在长上下文场景中更具竞争力的替代方案。

## 33.[X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning](https://arxiv.org/pdf/2509.21559)
summary:**文本到视频检索, 大语言模型, 思维链推理, 可解释性, 视频标注** 本文提出了X-CoT，一个基于大语言模型思维链推理的可解释文本到视频检索框架，旨在取代传统的基于嵌入模型相似度排序，从而提高检索性能并提供详细解释。该研究旨在解决现有文本到视频检索系统因低质量数据对检索的影响难以识别、以及余弦相似度无法解释排序结果、限制模型可解释性的问题。X-CoT通过用大语言模型思维链推理取代嵌入模型相似度排序，并扩展现有基准数据集以提供额外的视频标注来支持语义理解和减少数据偏差；同时，设计了包含成对比较步骤的检索思维链，以生成详细推理和完整排序。实验结果表明，X-CoT经验性地提高了检索性能，并能生成详细的推理过程，但抽象部分未提供具体的量化性能提升数据。这为AI从业者提供了一种评估检索模型和审查文本-视频数据质量的手段，通过提供可解释的推理过程，增强了文本到视频检索系统的透明度和可靠性。

## 34.[RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards](https://arxiv.org/pdf/2509.21319)
summary:**二进制灵活反馈强化学习 (RLBFF)**, **奖励模型 (RM)**, **人类反馈强化学习 (RLHF)**, **可验证奖励强化学习 (RLVR)**, **大语言模型对齐** 本文提出了二进制灵活反馈强化学习 (RLBFF)，它结合了人类驱动偏好的多功能性与基于规则验证的精确性，以训练奖励模型捕捉细微的响应质量，并在大语言模型后训练对齐方面取得最先进的性能，同时显著降低推理成本。  该研究旨在解决现有LLM后训练范式中RLHF解释性差和奖励操控问题，以及RLVR范围受限于基于正确性的验证问题，目标是提出一种能结合两者优势、捕获细致响应质量并提高对齐效率的新方法。RLBFF的核心方法是从自然语言反馈中提取可二元回答的原则（例如信息准确性：“是”或代码可读性：“否”），并将这些原则用于将奖励模型训练为蕴含任务，即判断响应是否满足特定原则。  实验结果表明，通过RLBFF训练的奖励模型在RM-Bench上达到86.2%，在JudgeBench上达到81.4%（截至2025年9月24日排名第一），性能优于Bradley-Terry模型。此外，使用RLBFF对Qwen3-32B进行的模型对齐，在MT-Bench、WildBench和Arena Hard v2等通用对齐基准测试上的表现与o3-mini和DeepSeek R1相当或更优，且推理成本低于5%。  对于AI从业者而言，RLBFF提供了一种训练可解释且不易奖励操控的奖励模型的新范式，并通过在推理时允许用户自定义原则，增强了模型的灵活性和实用性。此外，该研究提供了一个完全开源的配方，能以极低的推理成本实现高性能LLM对齐，具有重要的应用价值。

## 35.[CAD-Tokenizer: Towards Text-based CAD Prototyping via Modality-Specific Tokenization](https://arxiv.org/pdf/2509.21150)
summary:**CAD-Tokenizer, 文本引导CAD原型设计, VQ-VAE, 模态特定分词, 原始级别池化** 本文提出了CAD-Tokenizer框架，通过模态特定分词策略，将CAD数据表示为原始级别（primitive-level）令牌，从而显著提高了文本引导CAD原型设计（包括生成和编辑）的指令遵循和生成质量，超越了通用LLM和任务特定基线。 该研究旨在解决标准大型语言模型（LLM）分词器在处理CAD序列时，将其分解为自然语言词块，导致无法捕获原始级别CAD语义和建模几何结构的问题，从而阻碍了文本引导CAD原型设计（包含Text-to-CAD生成和CAD编辑）的流程化。 核心方法是开发一个序列化VQ-VAE模型，该模型采用原始级别池化（primitive-level pooling）和受限解码机制，以生成紧凑、原始感知的CAD数据表示，并将其与LLM的嵌入空间对齐。 实验结果表明，CAD-Tokenizer在统一的文本引导CAD原型设计任务中显著提升了性能，例如在CAD编辑任务中，F1分数提高了约10个百分点，并且在文本到CAD生成任务中也显示出显著改进。 对于AI从业者而言，CAD-Tokenizer为将LLM与CAD数据深度集成提供了一种更有效的方法，通过模态特定且原始感知的令牌化，能够更准确地理解和生成CAD结构，从而优化工业原型设计流程，提升设计自动化效率。

## 36.[Finding 3D Positions of Distant Objects from Noisy Camera Movement and Semantic Segmentation Sequences](https://arxiv.org/pdf/2509.20906)
summary:**3D目标定位**, **远距离目标**, **粒子滤波器**, **语义分割**, **无人机监测** 本研究提出了一种基于噪声相机运动和语义分割序列的粒子滤波器方法，用于解决远距离目标3D定位问题，特别是在传统密集深度估计或3D场景重建方法因计算资源限制或目标距离遥远而不可行的情况下，为安全关键型监控任务提供了实用的解决方案。 该论文旨在解决从噪声相机运动和语义分割序列中实现远距离目标3D定位的挑战，特别是在计算资源受限或目标距离遥远导致传统方法失效的场景中。 核心技术方法是利用粒子滤波器，该滤波器通过GNSS估计的相机姿态和图像分割序列迭代改进目标对象3D位置的定位和不确定性估计，并支持单目标和多目标场景，且其对具体检测方法具有独立性。 仿真和无人机图像分割序列实验结果表明，粒子滤波器能够有效解决在其他方案失败情况下，基于相机姿态和图像分割序列的实际定位任务；研究还证明该方法结合现有图像分割模型可用于无人机野火监测。 对于AI从业者，该研究提供了一种计算效率高、对检测方法不敏感的远距离目标3D定位范式，特别适用于边缘计算环境下的无人机监控等安全关键应用，扩展了现有3D定位技术的适用范围和灵活性。

## 37.[Instruction-Following Evaluation in Function Calling for Large Language Models](https://arxiv.org/pdf/2509.18420)
summary:**函数调用, 大型语言模型, 指令遵循, 基准测试, IFEval-FC**  本文引入了IFEval-FC基准测试，通过在JSON Schema描述中编码可验证的格式指令，评估大型语言模型在函数调用中遵循精确指令的能力，旨在解决现有基准测试未涵盖的格式遵循问题，并揭示了SOTA模型在此方面的实际局限性。现有函数调用基准测试主要评估参数正确性，但未测试对参数描述中嵌入的格式指令（如双引号或ISO日期格式）的遵循情况，IFEval-FC旨在弥补此空白，专注于评估大型语言模型的精确指令遵循能力。IFEval-FC基准测试受到了IFEval的启发，其方法论核心在于直接在JSON Schema描述中编码可验证的格式指令，例如要求值不得包含标点符号。该基准包含750个测试用例，每个用例由一个带有嵌入格式的函数输入参数及相应的用户查询组成，并通过完全算法化的方式进行评估，确保客观性、可复现性和可扩展性。评估结果表明，即使是GPT-5和Claude 4.1 Opus等最先进的专有模型，也经常未能遵循基本的格式规则。这一发现揭示了大型语言模型在实际代理系统应用中的显著局限性，对AI工程师和研究人员开发更鲁棒、能可靠与外部系统交互的AI代理具有重要指导意义。

