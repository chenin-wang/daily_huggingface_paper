

# Papers for 2025-09-23

## 0.[RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation](https://arxiv.org/pdf/2509.16198)
summary:**核心关键字**：仓库级代码生成, 仓库规划图(RPG), 统一规划, 可扩展生成  **1-Sentence Core Summary**：本文提出了一种名为仓库规划图（RPG）的持久化图表示方法，通过统一编码软件的功能、文件结构、数据流和函数来取代模糊的自然语言计划，从而解决了从零开始生成完整、可扩展软件仓库的根本挑战。  **Main Research Question or Objective**：该研究旨在解决大型语言模型在从零开始生成完整软件仓库时面临的规划难题，即如何克服传统自然语言计划因其模糊性和冗长性而难以忠实表示复杂软件结构的问题，从而实现连贯且可靠的跨层级规划。  **Key Methodology**：核心方法是引入仓库规划图（RPG），一种将提议级规划（定义功能范围和关键能力）与实现级规划（指定文件结构、接口、依赖和数据流）统一起来的图结构。基于RPG，论文构建了ZeroRepo框架，该框架通过三个阶段运行：1) 提议级规划和实现级精化来构建完整的RPG；2) 进行图引导的代码生成；3) 通过测试进行验证。这种方法用明确的结构化蓝图替代了不稳定的自然语言描述。  **Primary Results**：在包含1052个任务的RepoCraft基准测试上，ZeroRepo生成的代码库平均近36K代码行，规模是最强基线Claude Code的3.9倍。其功能覆盖率达到81.5%，测试通过率达到69.7%，分别比Claude Code高出27.3和35.8个百分点。分析表明，RPG支持近线性的规划扩展，并能增强大语言模型对仓库的理解力。  **Principal Implication for AI Practitioners**：对于AI从业者而言，本文提出的仓库规划图（RPG）提供了一种将大型语言模型从生成孤立函数或文件提升到自动化构建完整、复杂软件仓库的有效范式。工程师可以利用RPG作为一种明确、结构化的蓝图来替代模糊的自然语言需求，从而在自动化软件开发任务中实现更可靠的长期规划、更强的复杂依赖管理和更高的可扩展性，尤其适用于大型项目的自动化从零构建。

## 1.[MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer](https://arxiv.org/pdf/2509.16197)
summary:**核心关键词**：混合视觉分词器, 统一多模态模型, 图像理解与生成, 任务冲突  **一句话核心摘要**：该研究提出了一种名为Manzano的简单且可扩展的统一多模态框架，它通过一个混合图像分词器耦合精心设计的训练方案，从单一共享视觉编码器生成用于理解的连续嵌入和用于生成的离散词元，从而显著缓解了现有模型在视觉理解与生成能力之间的性能冲突。  **主要研究问题或目标**：该论文旨在解决统一多模态大语言模型（LLM）中普遍存在的一个核心问题：在单一模型中集成视觉内容理解和生成两种能力时，通常会导致两者之间出现性能权衡（trade-off），即提升一项能力会损害另一项。  **关键方法论**：Manzano的核心方法是采用一种混合视觉分词器架构。该架构使用一个共享的视觉编码器（ViT），并为其连接两个轻量级适配器：一个连续适配器，为图像到文本的理解任务生成连续特征嵌入；一个离散适配器，为文本到图像的生成任务产生离散的视觉词元（tokens）。这两种表征源于同一编码器，处于共同的语义空间中。一个统一的自回归LLM负责预测文本和离散图像词元等高层语义，再由一个辅助的扩散解码器将生成的图像词元渲染成最终的像素图像。  **主要成果**：实验结果表明，Manzano在统一多模态模型中达到了SOTA水平，并且与专用模型相比也具有竞争力，尤其是在富文本（text-rich）理解基准上表现突出。例如，Manzano-30B模型在DocVQA基准测试中取得了94.3%的准确率。消融研究证实，该混合分词器设计有效地将任务冲突降至最低，并且模型在尺寸扩展时表现出持续的性能增益。  **对AI从业者的主要启示**：对于AI工程师和研究者而言，Manzano提供了一个经过验证的、可有效平衡多模态理解与生成能力的简洁架构蓝图。其最关键的价值在于证明了通过共享编码器和混合表征（连续+离散）的设计，可以构建出既能高质量生成图像、又能精确理解视觉内容（特别是复杂的富文本文档）的统一模型，而无需在两种能力之间做出显著牺牲。该方法的可扩展性强，为开发更强大的下一代集成式多模态AI系统提供了实用范式。

## 2.[Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification](https://arxiv.org/pdf/2509.15591)
summary:**核心关键词**： **潜藏分区网络 (Latent Zoning Network)**，**统一框架 (Unified Framework)**，**共享潜藏空间 (Shared Latent Space)**，**流匹配 (Flow Matching)**  **一句话核心摘要**： 该研究提出一种名为潜藏分区网络（LZN）的统一原理，它通过构建一个共享的高斯潜藏空间，并为每种数据类型配备将样本映射到不相交潜藏分区的编码器和解码器，从而统一地解决生成建模、表示学习和分类这三个核心机器学习问题，旨在简化ML流程并促进跨任务协同。  **主要研究问题或目标**： 本文旨在解决生成建模、表示学习和分类这三大核心机器学习任务的SOTA解决方案相互独立、缺乏协同的问题，探索是否能用一个统一的原理来整合这三个任务。  **关键方法论**： LZN的核心是构建一个共享的高斯潜藏空间，并为每种数据类型（如图像、标签）设计独立的编码器和解码器。编码器利用流匹配（Flow Matching）技术将数据样本映射到该空间中互不相交的“潜藏分区”（latent zones），确保不同样本拥有独特的表征且整体符合先验分布。所有机器学习任务被重新定义为这些编码器和解码器的组合，并通过一个新颖的“潜藏对齐”损失函数来对齐不同数据类型（如图像和标签）的潜藏分区，从而实现跨任务操作。  **主要成果**： 实验证明了LZN的有效性：在无监督表示学习任务中，LZN在ImageNet上的下游线性分类性能超越了经典的MoCo方法9.3%；在与SOTA生成模型Rectified Flow结合时，LZN将CIFAR10上的FID得分从2.76提升至2.59；当联合执行生成和分类任务时，其在两个任务上的性能均超过了单独训练的性能，证明了框架的协同效应。  **对AI从业者的主要启示**： LZN为AI从业者提供了一个统一处理生成、表示和分类任务的通用框架，有望简化复杂的机器学习流水线。其最关键的价值在于，通过共享表示，不同任务可以相互促进，例如，联合训练生成和分类模型能同时提升两者的性能，这启示工程师可以构建更集成、更高效的多任务系统，减少为不同任务开发和维护独立模型的开销。

## 3.[SPATIALGEN: Layout-guided 3D Indoor Scene Generation](https://arxiv.org/pdf/2509.14981)
summary:**核心关键词**： **三维室内场景生成**，**布局引导**，**多视图扩散模型**，**多模态生成**  **一句话核心摘要**： 该研究基于其构建的一个包含470万张渲染图的大规模合成数据集，提出了一种名为SpatialGen的新型多视图多模态扩散模型，该模型能够根据给定的3D布局和参考图像，生成视觉真实且语义一致的3D室内场景，解决了现有方法在视觉质量和空间一致性上的挑战。  **主要研究问题或目标**： 本研究旨在解决自动化3D室内场景生成中的核心瓶颈，即缺乏大规模、高质量的标注数据集，并致力于开发一个能够平衡视觉质量、多样性、语义一致性和用户控制的生成模型，以实现从任意视点生成连贯且高保真的3D室内环境。  **关键方法**： 该研究的核心方法是一个布局引导的多视图多模态扩散模型，它通过一个“交替注意力机制”协同处理跨视图和跨模态信息，从而联合生成外观（RGB图像）、几何（场景坐标图）和语义（语义分割图）。该框架首先利用该模型通过迭代式稠密视图生成策略产出多张一致的2D视图，然后采用基于高斯溅射（Gaussian Splatting）的方法将这些视图重建为最终的3D场景。  **主要成果**： 实验证明SpatialGen的性能优于现有方法，在与基于分数蒸馏的基线模型对比中，当在组合数据集上训练后，SpatialGen在Hypersim测试集上取得了**-0.285**的图像奖励（Image Reward）分数，显著超越了SceneCraft（-1.096）。定性结果也显示，SpatialGen生成的场景在细节真实度、几何准确性和布局遵循度上均表现出明显优势。  **对AI从业者的主要启示**： 此研究为AI从业者提供了两项关键资产：一个大规模、高质量的3D室内场景数据集，有效缓解了该领域的数据稀缺问题；以及一个端到端的、可控的3D内容生成模型（SpatialGen）。该模型的架构，特别是其处理多视图与多模态一致性的方法，为开发用于虚拟现实（VR）、游戏设计或机器人仿真的复杂场景生成系统提供了重要的技术参考和即用工具。

## 4.[BaseReward: A Strong Baseline for Multimodal Reward Model](https://arxiv.org/pdf/2509.16127)
summary:**核心关键词**：**多模态奖励模型**，**人类偏好对齐**，**系统性实验分析**，**BaseReward**  **一句话核心总结**：该论文通过对多模态奖励模型（MRM）开发流程中的范式、架构、数据和策略等关键组件进行系统性实验分析，提出了一套清晰的构建“配方”，并基于此推出了一个在多个基准上达到SOTA性能的强大基线模型BaseReward。  **主要研究问题或目标**：本文旨在解决当前学术界和工业界在构建高性能多模态奖励模型（MRM）时，普遍缺乏系统性、有实证支持的指导方法这一问题。  **关键方法论**：研究人员系统性地对MRM开发流程进行了详尽的消融实验，涵盖了奖励建模范式（如Naive-RM、Critic-based RM）、奖励头架构、训练正则化策略、十余个多模态与纯文本偏好数据集的筛选与组合、以及基座模型选择等。基于实验洞察，论文提出的BaseReward模型采用Qwen2.5-VL为基座，并优化设计了一个使用SiLU激活函数的两层MLP奖励头，在精心筛选的多模态和纯文本混合偏好数据上进行训练。  **主要成果**：实验结果表明，BaseReward在多个主流MRM基准上确立了新的SOTA性能，例如，在MM-RLHF-Reward Bench上，其准确率达到91.76%，相较于之前的SOTA模型提升了约11.9%。研究还发现，简单的“Naive-RM”范式结合优化的两层奖励头最为高效，并且高质量的纯文本偏好数据能显著增强MRM在多模态任务上的判断力。  **对AI从业者的主要启示**：该研究为AI工程师和研究者提供了一套清晰、可复现的高性能多模态奖励模型构建指南。其最关键的启示在于，从业者无需采用复杂的架构或正则化策略，通过一个简单的两层MLP奖励头结构，并精心融合高质量的文本与多模态数据，即可高效构建出顶尖性能的MRM，从而有效改进多模态大模型的RLHF对齐流程。

## 5.[A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning](https://arxiv.org/pdf/2509.15937)
summary:**核心关键词**：**稠密奖励模型，真实世界强化学习，视觉-语言-动作模型，人机回环**  **一句话核心摘要**：该研究提出了一种基于InternVL构建的、名为VLAC的通用过程奖励模型，该模型通过统一评判家与策略的功能，为真实世界机器人强化学习生成稠密的进度差量和完成信号，从而解决了稀疏奖励和低效探索的瓶颈，显著提升了物理世界中的自主学习效率和任务成功率。  **主要研究问题或目标**：该研究旨在解决当前视觉-语言-动作（VLA）模型在真实世界机器人强化学习应用中面临的核心挑战，即依赖稀疏、人工设计的奖励函数以及探索效率低下的问题。其目标是创建一个无需为特定任务工程化奖励函数、并能高效探索和学习的通用模型。  **关键方法**：该研究的核心方法是VLAC模型，一个统一的自回归架构。VLAC将“评判家”（Critic）和“策略”（Policy）集成于单一模型中，通过提示控制交替生成奖励和动作。其奖励机制基于成对的图像观测（pairwise observations）和语言目标，输出一个有符号的进度差量（progress delta）作为稠密奖励。模型在包含视觉-语言、机器人轨迹和人类轨迹的大规模异构数据集上进行训练，并通过构建负样本和语义不匹配样本来增强其对任务倒退或停滞的判别能力。在部署时，该模型被集成到一个异步的真实世界RL循环中，并结合了分级的人机回环（HITL）协议（离线演示回放、返回探索、人工引导探索）以加速和稳定早期学习。  **主要成果**：实验结果表明，VLAC模型在四个不同的真实世界操作任务中，仅通过自主学习就能在200个交互回合内将任务成功率从约30%提升至约90%。当引入人机回环干预时，模型的样本效率进一步提升50%，并最终可以达到100%的任务成功率。  **对AI从业者的主要启示**：该研究为AI工程师和研究者提供了一套在物理世界中部署在线强化学习的实用范式。其核心价值在于，通过一个预训练的通用奖励模型取代了传统方法中耗时且难以泛化的任务特定奖励工程，显著降低了机器人RL应用的门槛。从业者可以借鉴其统一的“评判家-策略”架构和异步RL框架，开发能够从与物理世界交互中持续自我改进的、数据高效的机器人智能体。

## 6.[Lynx: Towards High-Fidelity Personalized Video Generation](https://arxiv.org/pdf/2509.15496)
summary:**个性化视频生成, 扩散Transformer, 身份保真度, 轻量级适配器** 该研究提出了一个名为Lynx的高保真个性化视频生成模型，它基于一个开源的扩散Transformer（DiT）基础模型，通过引入ID-adapter和Ref-adapter两个轻量级适配器，从单张输入图像实现了卓越的面部身份保持和高质量视频合成。 该研究旨在解决从单张参考图像生成高保真个性化视频的挑战，核心目标是在确保视频动态自然、时序连贯和视觉效果逼真的同时，稳健地保持视频主角的身份特征。 其核心方法论是在DiT基础模型中集成两个解耦的适配器：ID-adapter利用Perceiver Resampler将ArcFace面部嵌入编码为紧凑的身份令牌，而Ref-adapter则通过一个冻结的参考网络提取密集的VAE特征以注入精细细节，两者均通过交叉注意力机制将特征注入到去噪网络的每一层。 实验结果表明，在包含40个主体和20个提示词的800个测试用例基准上，Lynx在身份保真度上显著优于现有方法，例如在使用facexlib评估器时其面部相似度得分达到0.779，同时在由Gemini-2.5-Pro API评估的提示词遵循度和整体视频质量指标上也取得了最高分。 此研究为AI从业者提供了一个高效且可扩展的个性化视频生成框架，其双适配器设计展示了如何在不需对大型基础模型进行完整微调的情况下，通过解耦并注入高级身份特征与低级视觉细节来实现高保真内容生成，为开发定制化视频应用提供了直接的技术参考。

## 7.[BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent](https://arxiv.org/pdf/2509.15566)
summary:**核心关键词**： **GUI智能体**，**Blink-Think-Link (BTL)框架**，**认知启发**，**过程-结果整合奖励**  **一句话核心摘要**： 本文提出了一种名为“Blink-Think-Link”(BTL)的认知启发框架，通过模拟人类感知、思考、行动的认知过程，并引入一种同时优化过程与结果的BTL奖励机制，旨在解决现有GUI智能体交互逻辑与人类模式偏差的问题，从而提升其在复杂GUI任务中的性能。  **主要研究问题或目标**： 该研究旨在解决当前AI驱动的GUI智能体交互逻辑与人类自然的认知与沟通模式存在显著偏差的问题，其核心目标是构建一个更符合人类认知过程的框架，以提高智能体在静态GUI理解和动态交互任务中的性能与效率。  **关键方法论**： 论文的核心方法是BTL框架，它将GUI交互分解为三个认知阶段：1) **Blink（感知）**：通过`<blink>`标签输出，快速检测并关注屏幕上的相关区域(ROI)；2) **Think（思考）**：通过`<think>`标签输出，进行高层次的推理和决策规划；3) **Link（连接）**：通过`<link>`标签输出，生成精确的可执行动作指令。为支持该框架，论文提出了两大技术创新：一是“Blink数据生成”自动化管道，为训练样本标注与指令相关的ROI；二是“BTL奖励”机制，这是一种新颖的、结合了过程与结果的规则型强化学习奖励，它包含用于模板和内容检查的“双格式奖励”、用于评估ROI定位准确性的“Blink奖励”以及用于评估最终动作精确性的“Link奖励”。  **主要成果**： 实验证明，基于该框架开发的BTL-UI智能体在多个GUI基准测试中达到了SOTA水平。具体而言，在修正后的ScreenSpot-V2 grounding（定位）任务上，BTL-UI-7B模型取得了89.1%的平均准确率，超越了所有基线模型。在AndroidControl高阶规划任务中，其步骤成功率(SR)达到69.2%，同样显著优于现有方法。  **对AI从业者的主要启示**： 对于AI从业者而言，该研究最重要的启示是，在设计GUI智能体或其他人机交互系统时，显式地建模人类的“感知-认知-行动”闭环流程，而非仅仅关注最终任务结果，是提升模型性能和泛化能力的有效路径。论文提出的过程-结果整合奖励机制（BTL Reward）为开发更鲁棒、更符合人类直觉的智能体提供了具体的、可实施的强化学习奖励设计范式，尤其适用于需要多步推理和精确操作的复杂任务场景。

## 8.[RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes](https://arxiv.org/pdf/2509.15123)
summary:**核心关键词**： **动态场景, 相机参数优化, 仅RGB监督, 离群点感知联合优化**  **1-Sentence Core Summary**： 该论文提出了一种名为ROS-Cam的新方法，旨在仅使用单个RGB视频作为监督，通过结合分块跟踪滤波器、离群点感知联合优化和两阶段优化策略，实现对动态场景中相机参数更准确、更高效的优化。  **Main Research Question or Objective**： 本文旨在解决在仅有单目RGB视频输入、无任何额外真值（如运动掩码、深度图、相机位姿）监督的情况下，如何准确且高效地优化动态场景中的相机参数这一核心问题。  **Key Methodology**： 该方法的核心技术包含三个部分：首先，通过分块跟踪滤波器（Patch-wise Tracking Filters）在视频帧间建立稀疏且鲁棒的特征点对应关系作为伪监督；其次，引入一种离群点感知联合优化机制，为每个特征点轨迹关联一个可学习的不确定性参数，并使用柯西分布（Cauchy distribution）对其建模，从而在无需运动先验的情况下自适应地降低移动物体（离群点）的影响；最后，采用一个两阶段优化策略，先快速收敛再联合精调，以提升优化速度和稳定性。  **Primary Results**： 实验结果表明，该方法在多个公开数据集上均优于现有方法。例如，在NeRF-DS数据集上，本方法的平均运行时间为0.83小时，显著快于次优的仅RGB监督方法（1.8小时），同时在新视角合成任务中取得了33.55的PSNR，超越了依赖真值运动掩码的COLMAP（32.17）。  **Principal Implication for AI Practitioners**： 对于AI从业者而言，该研究提供了一个高效、准确且仅依赖普通RGB视频的相机姿态估计算法，显著降低了处理动态场景视频（如手机随意拍摄的视频）的技术门槛。它使得在没有运动掩码标注或LiDAR等昂贵硬件支持的情况下，进行高质量的4D场景重建、增强现实（AR）和动态场景理解成为可能，具有很强的实际应用价值。

## 9.[Do You Hear What I Mean? Quantifying the Instruction-Perception Gap in Instruction-Guided Expressive Text-To-Speech Systems](https://arxiv.org/pdf/2509.13989)
summary:**指令引导文本转语音**，**感知差距**，**人类评估**，**E-VOC语料库** 本研究通过构建一个名为E-VOC的大规模人类评估语料库，对指令引导TTS（ITTS）系统的可控性进行了全面的感知分析，旨在量化用户风格指令与听众实际感知之间的差距。 该研究的核心目标是解决现有ITTS评估方法无法衡量细粒度指令（如分级情感强度）与听众感知精确对齐度的难题。 其关键方法是提出一个包含四个控制维度（程度副词、分级情感强度、说话人年龄、词级重音）的评估框架，并创建E-VOC语料库，通过大规模（超过165名评估者）人类主观评分结合客观声学指标，系统地评估五个代表性ITTS模型。 主要结果表明，gpt-4o-mini-tts是指令与感知对齐度最高的模型，但所有受测系统在年龄控制上均表现不佳，倾向于生成成人声音，且细粒度控制仍是重大挑战；例如，表现最佳的gpt-4o-mini-tts在说话人年龄识别任务上的最高F1分也仅为0.339。 对于AI从业者而言，这项研究提供了首个用于评估ITTS细粒度可控性的公开基准（E-VOC语料库及框架），并揭示了当前模型在年龄和重音控制等方面的关键技术瓶颈，为开发更具表现力的下一代语音合成系统指明了具体优化方向。

## 10.[Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided Role-playing Agents](https://arxiv.org/pdf/2509.15233)
summary:**角色扮演智能体**, **视频模态**, **动态角色画像**, **多模态数据集**, **自适应时序采样** 该论文通过整合视频模态，将动态角色画像引入角色扮演智能体（RPAs），并构建了包含6万个视频和70万段对话的Role-playing-Video60k数据集，提出了一套结合自适应时序采样与动态和静态角色画像表示的综合性框架，显著增强了RPA的响应生成能力。 该研究旨在解决现有角色扮演智能体方法主要依赖静态角色画像、忽视人类固有动态感知能力的局限性。 所提出的核心技术方法包括通过自适应时序采样视频帧并按时间顺序输入到大型语言模型（LLM）来创建动态角色画像；同时，通过训练视频中的角色对话（用于微调）和输入视频的总结上下文（用于推理）来构建静态角色画像，实现两者的联合集成。 实验结果证明了该框架的有效性，强调了动态角色画像在开发角色扮演智能体中的重要性，并证实该框架能够使角色扮演智能体生成更出色的响应。 对于AI从业者而言，这项工作提供了一种新颖的方法，通过整合动态视频信息来增强角色扮演智能体，为开发更具沉浸感和人性化的交互式AI角色开辟了道路。

## 11.[Audio-Conditioned Diffusion LLMs for ASR and Deliberation Processing](https://arxiv.org/pdf/2509.16622)
summary:**核心关键词** 扩散大语言模型, 自动语音识别, 审议式处理, 非自回归解码  **一句话核心总结** 本研究实证探索了基于扩散的大语言模型LLaDA在自动语音识别任务中的应用，通过将其作为外部审议模块和独立解码器进行评估，验证了其在提升识别精度和解码速度方面的潜力，并揭示了音频条件嵌入的关键作用。  **主要研究问题或目标** 该研究旨在系统性地探究基于扩散的大语言模型（DLLMs）作为传统自回归解码器替代方案的有效性，具体评估其在外部审议式后处理（refinement）和作为独立解码器直接生成文本两种模式下的性能表现。  **关键方法** 论文构建了一个Whisper-LLaDA模型，该模型集成了Whisper音频编码器与LLaDA解码器。在审议式处理模式下，该模型对基线ASR系统生成的文本进行部分掩码（masking），并利用LLaDA的双向去噪能力进行修正与完善。在独立解码器模式下，模型以非自回归或半自回归的方式，通过多步并行的去噪过程直接从音频特征生成完整文本序列。  **主要成果** 实验表明，作为审议模块时，该方法显著降低了识别错误率；在LibriSpeech test-other测试集上，最佳的级联审议系统实现了4.94%的词错误率（WER），相比强大的Whisper-LLaMA基线获得了12.3%的相对提升。作为独立解码器时，多数配置的推理速度快于基线，但准确率略有下降。此外，缺乏音频特征的纯文本LLaDA模型无法提升性能，证实了音频条件嵌入的必要性。  **对AI从业者的主要启示** 这项工作为AI从业者揭示了扩散模型在ASR领域的一个有效应用路径：即利用其强大的双向上下文理解和去噪能力，作为审议模块来优化现有高性能ASR系统的输出，从而在不改变主干模型的情况下进一步提升系统精度。这一发现对于需要极致识别准确率的应用场景具有直接的工程价值，提供了一种高效的性能增强方案。

## 12.[Towards Human-like Multimodal Conversational Agent by Generating Engaging Speech](https://arxiv.org/pdf/2509.14627)
summary:**多模态对话智能体**, **语音生成**, **副语言学**, **多感官对话数据集** 该研究通过构建一个名为MultiSensory Conversation (MSenC)的新型多模态对话数据集，并提出一个基于多模态大语言模型的框架，该框架能够生成文本响应和包含副语言信息的语音描述，从而开发出一个能生成更自然、更具吸引力语音回复的类人对话智能体。 该研究旨在解决当前多模态大语言模型主要集中于生成文本响应，而忽略了生成能反映对话情绪和上下文的自然、富有吸引力语音的问题。 其核心技术方法包括：首先，构建了一个包含约31,000个带有文本、视觉和音频数据的话语片段的MSenC数据集；其次，提出一个多模态模型架构，该架构采用Q-Former处理视频和音频输入，将其与文本信息一同送入一个大语言模型（LLM）主干；最后，通过指令微调（instruction tuning）训练LLM，使其不仅生成文本回复，还生成一段描述应如何发声的“语音描述”（voice description），该描述随后被送入文本转语音（TTS）模块以合成最终的语音。 实验结果表明，融合视觉和音频信息能显著提升响应质量；在MSenC数据集上，完整的“文本+音频+视频”多模态模型取得了14.12的ROUGE分数，显著优于仅使用文本模态基线的11.90分。 该研究为AI从业者提供了一个将副语言信息整合到对话系统中的实用框架，其核心价值在于提出了一种由LLM生成中间“语音描述”来指导TTS模块的解耦方法。这一架构使AI智能体（如虚拟助手、客服机器人）能够根据多模态输入（如用户表情和语气）动态调整其语音风格，从而实现更具同理心和更高质量的人机交互，对提升用户体验具有直接的应用价值。

## 13.[WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers](https://arxiv.org/pdf/2509.10452)
summary:**ASR**, **领域适应**, **纯文本**, **VAE**, **深度监督** 本文提出了WhisTLE，一种针对预训练编码器-解码器ASR模型进行深度监督的纯文本领域适应方法，通过训练变分自编码器（VAE）从文本中建模编码器输出并微调解码器，以在无额外运行时成本下有效提升模型在未见领域中的性能。 该研究旨在解决预训练自动语音识别（ASR）模型在面对包含未见词汇和口语的领域时，由于实际操作中难以收集语音数据而导致的纯文本领域适应问题。 WhisTLE通过训练一个变分自编码器（VAE）来直接根据文本输入建模ASR编码器的输出，并使用这个学习到的文本到潜在表示编码器（TLE）微调ASR解码器，该过程可选择性地与文本到语音（TTS）适应相结合。 实验结果表明，在四个域外数据集和四个ASR模型上，结合TTS的WhisTLE相较于仅TTS适应将词错误率（WER）平均降低了12.3%，并且在32种实验场景中的27种中优于所有非WhisTLE基线。 对于AI从业者而言，WhisTLE提供了一种无需收集目标领域语音数据即可对现有预训练ASR模型进行有效领域适应的深度监督方法，显著提高了模型在新领域（特别是新词汇和口语）的准确性，降低了资源消耗和部署成本，具有重要的实际应用价值。

## 14.[Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue](https://arxiv.org/pdf/2509.15061)
summary:**核心关键词**：具身智能体，指令歧义，多轮对话，知识绝缘训练  **一句话核心总结**：该研究提出了一种名为“Ask-to-Clarify”的框架，通过结合用于协作的视觉语言模型（VLM）和用于动作生成的扩散模型，并采用一种两阶段知识绝缘训练策略，使具身智能体能够在执行端到端动作前通过多轮对话主动澄清模糊指令，从而实现从被动执行者到主动协作者的转变。  **主要研究问题或目标**：本研究旨在解决现有基于VLA的具身智能体因采用单向执行模式、缺乏反馈机制，而在面对现实世界中常见的模糊指令时导致任务失败的问题，其核心目标是构建一个能够主动交互以解决指令歧义的协作式机器人系统。  **关键方法论**：该框架由一个用于对话协作的VLM和一个用于生成低层级动作的扩散模型组成，并通过一个连接模块相连，该模块根据指令调整视觉观测，为扩散模型生成更可靠的条件。其训练采用“两阶段知识绝缘策略”：第一阶段，使用对话数据微调VLM以获得歧义解决能力；第二阶段，冻结VLM以保留其对话能力，同时整合并微调扩散模型以学习具体动作执行，从而有效避免灾难性遗忘。  **主要成果**：在8个真实世界任务上的评估显示，该框架性能显著优于现有先进模型。具体地，该模型在首先处理模糊指令后，在“倒水”类任务中取得了98.3%的平均成功率，而在“堆叠物块”任务中也达到了90.0%的成功率，作为对比，多个基线模型即使在直接接收清晰指令的情况下也无法完成任务。  **对AI从业者的主要启示**：该研究提出的“知识绝缘训练策略”为AI工程师提供了一个在单一模型中融合语言交互和物理控制等异构能力的关键范例。这一策略通过在训练后期冻结已习得能力的模块，有效解决了在新技能学习中发生的灾难性遗忘问题，对于开发需要融合多种复杂技能（如对话、推理、控制）的鲁棒AI系统具有重要的实践指导意义。

