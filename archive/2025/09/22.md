

# Papers for 2025-09-22

## [ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data](https://arxiv.org/pdf/2509.15221)
summary:核心关键词: 计算机使用智能体, 跨平台数据, 视觉语言模型, 数据驱动缩放, 开源  本研究介绍了ScaleCUA，旨在通过构建跨平台大规模数据集和闭环数据管道训练，提升开源计算机使用智能体在多操作系统和任务领域的泛化操作能力，并取得了先进性能。该论文致力于解决开源计算机使用智能体在数据规模和模型泛化性上的限制，以实现通用跨平台CUAs的扩展。  其核心方法是建立一个跨平台交互式数据管道，结合自动化智能体与人类专家，覆盖Windows、macOS、Linux、Android、iOS和Web六大操作系统。此管道支持GUI理解、GUI基础和任务完成三大任务领域的数据收集、标注和增强，并设计了统一动作空间。基于此，团队训练了ScaleCUA系列模型，支持基础、直接动作和推理动作三种推理模式。  主要结果显示，ScaleCUA-32B模型在MMBench-GUI L1-Hard上达到了94.4%的最新SOTA性能，在OSWorld-G上取得了60.6%，在WebArena-Lite-v2上达到47.4%，相较基线有显著提升，例如WebArena-Lite-v2提升了26.6%，ScreenSpot-Pro提升了10.7%。  对于AI工程师和数据科学家，ScaleCUA的工作强调了数据驱动扩展在开发通用跨平台计算机使用智能体方面的巨大潜力，并通过开源高质量数据集、模型和代码，极大地降低了该领域的研究和开发门槛，有助于加速未来计算机自动化技术的发展。

## [FlowRL: Matching Reward Distributions for LLM Reasoning](https://arxiv.org/pdf/2509.15207)
summary:核心关键词: LLM推理, 强化学习, 奖励分布匹配, 流平衡, 多样性  FlowRL是一种大型语言模型(LLM)强化学习方法，通过流平衡匹配完整的奖励分布而非最大化奖励，以促进多样化探索和泛化推理轨迹。  当前奖励最大化方法(如PPO和GRPO)在LLM推理中倾向于过度优化主导奖励信号而忽略不频繁但有效的推理路径，从而降低多样性；本文旨在通过奖励分布匹配来解决此问题，提升探索效率和推理多样性。  FlowRL将标量奖励转换为一个带有可学习分配函数的归一化目标分布，并通过最小化策略与目标分布之间的反向KL散度来实现流平衡优化。为解决长CoT推理中的梯度爆炸和采样不匹配问题，FlowRL还引入了重要性采样和长度归一化。  实验结果表明，FlowRL在数学和代码推理任务上表现优异，在数学基准测试中，平均性能相较GRPO提高了10.0%，相较PPO提高了5.1%，并在代码推理任务上始终表现更好。多样性分析也证实FlowRL促进了更广泛的解决方案探索。  对于AI从业者，FlowRL强调奖励分布匹配是LLM强化学习中实现高效探索和多样化推理的关键一步，为提升大型语言模型的泛化能力和鲁棒性提供了新的优化范式。

## [Reasoning over Boundaries: Enhancing Specification Alignment via Test-time Delibration](https://arxiv.org/pdf/2509.14760)
summary:核心关键词：规约对齐、测试时审议(TTD)、安全-行为权衡、SPECBENCH基准  一句话核心总结：本文研究大型语言模型遵循场景化、动态的安全与行为规约的“规约对齐”挑战，并提出一种名为ALIGN3的轻量级测试时审议方法与SPECBENCH评估基准，以在不重新训练的情况下有效提升模型对复杂规约的遵循能力。  主要研究问题或目标：本文旨在解决大型语言模型在多样化现实场景中难以遵循由用户或组织定制的、动态变化的行为规约（behavioral-spec）和安全规约（safety-spec）的问题，即“规约对齐”挑战。  关键方法论：研究提出了一种名为ALIGN3的轻量级测试时审议（TTD）方法，它在单次推理过程中通过一个三步分层反思与修正流程来增强规约对齐。该流程包括：(1) 行为优化，首先专注于满足行为规约以确保帮助性；(2) 安全引导的精炼，引入安全规约调整推理链以规避风险；(3) 整体规约审计，在最终输出前进行全面检查与补全，以平衡帮助性和安全性。  主要成果：实验证明，测试时审议（TTD）能显著增强模型的规约对齐能力，而ALIGN3方法能以极小的开销提升安全-行为权衡边界。具体而言，在Qwen3-14B模型上，ALIGN3将规约对齐率（SAR）从51.03%提升至62.92%，实现了11.89%的改进。同时，新构建的SPECBENCH基准能有效揭示不同模型在规约对齐方面的差距。  对AI从业者的主要启示：对于AI工程师和应用开发者而言，本文提出的ALIGN3方法提供了一种成本效益高且无需重新训练的实用技术，用于在特定应用部署时增强模型对定制化操作指南（如企业安全策略、产品行为准则）的遵循能力。此外，SPECBENCH基准和SAR评估指标为评估和选择能够满足复杂、场景化业务规则的模型提供了具体的量化工具。

## [Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation](https://arxiv.org/pdf/2509.15194)
summary:大语言模型, 无标签强化学习, 熵坍缩, EVOL-RL, 新颖性奖励。 本文提出了EVOL-RL框架，通过结合多数投票进行选择和新颖性奖励促进变异，解决了大语言模型无标签强化学习中探索收缩和熵坍缩问题，显著提升了模型泛化能力。现有无标签学习方法（如置信度最小化、自洽性或多数投票）虽能稳定学习但会收缩探索空间，导致熵坍缩，降低模型多样性和泛化能力；该研究旨在使大语言模型在无标签设定下实现自主进化，在不牺牲探索能力和泛化能力的前提下持续自我提升。EVOL-RL通过多数投票将多数答案作为稳定锚点（选择），并引入新颖性奖励来鼓励生成与现有推理路径不同的响应（变异），新颖性通过语义空间差异衡量；该方法基于GRPO实现，并结合非对称裁剪和熵正则化来维持信号强度和搜索多样性。EVOL-RL成功阻止了多样性坍缩，维持了更长、信息量更大的思维链，并同时提高了pass@1和pass@n性能；例如，在无标签AIME24数据集上训练后，将Qwen3-4B-Base在AIME25上的pass@1从TTRL的4.6%提升到16.4%，pass@16从18.5%提升到37.9%。EVOL-RL为AI从业者提供了一种在无外部标签或评判者情况下，让大语言模型持续自主改进的鲁棒且实用的方法，对于实现现实世界中模型自我进化和泛化能力具有重要意义。

## [FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial Search and Reasoning](https://arxiv.org/pdf/2509.13160)
summary:核心关键词：金融搜索，智能体基准测试，端到端评估，时效性数据，知识推理  一句核心总结：该研究提出了首个开源的金融智能体基准FinSearchComp，通过模拟真实分析师工作流的三类任务来端到端地评估大语言模型在开放域金融搜索与推理方面的能力，为该领域提供了一个专业的高难度测试平台。  主要研究问题或目标：该研究旨在解决现有金融基准无法对大语言模型智能体进行端到端、开放域数据搜索和推理能力评估的问题，特别是处理金融领域中对时效性、准确性和多源信息整合要求极高的复杂任务。  关键方法论：研究团队构建了名为FinSearchComp的基准，其包含635个问题，分为时效性数据获取、简单历史查询和复杂历史调查三大任务。该基准由70名金融专家进行标注和多阶段质量审核，覆盖全球和泛中华区市场，并采用基于规则指导的LLM-as-a-Judge（LLM作为裁判）方法进行端到端评估，以应对答案的动态性和数值容错需求。  主要结果：在对21个模型产品的评估中，Grok 4 (web) 在全球子集上以68.9%的平均分领先，接近人类专家75.0%的水平，而DouBao (web)在泛中华区子集上表现最佳。实验明确指出，为智能体配备网络搜索和专用金融插件能显著提升性能，例如在时效性任务上，启用搜索的模型的平均得分比未启用搜索的模型高出40.8分。  对AI从业者的主要启示：该研究提供了一个用于评估和迭代金融LLM智能体的专业测试工具，并揭示了当前模型在深度搜索、信息时效性处理及多源证据整合方面的具体短板。研究结果强调，在开发面向金融等专业领域的智能体时，集成专用的数据插件（如金融API）比仅依赖通用网页搜索更为关键，这为提升智能体在实际应用中的可靠性和准确性提供了明确的技术优化方向。

## [Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation](https://arxiv.org/pdf/2509.15185)
summary:核心关键词：自回归图像生成，自监督学习，视觉表示学习，自引导训练 (ST-AR)  一句话核心总结：该研究提出了一种名为ST-AR的自引导训练框架，通过将掩码图像建模和对比学习等自监督目标融入传统的下一令牌预测范式，显著增强了自回归模型的视觉理解能力和图像生成质量。  主要研究问题或目标：本文旨在解决自回归模型应用于图像生成时，因其下一令牌预测范式所固有的三个问题——局部和条件依赖、跨步语义不一致、空间不变性缺失——而导致的学习高级视觉语义能力不足的问题。  关键方法论：研究提出的ST-AR框架在标准的自回归损失基础上，引入了三个自监督损失函数：1) 通过在Transformer的注意力图上进行随机掩码，并对齐师生网络特征的掩码图像建模损失(LMIM)，以扩大感受野，解决局部依赖问题；2) 引入跨步对比损失(Lstep)和跨视图对比损失(Lview)，分别确保同一图像在不同生成步骤间以及不同数据增强视图间的特征表示一致性，以解决语义不一致和不变性缺失问题。  主要成果：实验证明，ST-AR显著提升了模型的性能。在ImageNet数据集上，ST-AR使LlamaGen-XL模型在仅训练50个epoch的情况下，FID分数从19.42降低到9.81，实现了约49%的性能提升。同时，模型的线性探测准确率也从18.68%大幅提升至45.27%，证明了其视觉理解能力的增强。  对AI从业者的主要启示：该研究为AI工程师提供了一个有效的训练范式，证明了在不改变自回归模型核心架构和推理逻辑的前提下，仅通过在训练阶段融合自监督表示学习目标，就能有效克服其在视觉领域的内在局限性。这启示我们，将生成任务与显式的表示学习任务相结合，是构建更高效、更强大的生成模型的一个重要方向，尤其对于依赖序列预测的范式。

## [WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance](https://arxiv.org/pdf/2509.15130)
summary:核心关键词: 视频扩散模型, 免训练引导, 3D/4D生成, 轨迹控制, 潜空间融合  一句话核心摘要: 本文提出了WorldForge，一个免训练、推理时框架，通过引入三个新颖的引导模块，增强了视频扩散模型在3D/4D场景生成和重渲染中的精确轨迹控制能力和照片级真实感输出，且无需重新训练。  主要研究问题或目标: 本研究旨在解决当前视频扩散模型在3D/4D任务中存在的控制性有限和几何不一致问题，这些问题通常需要耗费计算资源的重训练或微调。  关键方法: WorldForge框架包含三个组件：1) **步内递归细化 (IRR)** 在每个去噪步骤中嵌入微观预测-校正循环，实现精确轨迹注入。2) **光流门控潜在融合 (FLF)** 利用光流相似性将轨迹信息选择性注入运动相关潜在通道，实现运动与外观的解耦。3) **双路径自校正引导 (DSG)** 比较引导和非引导去噪路径，自适应校正轨迹漂移和抑制伪影。  主要成果: 广泛实验验证了WorldForge的优越性，在真实感、轨迹一致性和视觉保真度方面均表现出色。例如，在静态3D场景生成中，WorldForge实现了0.077的绝对轨迹误差（ATE），优于现有最佳方法，如See3D的0.091和ViewCrafter的0.236。  对AI从业者的主要启示: WorldForge为AI从业者提供了一个即插即用、免训练的视频合成范式，使其能够利用预训练模型进行精确的摄像机轨迹控制和高质量的3D/4D场景生成与重渲染，显著降低计算成本并有效利用现有生成先验。

## [AToken: A Unified Tokenizer for Vision](https://arxiv.org/pdf/2509.14476)
summary:核心关键词：统一视觉分词器，4D潜空间，纯Transformer架构，无对抗训练，多模态  一句话核心概要：该研究提出了首个统一视觉分词器AToken，它通过一个基于纯Transformer架构和共享4D潜空间的单一框架，将图像、视频和3D资产等多样化视觉输入进行编码，首次同时实现了高保真重建和高层语义理解。  主要研究问题或目标：该研究旨在解决现有视觉表征的碎片化问题，即当前视觉分词器通常只能针对单一模态（如仅图像）并专攻于单一任务（重建或理解），无法像语言模型一样形成统一的表示空间。其目标是创建一个能跨图像、视频、3D资产三种模态，并同时支持生成（重建）和理解（分类、检索）双重任务的通用视觉分词器。  关键方法论：AToken的核心方法论包含四个关键部分：1）**统一4D表示**：引入一个稀疏的4D潜空间（时间t, 空间x, y, z），将图像视为t=z=0的2D切片，视频视为z=0的时间序列堆栈，3D资产视为t=0的体素，从而统一了不同模态的输入格式。2）**纯Transformer架构**：采用基于4D旋转位置编码（RoPE）的纯Transformer架构，使其能够原生处理任意分辨率和时长的视觉输入。3）**无对抗训练**：为保证训练稳定性和重建质量，设计了一种无对抗的训练目标，该目标结合了感知损失（perceptual loss）和格拉姆矩阵损失（Gram matrix loss），直接优化特征的二阶统计量（如纹理和风格）。4）**渐进式训练课程**：采用四阶段的渐进式课程学习策略，从预训练的图像理解模型（SigLIP2）开始，逐步增加图像重建、视频、3D资产的能力，并最终可选地加入离散量化功能。  主要结果：AToken在多个基准测试中取得了具有竞争力的性能，证明了其统一能力的有效性。具体量化结果包括：在图像任务上，达到了0.21 rFID的重建保真度和82.2%的ImageNet零样本分类准确率；在视频任务上，达到了3.01 rFVD和40.2%的MSRVTT检索R@1；在3D任务上，实现了28.28 PSNR和90.9%的分类准确率。实验证明，多模态联合训练甚至能提升单模态的性能（例如，加入视频和3D数据后，图像重建rFID提升了19%）。  对AI从业者的主要启示：AToken为AI从业者提供了一个通用的、即插即用的视觉“基座模型”或编码前端。开发者无需再为图像、视频、3D等不同模态或为生成、理解等不同任务分别寻找和集成专用的编码器。该模型作为一个统一的接口，可以简化多模态大语言模型（MLLM）、文生视频、图像到3D合成等复杂系统的构建，促进了跨视觉模态的知识迁移和泛化能力，为构建下一代统一多模态AI系统奠定了基础。其最具影响力的发现是，一个模型可以同时擅长细节重建和语义理解，打破了两者不可兼得的传统困境。

## [RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation](https://arxiv.org/pdf/2509.15212)
summary:核心关键词：视觉-语言-动作模型, 人类演示, 视频生成预训练, 轨迹感知建模, ActionVAE  一句话核心总结：该研究提出了一种名为RynnVLA-001的视觉-语言-动作模型，其核心方法是利用大规模人类第一视角视频进行分阶段生成式预训练，从而将人类操作技能迁移至机器人，显著提升了机器人的操作性能。  主要研究问题或目标：旨在解决机器人操作领域中大规模高质量训练数据稀缺的核心难题，通过设计一种有效的预训练策略，将海量的人类第一视角视频中蕴含的操作知识迁移到视觉-语言-动作（VLA）模型，从而为机器人模型提供更优的初始化权重，提升其在下游任务中的性能。  关键方法论：该模型采用了一个三阶段的渐进式训练流程。第一阶段为“以自我为中心的视频生成预训练”，在大约1200万条人类第一视角操作视频上训练一个图像到视频（I2V）模型，使其根据初始帧和语言指令预测未来视频帧，从而学习物理世界的操作动态。第二阶段为“以人为中心的轨迹感知建模”，在第一阶段模型基础上，联合预测未来视频帧和人类手部关键点轨迹，建立视觉变化与底层动作之间的联系。第三阶段为“以机器人为中心的视觉-语言-动作建模”，将模型迁移到机器人数据上，并引入一个名为ActionVAE的变分自编码器将动作序列压缩为紧凑的潜在嵌入，模型最终在机器人数据上微调以预测该动作嵌入。  主要成果：实验结果表明，RynnVLA-001模型性能优于当前最先进的基线模型。在使用LeRobot SO100机械臂收集的真实世界操作任务数据集上，RynnVLA-001在三个任务上的平均成功率达到了**90.6%**，显著高于GR00T N1.5（55.6%）和Pi0（70.4%）等模型。消融实验也证实，从视频生成预训练到轨迹感知建模的每个阶段都对最终性能有明显的提升作用。  对AI从业者的主要启示：该研究为机器人模型的训练提供了一个有效且数据高效的范式，即利用易于获取的大规模人类视频数据来替代昂贵且稀缺的机器人专用数据进行预训练。AI工程师和研究者可以借鉴这种从通用视频预测到具体轨迹建模的多阶段预训练策略，为自己的机器人VLA模型构建更强大的基础，从而减少对特定机器人数据的依赖，并开发出更具泛化能力的机器人智能体。其中ActionVAE的应用也为动作表征和分块预测提供了一个鲁棒的解决方案，有助于提升推理效率和动作的连贯性。

## [MultiEdit: Advancing Instruction-based Image Editing on Diverse and Challenging Tasks](https://arxiv.org/pdf/2509.14638)
summary:Core Keywords: 指令引导图像编辑 (IBIE), MultiEdit, 多模态大语言模型 (MLLM), 数据集, 风格迁移  MultiEdit引入了一个包含107K+高质量图像编辑样本的综合数据集，旨在解决现有指令引导图像编辑（IBIE）方法在处理复杂、多样化编辑任务时的数据集局限。该研究通过新颖的多模态大语言模型（MLLM）驱动管道构建数据集，利用SOTA MLLM生成视觉自适应编辑指令，并使用SOTA ImageGen生成高保真编辑图像，从而避免了传统基于文本描述方法的潜在噪声和偏差。MultiEdit数据集涵盖6类挑战性编辑任务，包括18种非风格迁移编辑类型和38种风格迁移操作，覆盖了从复杂语义操作到精细化风格迁移的广泛范围。实验证明，利用MultiEdit-Train数据集对基础开源模型（如SD3和UltraEdit）进行微调，能显著提升其在MultiEdit-Test基准测试上的复杂编辑任务性能，例如SD3模型的CLIPimg和DINO分数分别提高了约9.4%和16.1%。此外，任务加权数据采样策略进一步优化了模型性能，UltraEdit在DINO分数上超越了SOTA模型Step1X-Edit。MultiEdit为AI从业者提供了宝贵的资源，以促进更强大、更通用的IBIE模型的开发，使其能更有效地处理多样和挑战性的图像编辑场景。

## [Apertus: Democratizing Open and Compliant LLMs for Global Language Environments](https://arxiv.org/pdf/2509.14233)
summary:核心关键词：数据合规性，多语言大模型，完全开源，记忆抑制  一句核心总结：该研究发布了完全开源的Apertus大语言模型套件（8B和70B），通过在15T合规多语言数据上采用Goldfish目标函数进行预训练，旨在解决当前开源生态中的数据合规性与多语言覆盖不足两大系统性缺陷。  主要研究问题或目标：该研究旨在开发一个既强大又合规的大语言模型，以解决现有开源模型普遍存在的数据来源不透明、侵犯内容所有者权利以及多语言（尤其是低资源语言）代表性不足的问题。  关键方法论：Apertus模型套件的开发基于三大技术支柱：首先，在数据层面，模型仅在公开可用的数据源上进行预训练，并首次大规模地追溯性遵循`robots.txt`的排除协议，同时过滤了个人身份信息（PII）和有毒内容；其次，在训练目标上，采用Goldfish目标函数，通过选择性地掩码输入token来显著抑制模型对训练数据的逐字记忆，降低隐私和版权风险；最后，在模型和训练架构上，采用了包括xIELU激活函数和AdEMAMix优化器在内的技术创新，以确保在高达4096个GPU上的大规模训练的稳定性和效率，并完整开源所有代码、数据处理脚本和模型检查点。  主要成果：Apertus在8B和70B规模上均达到了领先的性能，尤其在多语言基准测试中，其表现可与甚至超越了同规模的仅开放权重的模型。例如，在多语言常识推理任务XCOPA上，Apertus-70B模型取得了69.8%的准确率，显著优于其他同类完全开源模型。实验证明，Goldfish目标函数在抑制数据记忆的同时，并未损害模型的下游任务性能。  对AI从业者的主要启示：该研究为AI从业者提供了一个高性能、高度多语言（覆盖1800+语言）且数据来源完全合规、可复现、可审计的基础模型。这意味着开发者可以在一个坚实且法律风险较低的基础上构建全球化的商业应用，无需担心因训练数据版权问题而引发的法律纠纷，这对于在受监管行业或面向全球市场开发AI产品的团队尤其重要。

## [Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding](https://arxiv.org/pdf/2509.15178)
summary:核心关键词：时空视频定位，多模态大语言模型 (MLLM)，零样本，分解式时空高亮 (DSTH)  一句话核心总结：该研究提出了一种基于多模态大语言模型的零样本时空视频定位框架，通过创新的分解式时空高亮（DSTH）和时间增强组装（TAS）策略，在无需训练的情况下提升了模型对文本查询的定位精度。  主要研究问题或目标：本文旨在解决如何利用现有多模态大语言模型的内在能力，实现无需任务特定训练的零样本时空视频定位（STVG），同时克服模型在整合文本查询中细粒度属性和动作线索时的次优定位问题。  关键方法论：核心方法是分解式时空高亮（DSTH）策略。首先，该策略将原始文本查询分解为独立的属性（空间）和动作（时间）子查询。然后，引入一个新颖的对数引导重注意力（LRA）模块，通过在测试时优化可学习的视觉提示来调整模型对这些子查询的响应，从而增强模型对特定空间属性和时间动作相关视觉区域的关注。此外，结合时间增强组装（TAS）策略，通过处理原始和时间增强的视频帧来提升空间定位的时间一致性。  主要成果：实验结果表明，该框架在三个主流STVG基准测试中均优于现有的SOTA方法。具体而言，在HC-STVGv1数据集上，结合LLaVA-OneVision-7B模型，该方法在m_vIoU指标上达到了24.8%，显著超过了先前零样本SOTA方法E3M的19.1%。  对AI从业者的主要启示：该研究为AI从业者提供了一种无需微调、成本效益高的利用现有大型基础模型解决复杂视频理解任务的范式。其核心价值在于展示了如何通过巧妙的测试时提示工程（将复杂查询分解并引导模型注意力）来解锁和引导MLLMs的细粒度推理能力，这对于在缺乏大量标注数据的场景下快速部署视频分析应用具有重要的实践指导意义。

## [RecoWorld: Building Simulated Environments for Agentic Recommender Systems](https://arxiv.org/pdf/2509.10397)
summary:i) **Core Keywords**: 智能体推荐系统、模拟环境、多轮交互、大语言模型、用户留存  ii) **1-Sentence Core Summary**: RecoWorld提出一个用于智能体推荐系统的双视图模拟环境蓝图，通过模拟用户与智能体推荐系统的多轮交互，利用LLM的推理能力优化用户留存，为推荐策略的迭代提供了无风险的训练空间。  iii) **Main Research Question or Objective**: 该研究旨在为智能体推荐系统构建模拟环境，以在不影响真实用户的情况下，提供一个学习和完善推荐策略的训练空间，并探索用户与智能体推荐系统协同塑造个性化信息流的新交互范式。  iv) **Key Methodology**: RecoWorld采用双视图架构，通过模拟用户与智能体推荐系统的多轮交互来最大化用户留存；其中模拟用户生成反思性指令，智能体推荐系统则利用大语言模型的推理能力整合指令和推理轨迹动态调整推荐。该框架支持文本、多模态及语义ID等多样化内容表示，并通过多轮强化学习迭代优化策略，并可扩展至多智能体模拟。  v) **Primary Results**: 本论文未提供RecoWorld的实验结果，但阐述了评估设计，提出通过模拟用户与人类标注者之间会话级交互统计数据进行对比，并利用现有推荐数据集进行用户模拟器评估。  vi) **Principal Implication for AI Practitioners**: 对于AI从业者而言，RecoWorld提供了一个在安全、受控环境中开发和测试智能体推荐系统的框架，规避了直接在真实用户上实验的风险，加速了基于LLM的推荐系统策略迭代和优化，尤其是在用户留存和动态反馈方面。

## [Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization](https://arxiv.org/pdf/2509.09307)
summary:核心关键词：多模态大语言模型（MLLMs），材料表征，基准测试，MatCha，性能差距  一句话核心总结：该研究提出了首个用于评估多模态大语言模型（MLLMs）在材料表征图像理解能力的基准测试MatCha，通过全面的实验揭示了当前模型相较于人类专家在专业知识和视觉感知方面存在的显著性能差距。  主要研究问题或目标：旨在系统性地评估当前先进的多模态大语言模型（MLLMs）在理解真实世界材料科学研究中的复杂表征图像时的能力，特别是在处理需要深层领域知识和高级视觉推理的任务时的性能局限。  关键方法论：研究团队构建了名为MatCha的基准测试，包含1500个专家级的多项选择题。其构建流程为：1) 与材料科学家合作，依据“加工-形貌-结构-性质”的科研流程设计了4个阶段、21个具体的子任务；2) 从Nature平台下的科学文献及补充数据集中搜集和处理真实的材料表征图像、图注及相关文本；3) 利用GPT-4o生成与任务相关的视觉问答（VQA）题目，并经过AI和人类专家的双重过滤与审查以确保问题的挑战性和真实性；4) 在此基准上，对包括GPT-4o、Gemini-1.5-Pro等在内的多种闭源和开源MLLMs进行了零样本、少样本及思维链（CoT）等多种设置下的综合性能评测。  主要结果：实验表明，所有被测MLLM与人类专家之间存在巨大性能鸿沟。在生成式VQA子集上，表现最好的模型GPT-4o准确率仅为62.58%，与人类专家的88.87%相差26.29%。随着任务从基础的形态学分析升级到需要更深层知识的结构与性质分析，模型的性能普遍出现显著下降。此外，少样本学习和思维链提示等简单方法难以有效弥补这一差距，显示出当前模型在适应真实世界材料科学场景中的内在局限性。  对AI从业者的主要启示：该研究为AI从业者提供了一个评估模型在专业科学领域（特别是材料科学）视觉理解能力的标准化工具（MatCha）。研究结果明确指出，当前通用的MLLMs在处理需要精细视觉感知和深度领域知识的科学图像时能力严重不足，直接应用到自主科学发现或新材料研发等高要求场景中是不可靠的。这凸显了为科学应用开发模型时，必须超越通用预训练，转向结合领域特异性数据、多模态知识对齐以及更复杂的推理机制（如检索增强生成RAG）进行针对性优化。

## [Agentic Software Engineering: Foundational Pillars and a Research Roadmap](https://arxiv.org/pdf/2509.06216)
summary:核心关键词：智能体软件工程 (Agentic SE), 结构化智能体软件工程 (SASE), 人机协作, 智能体命令环境 (ACE), 智能体执行环境 (AEE)。  一句话核心摘要：本文提出结构化智能体软件工程 (SASE) 愿景，旨在通过定义人与智能体的双重模式、专用工作台、结构化工件和工程活动，将智能体编码提升为可信赖、可扩展且规范化的智能体软件工程实践，以实现复杂的、面向目标的软件工程目标。  主要研究问题或目标：本研究旨在解决智能体软件工程 (SE 3.0) 时代中，智能体虽然具有高生产力，但其生成的代码在合并就绪度方面存在“速度与信任”差距，以及人机协作缺乏结构化、可追溯和可信赖机制的问题。  核心方法论：SASE核心方法论建立在“SE for Humans” (SE4H) 和 “SE for Agents” (SE4A) 的结构化二元性之上，提出了供人类“智能体教练”编排和审查的智能体命令环境 (ACE) 和供智能体执行任务并主动寻求人类专业知识的智能体执行环境 (AEE) 两种专用工作台，并定义了BriefingScript、LoopScript、MentorScript等结构化、版本控制的工件，以及Agentic Guidance Engineering (AGE) 等工程活动以实现人机双向协作。  主要研究成果：本文提出了结构化智能体软件工程 (SASE) 框架作为核心成果，概述了其基础支柱和研究路线图，旨在解决现有智能体解决方案的局限性，例如GPT-4补丁在经过详细人工审计后，其真实解决率从12.47%下降至3.97%，表明当前智能体生成代码的合并就绪度不足。  对AI从业者的主要启示：对于AI从业者而言，SASE框架将人类角色从编码者提升为“智能体教练与协调者”，通过提供结构化、可追溯的机制，支持N对N的人机协作，显著提升软件工程的生产力（潜在达到100倍甚至1000倍），并确保智能体生成的软件具有更高可信度和可维护性，推动行业从零散的智能体编码走向成熟的工程实践。

## [Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question Answering with LLMs](https://arxiv.org/pdf/2509.15020)
summary:大语言模型 (LLMs), 多项选择问答 (MCQA), 分词 (Tokenization), 准确率 (Accuracy), 校准 (Calibration) 本文研究了大语言模型在多项选择问答（MCQA）任务中，提示符末尾“Answer:”后空格的分词方式对模型性能和可靠性的影响，并发现了一种显著提升准确率和校准度，且可改变模型排名的优化策略。论文旨在探究LLMs在MCQA评估中，“Answer:”后空格字符的分词方式（独立分词或与答案字母一同分词）如何影响模型性能及其评估结果的可靠性。研究通过对比两种分词策略——将空格独立分词（“χ”）或将空格与答案字母一起分词（“_X”），评估了15个LLMs在MMLU及其他五个MCQA数据集上的表现，采用准确率和预期校准误差（ECE）进行性能和可靠性评估。实验发现，将空格与答案字母一同分词的策略能带来持续且统计学显著的性能提升，其中准确率最高可提高11%。此外，该策略还能显著改善模型校准，并导致模型排行榜发生变化，例如Llama 3.1 70B Instruct和Qwen 2.5 72B的模型排名互换。对于AI从业者而言，这项研究强调了在设计LLMs评估协议时，即使是微小的分词细节也至关重要，并提供了一个具体的实践建议：在多项选择问答中，将“Answer:”后的空格与答案字母一同分词可以一致地提升模型准确率和置信度校准，从而提高评估的公平性和结果的可靠性。

## [EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence](https://arxiv.org/pdf/2509.14977)
summary:超声影像, 视觉语言模型, 专家混合模型, 诊断准确性 EchoVLM是一个为超声影像诊断专门设计的视觉语言模型，通过动态专家混合架构和多任务学习，显著提升了超声报告生成和诊断的准确性，为临床应用提供了可行方案。 该研究旨在解决现有通用视觉语言模型在超声医学任务中知识有限、多器官病变识别泛化能力差、以及多任务诊断效率低的问题，以提升超声影像诊断的准确性和效率。 EchoVLM模型采用了动态专家混合（MoE）架构，该架构在涵盖七个解剖区域的数据上进行训练，并专门用于超声医学影像。此设计使其能够执行多种任务，包括超声报告生成、诊断和视觉问答（VQA）。 实验结果表明，在超声报告生成任务中，EchoVLM在BLEU-1分数上相比Qwen2-VL提高了10.15分，在ROUGE-1分数上提高了4.77分，取得了显著提升。 EchoVLM的发现对AI从业者具有重要意义，它展示了通过领域专用视觉语言模型结合MoE架构，可以显著提高医学影像诊断的准确性，为开发更高效、更具泛化能力的临床AI解决方案提供了技术基础。

## [EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing](https://arxiv.org/pdf/2509.13399)
summary:核心关键词：多轮图像编辑, 评估框架, 目标中心, 视觉语言模型 (VLM), 指令遵循。  该论文提出EdiVal-Agent，一个自动化、可扩展、细粒度的目标中心多轮图像编辑评估框架，旨在通过整合VLM与专业工具，实现对编辑模型指令遵循、内容一致性和视觉质量的全面评价。该研究旨在解决指令驱动图像编辑中，尤其多轮编辑场景下，缺乏可靠且可解释的评估方法这一瓶颈问题。EdiVal-Agent框架首先将图像分解为语义对象并生成上下文感知的多轮编辑指令；随后，通过整合VLM与开放词汇对象检测器评估指令遵循，利用语义级特征提取器评估内容一致性，并使用人类偏好模型判断视觉质量。实验结果显示，结合VLM与对象检测器的方法在指令遵循评估上与人类判断的一致性达到81.3%，显著优于单独使用VLM（75.2%）和CLIP-based指标（65.4%）。此外，EdiVal-Agent能够有效识别现有模型的失败模式，例如Nano Banana在指令遵循和内容一致性之间表现最佳。这项工作为AI从业者提供了一个标准化的评估工具，有助于识别多轮图像编辑模型的具体弱点，从而指导下一代编辑模型在上下文连贯性、指令遵循准确性及视觉质量方面的开发和改进。

## [Developer-LLM Conversations: An Empirical Study of Interactions and Generated Code Quality](https://arxiv.org/pdf/2509.10402)
summary:Core Keywords: 开发者-LLM对话, 代码质量, 经验研究, 对话动力学, 代码生成 本研究利用CodeChat大规模数据集，实证分析了开发者与大型语言模型（LLM）的交互模式及其生成的代码质量，旨在深入理解LLM在软件开发工作流中的实际应用和影响。论文旨在探究开发者在实际中如何与LLM交互，以及这些对话动态如何影响任务结果、代码质量及软件工程工作流。研究基于CodeChat数据集（包含82,845个开发者-LLM对话及368,506个代码片段），通过分析对话结构、开发者行为模式和LLM生成的代码质量，进行定量和定性评估。LLM响应显著长于开发者提示，中位数令牌长度比为14:1；多轮对话占68%，常因需求变化、提示不完整或澄清请求而演变。分析显示，网页设计（9.6%）和神经网络训练（8.7%）是最常见的LLM辅助任务。LLM生成的代码普遍存在语言特异性缺陷，例如Python代码83.4%和JavaScript代码75.3%包含未定义变量，Java代码75.9%缺少必需注释。对话中，Java文档质量可提高达14.7%，Python导入处理可提高3.7%（5轮后）。对AI工程师而言，这些发现表明亟需提升LLM代码生成质量和对话理解能力，加强LLM辅助软件开发工作流中的系统质量控制和工具支持。

## [FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution Remote Sensing Change Detection](https://arxiv.org/pdf/2509.06482)
summary:核心关键词: 遥感变化检测, 频率-空间协同, 伪变化抑制, 语义鸿沟, 门控融合 一句话核心总结: 本文提出FSG-Net，通过频率域处理抑制伪变化、空间域增强真实变化区域，并利用门控融合单元弥合语义鸿沟，旨在提升高分辨率遥感图像变化检测的准确性。  该研究旨在解决高分辨率遥感图像变化检测中，由辐射变化引起的伪变化和深层抽象特征与浅层细节特征之间的语义鸿沟导致边界模糊的问题。FSG-Net采用频率-空间协同策略，首先通过差异感知小波交互模块（DAWIM）在频率域自适应处理不同频率分量以减轻伪变化；随后，协同时空注意力模块（STSAM）在空间域增强真实变化区域的显著性；最后，轻量级门控融合单元（LGFU）利用高层语义选择性地整合浅层细节，以弥合语义鸿沟。FSG-Net在CDD、GZ-CD和LEVIR-CD基准测试中取得了最先进的性能，F1-score分别达到94.16%、89.51%和91.27%。FSG-Net的频率-空间协同和门控融合机制为AI从业者提供了处理复杂遥感变化检测中伪变化和语义对齐挑战的有效范式，有助于开发更鲁棒、边界更清晰的模型。

