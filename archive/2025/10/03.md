

# Papers for 2025-10-03

## 0.[DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search](https://arxiv.org/pdf/2509.25454)
summary:**可验证奖励强化学习 (RLVR)**, **蒙特卡洛树搜索 (MCTS)**, **大型语言模型 (LLMs)**, **数学推理**, **系统探索**  本文提出了DeepSearch，一个将蒙特卡洛树搜索（MCTS）直接集成到可验证奖励强化学习（RLVR）训练中的框架，旨在通过系统探索克服LLM推理能力中的RLVR训练瓶颈，并以更低的计算成本实现最先进的性能。当前，大型语言模型（LLMs）在可验证奖励强化学习（RLVR）训练中面临性能瓶颈，表现为训练后期性能提升显著下降，这源于稀疏的探索模式，模型难以系统性覆盖解决方案空间。本文旨在解决这一问题，通过在训练阶段引入系统性探索来提高LLMs的推理能力。DeepSearch将MCTS直接嵌入RLVR训练循环中，而非仅在推理时使用。其核心技术包括：(1) 一种全局前沿选择策略，优先考虑整个搜索树中有潜力的节点；(2) 结合熵的指导选择，识别用于监督的置信路径；(3) 结合解决方案缓存的自适应重放缓冲区训练以提高效率。这种方法实现了跨推理步骤的系统探索和细粒度信用分配。DeepSearch在数学推理基准测试中取得了62.95%的平均准确率，为1.5B推理模型树立了新的SOTA，相比现有最佳方法，性能提升了1.25个百分点，同时将GPU小时数减少了5.7倍。这些结果强调了战略性探索而非蛮力扩展在推进RLVR方法论中的重要性，为AI从业者提供了通过系统搜索而非长时间计算来提升推理能力的新方向。

## 1.[GEM: A Gym for Agentic LLMs](https://arxiv.org/pdf/2510.01051)
summary:**Agentic LLMs, 强化学习, 环境模拟器, GEM, 返回批标准化** GEM (General Experience Maker)是一个开源环境模拟器，旨在为LLMs从静态数据集训练转向基于经验的学习提供标准化环境-智能体接口、多样化环境套件和集成工具，并通过引入带返回批标准化（ReBN）的REINFORCE算法，加速了智能体LLM的研究。 论文旨在解决大型语言模型（LLMs）向基于经验的学习范式转变中，缺乏标准化、多样化且支持多回合交互的训练与评估环境的问题，以加速智能体LLM研究。 GEM提供一个遵循OpenAI Gym API的标准化环境-智能体接口，支持异步向量化执行以实现高吞吐量和灵活的封装器，并包含游戏、推理、编程、数学及问答等五大类任务的24个多样化环境，集成了Python、搜索和MCP等工具。此外，论文提出并采用了带返回批标准化（ReBN）的REINFORCE算法作为基线，该算法兼容多回合RL设置下的密集每步奖励和任意折扣因子。 实验结果表明，ReBN在所有评估环境中均能持续显著提升香草REINFORCE的性能，并与PPO及GRPO表现相当或更优。例如，在RL2框架中启用异步rollout可将挂钟效率提高2倍。 GEM为AI从业者提供了一个统一、解耦且支持多框架集成的智能体LLM训练与评估基础设施，显著降低了环境开发与设置的复杂性。这使得研究人员能更便捷地进行算法设计、快速原型开发和新思想评估，尤其在处理多回合、工具集成和多智能体交互等复杂场景时。

## 2.[VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators](https://arxiv.org/pdf/2510.00406)
summary:**VLA模型**, **强化微调**, **世界模型**, **验证奖励**, **GRPO优化** VLA-RFT提出了一种利用数据驱动世界模型作为可控模拟器、通过强化学习微调VLA模型（Vision-Language-Action）的框架，旨在解决模仿学习的局限性以及传统强化学习的成本和模拟-现实差距，从而实现高效、鲁棒的VLA模型泛化和部署。  该研究旨在解决VLA模型在模仿学习下易产生复合误差和鲁棒性差的问题，同时规避传统强化学习（RL）所需昂贵真实世界交互或模拟-现实鸿沟的挑战。  VLA-RFT通过将数据驱动的世界模型用作可控模拟器，预测基于动作的未来视觉观测，从而允许策略在模拟器中进行多步推演；系统通过与目标参考的视觉轨迹对比生成密集的、轨迹级别的验证奖励信号，并利用GRPO优化框架对VLA策略进行端到端更新。  VLA-RFT在少于400步的微调迭代下，显著超越了强大的监督基线并比基于模拟器的RL更高效，例如在LIBERO标准套件上将平均成功率从86.6%提升至91.3%（增加4.7个百分点），同时在扰动条件下表现出强大的鲁棒性，维持稳定的任务执行。  VLA-RFT为AI从业者提供了一个高效且实用的后训练范式，利用世界模型显著降低了VLA模型的样本需求，同时提升其泛化能力和在复杂、扰动环境下的鲁棒性，从而加速VLA模型在实际机器人应用中的部署。

## 3.[Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation](https://arxiv.org/pdf/2509.25849)
summary:**核心关键词**： 强化学习, 预算分配, 背包问题, 探索效率  **一句话核心摘要**： 该研究通过将大语言模型强化学习中的探索预算分配问题建模为经典的背包问题，提出了一种名为Knapsack RL的方法，在不增加额外计算资源的情况下，通过自适应地分配探索预算，显著提升了训练效率和模型在数学推理等任务上的性能。  **主要研究问题或目标**： 本研究旨在解决大语言模型在使用强化学习（特别是GRPO算法）进行自优化时，因采用统一探索预算分配策略而导致的计算资源浪费问题，即简单任务探索过度、困难任务探索不足，从而产生大量零梯度并阻碍模型训练。  **关键方法论**： 该方法将每个任务的探索过程视为一个具有“成本”和“价值”的物品，以构建背包问题模型进行优化。具体而言，分配给任务的探索轨迹数（rollouts）被定义为“成本”，而“价值”则由产生非零梯度的概率与一个信息增益项的乘积来量化。系统根据模型在上一轮训练中的成功率动态估算每个任务的价值，并使用动态规划求解该背包问题，从而在固定的总计算预算内为不同难度的任务分配最优的探索资源。  **主要成果**： 实验表明，该方法能将有效非零梯度的比例提升20-40%。在多个数学推理基准测试中，模型性能平均提升2-4个百分点，在特定任务上峰值提升达到9个百分点。与传统的均匀分配策略相比，Knapsack RL能以大约一半的计算资源达到同等的模型性能水平。  **对AI从业者的主要启示**： 对于AI工程师和研究者而言，这项工作提供了一种“计算免费午餐”，即无需增加硬件投入即可显著提升大模型强化学习效率和最终模型性能的实用方法。它证明了智能地动态再分配现有计算资源，是比单纯增加总资源更具成本效益的优化路径，这一思想可为其他计算密集型AI任务的资源调度提供借鉴。

## 4.[SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights](https://arxiv.org/pdf/2509.22944)
summary:**核心关键词**：量化, 双轴缩放, 矩阵不平衡度, Sinkhorn-Knopp算法  **1-Sentence Core Summary**：该论文提出了一种名为SINQ的免校准训练后量化方法，它通过为权重矩阵增加一个第二轴缩放因子，并使用一种快速的Sinkhorn-Knopp风格算法来归一化行和列的方差，从而最小化一个名为“矩阵不平衡度”的新颖代理目标，并显著改善了低精度大语言模型的困惑度。  **Main Research Question or Objective**：解决低精度（≤4-bit）训练后量化中，因离群值导致共享缩放因子的参数精度下降，进而引发模型性能（如困惑度）显著退化的问题，尤其是在免校准的均匀量化场景中。  **Key Methodology**：核心方法SINQ引入了一种“双轴缩放”参数化方案，即对每个待量化的矩阵块同时使用行缩放因子和列缩放因子。为确定最优的缩放因子，该方法提出了一个名为“矩阵不平衡度”的代理指标（定义为所有行和列标准差的最大值与最小值之比），并采用一个修改版的Sinkhorn-Knopp迭代算法，通过交替归一化行和列的标准差来最小化该指标。该方法无需校准数据，且各层之间独立处理。  **Primary Results**：实验表明，SINQ在多个模型和数据集上均优于基线方法。例如，在Qwen3-14B模型的3-bit量化任务中，SINQ在WikiText2数据集上取得了9.33的困惑度，显著优于RTN基线的10.50和HQQ基线的10.73。  **Principal Implication for AI Practitioners**：对于AI从业者，SINQ提供了一种快速、简单且无需校准数据的权重纯量化增强技术。由于其架构无关和层间独立的特性，工程师可以轻松地将其应用于现有的量化流程中，以更低的性能损失将大模型部署到资源受限的环境中，尤其适用于需要快速量化而无法承担校准开销的场景。

## 5.[PIPer: On-Device Environment Setup via Online Reinforcement Learning](https://arxiv.org/pdf/2509.25455)
summary:**核心关键词**：环境设置, 强化学习, 在端模型, 可验证奖励  **一句话核心摘要**：该研究针对自动化软件环境设置难题，提出一种结合监督微调与可验证奖励强化学习（RLVR）的训练方法PIPER，使小型在端模型Qwen3-8B能够达到与Qwen3-32B及GPT-4o等大型模型相媲美的性能。  **主要研究问题**：旨在解决现有大语言模型在自动化配置复杂软件项目环境任务上成功率有限的问题，特别是探索如何有效提升小型、可在消费级硬件上运行的开源模型的性能，使其在该任务上更具实用性和成本效益。  **关键方法**：该研究提出名为PIPER的两阶段训练流程。第一阶段，采用监督微调（SFT）技术，通过从更强大的教师模型（Qwen3-32B）生成的高质量Bash脚本中进行知识蒸馏，教导基础模型生成正确的脚本。第二阶段，应用带可验证奖励的强化学习（RLVR），设计了一个轻量级的LLM-as-a-Judge代理奖励函数，该函数无需真实执行即可预测脚本的退出码和导入问题数量，以此指导模型进行策略优化，规避了大规模容器化执行的高昂成本。  **主要成果**：实验结果表明，在EnvBench-Python基准测试上，经过PIPER方法训练的Qwen3-8B模型（80亿参数）取得了27个成功样本的`pass@5`分数，这一成绩与参数量远大于它的Qwen3-32B（29个）和GPT-4o（29个）表现相当，并显著优于其8个成功样本的基础模型性能。  **对AI从业者的主要启示**：该研究为AI从业者提供了一种经济高效的范式，用以在复杂的软件工程自动化任务上特化小型开源模型。其最具影响力的发现是，使用LLM-as-a-Judge代理奖励的RLVR流程，为在需要昂贵环境交互的任务中应用强化学习提供了一条可行的低成本路径。AI工程师可借鉴此SFT与RLVR相结合的策略，在其他特定领域的代码生成或工具使用任务中训练出高性能的小型专用模型，从而减少对大型闭源API的依赖并实现本地化部署。

## 6.[ACON: Optimizing Context Compression for Long-horizon LLM Agents](https://arxiv.org/pdf/2510.00615)
summary:**核心关键词**: **长程智能体, 上下文压缩, 指引优化, 模型蒸馏**  **一句话核心总结**: 本研究提出名为ACON的统一框架，通过在自然语言空间中优化压缩指引并结合模型蒸馏，为长程LLM智能体压缩交互历史与环境观测，最终在降低26-54%的峰值内存使用率的同时，保持甚至提升了任务性能。  **主要研究问题或目标**: 该研究旨在解决长程LLM智能体在执行多步复杂任务时，因不断累积的交互历史和环境观测导致上下文过长，进而引发计算成本高昂和效率降低的问题。  **关键方法论**: 该研究的核心方法是“智能体上下文优化”（ACON）。此方法首先通过一种无梯度的指引优化流程，利用一个能力强的大模型（优化器）对比完整上下文成功和压缩上下文失败的轨迹对，分析失败原因并生成自然语言反馈。这些反馈被用于迭代式地优化负责压缩上下文的模型的提示词指引，以确保关键信息得以保留。最后，为了降低开销，该框架将优化后的大模型压缩器蒸馏到一个更小的模型中进行部署。  **主要成果**: 实验结果表明，ACON在AppWorld、OfficeBench和多目标问答等三个基准测试上，能够在基本保持任务性能的同时，将峰值内存使用量（峰值令牌数）降低26-54%。此外，通过蒸馏技术，压缩器可以在保留超过95%教师模型精度的同时被缩小，并且能够使小型语言模型在长程任务中的性能提升高达46%。  **对AI从业者的主要启示**: 对于AI工程师和研究人员而言，该研究提供了一个实用且模型无关的框架来解决LLM智能体在长程应用中的上下文管理难题。其基于自然语言反馈的无梯度优化方法，使得该技术可以直接应用于包括闭源API模型在内的各类LLM，而模型蒸馏的引入进一步降低了部署成本和延迟，为在真实世界中部署经济、高效的长程智能体提供了可行的技术路径。

## 7.[Code2Video: A Code-centric Paradigm for Educational Video Generation](https://arxiv.org/pdf/2510.01174)
summary:**核心关键词**：代码中心范式，教育视频生成，多智能体框架，知识传递评估，TeachQuiz  **一句话核心总结**：本研究提出了一种名为Code2Video的代码中心多智能体框架，通过规划器、编码器和评论家三个协作智能体生成可执行的Python代码，以可解释和可控的方式创建具有精确视觉结构和高教学效率的专业教育视频。  **主要研究问题或目标**：该研究旨在解决现有像素空间视频生成模型在制作专业教育视频方面的核心局限性，这些模型难以处理领域知识、精确的视觉布局和连贯的内容流程，研究目标是创建一个可控、可解释且可扩展的教育视频生成新范式。  **关键方法论**：其核心方法是一个名为Code2Video的三智能体协作框架：1) **规划器 (Planner)** 负责将教学主题分解为具有时间连贯性的故事板，并准备相应的视觉资产；2) **编码器 (Coder)** 将结构化指令转换为可执行的Manim Python代码，并采用一种分层范围的自动修复策略（Scope-guided auto-fix）来提升代码生成效率和准确性；3) **评论家 (Critic)** 利用视觉语言模型（VLM）和创新的“视觉锚点提示”（将画布网格化以指导布局），对渲染视频进行迭代式空间布局优化，确保视觉元素的清晰度。  **主要成果**：实验结果表明，Code2Video框架显著优于基线模型，与直接代码生成方法相比，其在衡量知识传递效果的TeachQuiz指标上取得了超过40%的性能提升。具体而言，基于Claude Opus 4.1模型的Code2Video框架在TeachQuiz上得分高达86.0，远超直接代码生成方法的40.0分，其生成的视频质量与人类专家制作的教程相当。  **对AI从业者的主要启示**：对于AI从业者而言，该研究证明了在需要高精度、逻辑连贯性和结构化输出的生成任务（如教学内容、技术图表动画）中，以代码为中心的生成范式相比端到端的像素空间模型，提供了更强的可控性、可解释性和可靠性。其模块化的“规划-编码-评审”多智能体架构为构建复杂且稳健的生成系统提供了一个可复用的工程模板，展示了将大模型作为符号推理和代码生成引擎的有效路径。

## 8.[It Takes Two: Your GRPO Is Secretly DPO](https://arxiv.org/pdf/2510.00977)
summary:**核心关键词**：群体相对策略优化 (GRPO)，直接偏好优化 (DPO)，对比学习，2-GRPO  **一句话核心总结**：该研究通过将强化学习算法GRPO重构为一种对比学习形式，揭示了其与DPO的内在联系，并在此基础上提出并验证了仅需两个样本的2-GRPO变体，证明其在大幅降低计算成本的同时能达到与标准大样本GRPO相当的性能。  **主要研究问题或目标**：本文旨在挑战GRPO算法必须依赖大样本组（large group size）以保证训练稳定性和性能的普遍认知，并探索和验证使用最小样本组（两个样本）进行LLM后训练的可行性与高效性。  **关键方法论**：研究的核心方法论是首先从理论上将GRPO的目标函数重新诠释为一种对比损失。通过梯度分析，证明了GRPO通过组内奖励归一化隐式地将响应分为正负样本，其优化目标在形式上等价于对比学习，并与DPO的目标函数有本质关联。基于此理论联系，作者提出了2-GRPO，即每个提示（prompt）仅生成两个响应（rollouts）进行训练，并通过理论证明2-GRPO保留了无偏梯度估计和隐式的优势估计（advantage estimation）。  **主要成果**：实验证明，2-GRPO的性能与需要16个样本的16-GRPO相当。具体而言，在多个数学推理基准测试中，2-GRPO在性能上与16-GRPO持平，但仅使用了1/8的生成数据量（rollouts），并将训练的墙钟时间（wall-clock time）减少了超过70%。  **对AI从业者的主要启示**：这项研究为AI从业者提供了一种大幅降低大语言模型强化学习后训练成本的实用方案。开发者和工程师在应用GRPO算法时，可以直接采用计算效率极高的2-GRPO配置，从而在不牺牲模型最终性能的前提下，显著减少GPU资源消耗和训练时间，加速模型迭代和部署流程。

## 9.[BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses](https://arxiv.org/pdf/2510.00232)
summary:**核心关键词**：大语言模型偏见, 偏见缓解, 基准测试, 响应级评估  **一句话核心总结**：为解决大语言模型（LLM）偏见缓解方法评估不一致且脱离真实应用的问题，该研究提出了一个名为BiasFreeBench的经验性基准，通过统一的查询-响应设置和新的响应级度量“无偏见分数”（Bias-Free Score），全面地比较了八种主流的基于提示和基于训练的偏见缓解技术。  **主要研究问题或目标**：该研究旨在解决当前LLM偏见缓解研究中存在的两大核心问题：1）由于基线、指标和评估设置的多样性，导致不同方法间的比较缺乏一致性和公平性；2）评估大多基于模型内部概率，忽视了用户直接与之交互的生成式响应，造成了研究与实际应用之间的脱节。其目标是建立一个统一的测试平台，以系统性地评估和比较LLM在真实响应层面的偏见缓解效果。  **关键方法**：研究的核心方法是构建并应用BiasFreeBench基准。该基准通过重组现有数据集（如BBQ和FairMT-Bench），创建了统一的查询-响应式评估场景，覆盖了单轮多选问答和多轮开放式问答。在此框架下，论文系统评估了八种代表性技术，包括四种基于提示的方法（如思维链CoT）和四种基于训练的方法（如直接偏好优化DPO）。同时，研究创新性地提出了一个响应级评估指标——无偏见分数（Bias-Free Score, BFS），用于直接量化模型生成内容的公平性、安全性及反刻板印象程度。  **主要结果**：实验结果明确显示，基于提示的偏见缓解技术在性能上普遍优于基于训练的技术。具体量化结果显示，在BBQ数据集上，针对Llama-3.1-8B模型，四种提示方法的平均“无偏见分数”为78.39%，显著高于四种训练方法的平均分59.88%。此外，研究发现，像DPO这样的训练方法虽整体表现不及提示方法，但在偏见泛化能力上表现出优势，即在单一偏见类型上训练后能有效缓解未见过的偏见类型。  **对AI从业者的主要启示**：该研究最重要的发现是，精心设计的提示工程是比模型再训练更有效且成本更低的偏见缓解策略。对于致力于构建公平、安全的AI应用的工程师和数据科学家而言，这意味着优先采用和优化提示策略（如思维链CoT）可能带来更高的投入产出比。此外，BiasFreeBench和“无偏见分数”指标为从业者提供了一套标准化的工具，使其能够直接评估模型在真实交互场景中的偏见表现，从而在多种技术方案中做出更可靠的选择。

## 10.[Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range Dependency Pitfalls](https://arxiv.org/pdf/2510.00184)
summary:**核心关键词**：长程依赖, 逆向工程, 隐式思维链, 傅里叶基  **一句话核心摘要**：本研究通过逆向工程一个经由隐式思维链成功学会乘法的模型，揭示了标准Transformer因无法学习长程依赖而失败的机制，并证明了引入正确的归纳偏置（如辅助损失）可以解决这一问题。  **主要研究问题或目标**：旨在探究为何标准Transformer模型无法学会多位数乘法这一算法任务，并通过对比分析成功学会该任务的隐式思维链（ICoT）模型，揭示其失败的根本原因。  **关键方法论**：研究通过逆向工程一个成功的ICoT模型并与失败的标准微调（SFT）模型进行对比。核心技术手段包括：使用logit归因和线性探针验证模型是否编码了计算所需的中间值（如“运行总和”）；分析注意力机制，发现ICoT模型构建了“注意力树”来缓存和检索部分积；利用主成分分析（PCA）等方法分析特征几何，发现模型使用傅里叶基来表征数字。  **主要成果**：实验证明，标准微调模型因陷入局部最优而无法学习到乘法所需的长程依赖，而ICoT模型则成功构建了这种依赖。关键发现包括：1）ICoT模型利用注意力机制构建了分层计算图，用于缓存和检索部分积；2）线性探针实验显示，ICoT模型能够精确地在其隐状态中编码“运行总和”这一中间变量（平均绝对误差低至0.56），而标准模型则不能；3）通过引入一个预测“运行总和”的辅助损失，一个原本失败（准确率<1%）的2层模型在4x4乘法任务上达到了99%的准确率。  **对AI从业者的主要启示**：对于需要复杂长程依赖的算法任务，标准的自回归损失函数可能不足以引导模型学习到正确的解法，因为它倾向于收敛到缺乏全局结构的局部最优解。AI从业者在处理类似问题时，应考虑引入显式的归纳偏置，例如通过设计辅助损失函数来监督模型内部的中间计算步骤，这能有效帮助模型构建必要的长程依赖，从而解决性能瓶颈。

## 11.[EditReward: A Human-Aligned Reward Model for Instruction-Guided Image Editing](https://arxiv.org/pdf/2509.26346)
summary:**核心关键词**：奖励模型, 人类偏好对齐, 指令引导图像编辑, 数据策展  **一句话核心摘要**：该研究提出了一种名为EDITREWARD的人类对齐奖励模型，该模型通过一个包含超20万偏好对的大规模专家标注数据集进行训练，旨在解决开源指令引导图像编辑领域因缺乏高质量训练数据而发展滞后的核心瓶颈。  **主要研究问题**：该研究旨在解决开源指令引导图像编辑模型性能落后的问题，其核心目标是开发一个能准确对齐人类偏好、可用于大规模筛选和生成高质量训练数据的可靠奖励模型。  **关键方法**：核心方法是首先构建了一个大规模、高质量的人类偏好数据集EDITREWARD-DATA，然后基于该数据集训练了一个以视觉语言模型（VLM）为骨干的奖励模型EDITREWARD，并采用多维度不确定性感知排序损失进行优化，使其能精准评估编辑后图像的质量。  **主要结果**：实验表明，EDITREWARD在多个基准测试中表现优异，例如在GenAI-Bench上取得了65.72%的准确率，显著优于GPT-5（59.61%）等模型；同时，利用该模型筛选数据对下游编辑模型进行微调，使其在GEdit-Bench上的整体得分从未经筛选数据训练的6.780分提升至7.086分。  **对AI从业者的主要启示**：这项工作为AI从业者提供了一个可靠的、与人类判断高度对齐的自动化评估与数据策展工具，可直接用于现有图像编辑数据集的质量过滤，从而以更低成本高效地提升下游生成模型的训练质量和最终性能，加速了高质量开源图像编辑技术的发展。

## 12.[Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel Execution](https://arxiv.org/pdf/2509.25301)
summary:**核心关键词**：并行Agent推理框架, 有向无环图 (DAG), 并发执行, 动态工作流优化  **一句话核心摘要**：本文提出了一种名为FLASH-SEARCHER的新型并行Agent推理框架，它通过将顺序执行链重构为有向无环图（DAG）来分解复杂任务并进行并发执行，从而在显著提升复杂推理任务性能的同时，将执行步骤最多减少了35%。  **主要研究问题或目标**：该研究旨在解决当前工具增强型语言模型Agent在处理需要大量工具交互的复杂任务时，因普遍依赖顺序处理而导致的执行效率低下问题。  **关键方法论**：核心方法是将传统的顺序链式执行范式重构为基于有向无环图（DAG）的并行执行。FLASH-SEARCHER首先将复杂任务分解为带有显式依赖关系的多个子任务，从而识别并并发执行独立的推理路径。同时，该框架通过动态工作流优化机制，根据中间结果持续优化执行图，以在维持逻辑约束的同时最大化并行效率。  **主要成果**：实验结果表明，FLASH-SEARCHER在多个基准测试中 consistently 优于现有方法，在BrowseComp上实现了67.7%的准确率，在xbench-DeepSearch上实现了83%的准确率。最关键的是，与现有顺序执行框架相比，该方法能将Agent的执行步骤最多减少35%，显著提升了计算效率。  **对AI从业者的主要启示**：该工作为AI工程师构建更高效、可扩展的自主Agent系统提供了一个新的架构范式。通过采用基于DAG的并行执行策略，开发者可以在处理信息检索、网页浏览等复杂任务时，显著降低Agent的端到端延迟、提升吞吐量，这对于构建需要快速响应和处理大规模请求的Agent应用至关重要。

## 13.[BroRL: Scaling Reinforcement Learning via Broadened Exploration](https://arxiv.org/pdf/2510.01180)
summary:**核心关键词**：强化学习缩放, 展宽探索, RLVR, 质量平衡分析  **一句话核心摘要**：本研究提出了一种名为BroRL的强化学习缩放新范式，通过将每个样本的rollout数量增加至数百来展宽探索，解决了现有方法在增加训练步数后面临的性能饱和问题，从而实现了持续的性能增益。  **主要研究问题**：该研究旨在解决大型语言模型在采用可验证奖励强化学习（RLVR）时，通过增加训练步数（如ProRL方法）进行缩放后出现的性能饱和甚至下降的问题。  **关键方法**：BroRL的核心方法是显著增加每个样本的rollout数量N（例如从16增加到512），其理论依据是质量平衡方程分析，该分析表明增加N可以减小策略更新中一个潜在的负面“未采样耦合项”的影响，从而更稳定地保证正确token的概率质量增加。  **主要成果**：实验证明，对于一个经过3000步ProRL训练后性能饱和的模型，BroRL能够使其性能持续提升，例如在数学、代码和推理基准上分别达到63.03、54.20和63.09分，超越了继续使用ProRL后停滞或下降的性能；同时，在硬件层面，BroRL将样本生成吞吐量提高了近一倍（从36.5 samples/s提升至72.4 samples/s）。  **对AI从业者的主要启示**：对于AI工程师和研究者而言，本研究揭示了rollout数量是比训练步数更关键、更高效的RL缩放维度，尤其是在模型性能饱和时，增加探索宽度不仅能突破性能瓶颈，还能通过将计算从内存密集型转为计算密集型来显著提升硬件利用率和训练效率。

## 14.[QUASAR: Quantum Assembly Code Generation Using Tool-Augmented LLMs via Agentic RL](https://arxiv.org/pdf/2510.00967)
summary:**核心关键词** 量子汇编代码, 智能体强化学习, 工具增强大语言模型, 分层奖励机制  **一句话核心总结** 该研究提出QUASAR，一个基于工具增强大语言模型的智能体强化学习框架，通过外部模拟器验证和分层奖励机制来优化量子汇编代码的生成，从而显著提升生成电路的语法正确性和语义性能。  **主要研究问题或目标** 旨在解决大语言模型因缺乏特定领域的量子知识而难以生成高质量且语法正确的量子汇编代码（OpenQASM）的问题，特别是如何确定参数化量子门所需的精确数值以及如何确保电路的整体性能。  **关键方法** 核心方法是QUASAR，一个智能体强化学习框架。该框架让大语言模型作为智能体生成OpenQASM代码，并调用一个外部量子工具服务器进行模拟和验证。其关键创新是一个精细的分层奖励机制，该机制按层级评估代码：首先验证语法正确性，然后通过詹森-香农距离评估与基准电路的分布对齐情况，接着计算任务相关的哈密顿量期望值，最后根据电路距离最优解的优化步数给予奖励。  **主要成果** 实验结果表明，QUASAR在增强一个4B参数的大语言模型后，在Pass@1的生成任务中达到了99.31%的语法有效性，在Pass@10中达到100%，显著优于GPT-4o、GPT-5等工业级大模型以及仅使用监督微调或强化学习的基线方法。消融实验证实，分布对齐奖励是提升模型性能最关键的驱动因素。  **对AI从业者的主要启示** 该研究为AI从业者展示了一种将大语言模型应用于高度专业化且可验证领域的有效范式，即“智能体强化学习 + 外部验证工具 + 领域知识驱动的奖励工程”。对于需要生成语法严格且语义精确的特定领域语言（如硬件描述语言、科学计算代码）的任务，开发者可以借鉴QUASAR的思路，通过集成外部模拟器或编译器来提供高质量的反馈信号，并通过设计分层奖励函数来指导模型学习复杂的领域知识，从而有效提升代码生成的质量和可靠性。

## 15.[On Predictability of Reinforcement Learning Dynamics for Large Language Models](https://arxiv.org/pdf/2510.00553)
summary:**核心关键词**：强化学习，参数动态，秩一主导性，线性预测  **一句话核心摘要**：该研究通过分析大语言模型在强化学习训练中的参数更新矩阵，揭示了其具有“秩一主导性”和“秩一线性动态”两个基本特性，并基于此提出了一种名为AlphaRL的插件式框架，用于从早期训练阶段预测并外推最终模型参数以加速训练。  **主要研究问题或目标**：本文旨在解决对大语言模型进行强化学习（RL）训练时，其底层参数动态变化规律尚不明确的问题，核心目标是识别并验证RL引发的参数更新是否遵循可预测的模式。  **关键方法论**：研究的核心方法是分析RL训练后模型与基础模型之间的参数差异矩阵ΔW。通过对ΔW进行奇异值分解（SVD），研究者发现其顶部的秩一子空间（Rank-1 Subspace）几乎完全决定了模型推理能力的提升。随后，他们追踪该秩一子空间在训练过程中的演化轨迹，发现其呈现近似线性的动态变化，从而能够利用早期训练检查点的数据来线性外推出训练终点的参数更新。  **主要成果**：实验证明，仅使用参数更新矩阵的秩一子空间便可恢复超过99%的推理性能增益。基于此发现提出的AlphaRL加速框架，能够利用短暂的早期训练窗口，实现最高2.5倍的训练加速，同时保留超过96%的最终模型推理性能，且无需引入额外模块或调整超参数。  **对AI从业者的主要启示**：此项研究为AI工程师提供了一个极具实用价值的工具AlphaRL，它揭示了RL对LLM的优化主要集中在一个可预测的低维方向上。这意味着从业者可以在不进行完整、昂贵的RL训练过程的情况下，通过早期短时间的训练数据来预测模型的最终优化状态，从而极大地节省计算资源和训练时间，为大规模模型的迭代和部署提供了更高效的路径。

## 16.[Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum](https://arxiv.org/pdf/2510.00526)
summary:**核心关键词**：监督微调，模型能力连续体，概率基目标函数，负对数似然  **一句话核心总结**：本研究通过分析一个通用的概率基目标函数家族，揭示了其在大型语言模型监督微调中的有效性由一个“模型能力连续体”关键维度决定，从而为超越传统负对数似然（NLL）的目标函数选择提供了理论与实践基础。  **主要研究问题或目标**：该研究旨在解决标准监督微调（SFT）中默认的负对数似然（NLL）目标函数导致模型泛化能力有限的问题，并系统性地探究在不同条件下（特别是模型先验知识强度不同时）何种概率基目标函数更为有效。  **关键方法论**：研究提出并评估了一个通用的概率基目标函数家族，该家族通过调整对不同概率区间token的梯度权重，区分为“先验倾向型”（prior-leaning）和“先验规避型”（prior-averse）目标。研究的核心是引入“模型能力连续体”这一概念，将任务划分为模型强（如数学）、中（如医疗）、弱（如新型谜题）三个区域，并通过在7个模型、14个基准上的全面实验，系统地分析不同目标函数在该连续体上的性能表现。  **主要成果**：实验明确揭示了目标函数的性能在模型能力连续体两端发生反转。在模型能力强的场景下，倾向于模型先验的-p目标函数显著优于NLL，性能提升最高可达16%。例如，在Qwen2.5-Math-7B模型上，-p目标的平均分（36.51）远超NLL（22.67）。而在模型能力弱的场景下，传统的NLL目标函数则表现更优。  **对AI从业者的主要启示**：这项研究为AI工程师和研究者提供了在进行监督微调时选择训练目标的实用指南。从业者不应默认使用NLL，而应首先评估基础模型在目标任务上的先验能力。如果模型已有很强的相关知识（模型强），应选用-p等先验倾向型目标函数以精炼已有能力；反之，如果模型对任务知识储备不足（模型弱），则NLL是更稳健的选择。

## 17.[Making, not Taking, the Best of N](https://arxiv.org/pdf/2510.00931)
summary:**N-选一 (Best-of-N)**, **N-融合 (Fusion-of-N)**, **合成数据生成**, **测试时扩展 (test-time scaling)** 该研究提出了一种名为“N-融合”(FusioN)的新方法，通过使用一个通用大语言模型（LLM）裁判来综合N个候选样本中的有用信息以生成一个更优的最终答案，从而在测试时扩展和合成数据生成两个场景下均优于传统的“N-选一”(BoN)方法。 本研究旨在解决现有大语言模型（LLM）中主流的“N-选一”（Best-of-N, BoN）聚合方法的局限性，即该方法在从N个样本中选择最佳答案时，会丢弃其余样本中潜在有价值和多样化的信息。 核心方法是“N-融合”（Fusion-of-N, FusioN）。该方法将聚合问题从“选择”转变为“合成”。它使用一个通用的LLM作为“融合器”（fusor），将给定的N个候选生成结果作为输入，并指示该模型分析每个样本的优点和缺点，然后综合各个样本中最具信息量的元素，最终生成一个全新的、质量更高的单一答案。 实验表明，FusioN在多个基准测试中持续优于BoN。例如，在测试时扩展（test-time scaling）场景下，相较于BoN，FusioN在mArena-v2基准上对GEMINI2.5-PRO的胜率提升了3.8%；在合成数据生成方面，使用FusioN生成的数据训练的模型在mArena-v2上比使用BoN数据的模型胜率高2.5%。 对于AI从业者而言，该研究提供了一种简单且有效的替代BoN的方案。在需要从多个模型输出中获取最佳结果的场景（如合成数据生成或提升推理质量）中，工程师可以直接利用一个强大的LLM作为“融合器”来整合多个候选输出，而不是依赖于复杂的奖励模型进行选择。这种“合成而非选择”的范式能够更充分地利用计算资源，整合不同模型的优势，从而获得仅通过选择无法达到的性能提升。

## 18.[GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness](https://arxiv.org/pdf/2510.00536)
summary:**核心关键词**：GUI代理, KV缓存压缩, 时空冗余  **一句话核心摘要**：本文提出了一种名为GUI-KV的即插即用KV缓存压缩方法，通过结合空间显著性引导与时间冗余评分来利用GUI的特有冗余，从而在无需再训练的情况下为GUI代理实现高效且可靠的性能。  **主要研究问题或目标**：该研究旨在解决基于视觉语言模型的GUI代理在处理长序列高分辨率截图时，因推理效率低下、成本高昂和内存限制而难以部署的问题，特别是针对现有KV缓存压缩方法未能有效利用GUI特有时空冗余性的不足。  **关键方法**：该研究提出GUI-KV方法，它首先通过分析证明GUI注意力稀疏度在所有层中均一且极高，故采用统一的缓存预算分配策略。其核心技术包含两个创新点：（1）空间显著性引导，通过计算隐藏状态的L2范数来增强注意力分数，以更准确地保留语义上重要的视觉令牌；（2）时间冗余评分，利用QR分解将先前帧的键向量投影到当前帧的键子空间上，通过量化其正交残差来识别并优先剪除冗余的历史信息。  **主要成果**：实验结果表明，GUI-KV在多个基准测试中均优于竞争基线，并能在较小的预算下达到接近全缓存的精度；其中最显著的发现是，在AgentNetBench基准的5截图设置下，GUI-KV不仅将解码FLOPs降低了38.9%，还将单步任务准确率相对于全缓存基线提升了4.1%。  **对AI从业者的主要启示**：对于AI工程师和研究人员，该研究提供了一种无需重新训练的即插即用型工具（GUI-KV），可直接应用于现有的GUI代理模型，以显著降低GPU内存占用和加速推理，从而使在资源受限环境下部署高效、可靠的GUI自动化代理成为可能；其核心价值在于揭示了利用特定数据模态（如GUI）的时空冗余不仅能提升效率，甚至可能通过滤除噪声上下文来提高模型准确性。

## 19.[Training Vision-Language Process Reward Models for Test-Time Scaling in Multimodal Reasoning: Key Insights and Lessons Learned](https://arxiv.org/pdf/2509.23250)
summary:**核心关键词**： **视觉语言过程奖励模型**，**测试时扩展**，**多模态推理**，**感知焦点监督**  **一句话核心摘要**： 本文研究了视觉语言过程奖励模型（VL-PRM），通过提出一种结合蒙特卡洛树搜索（MCTS）与强VLM判断的混合数据合成框架及感知焦点监督方法，旨在提升多模态推理中过程监督的准确性并有效指导视觉语言模型（VLM）进行测试时扩展。  **主要研究问题或目标**： 该研究旨在解决现有VL-PRM因依赖MCTS构建数据而产生的监督信号噪声大和泛化能力有限的问题，其目标是系统性地探索VL-PRM的设计空间，包括数据集构建、训练和测试时扩展策略，以提升其在多模态推理任务中的性能。  **关键方法论**： 研究的核心方法论包含两个层面：首先，在数据构建上，提出了一种混合数据合成框架，它将MCTS与一个强VLM（o4-mini）的判断相结合，为推理的每一步生成更准确的标签，从而构建了VL-PRM300K数据集；其次，在模型训练上，引入了感知焦点监督，使VL-PRM能明确地检测视觉感知阶段的错误。在评估阶段，研究系统地比较了多种测试时扩展策略，包括将VL-PRM用作结果奖励模型（ORM）进行一次性搜索。  **主要成果**： 实验结果表明，所提出的VL-PRM在多个方面表现出色，其中最显著的发现是，引入感知焦点监督能大幅提升模型能力，例如，在感知错误检测任务上，Qwen-VL-PRM-7B模型的F1分数从未经感知监督训练的33.3提升至70.3。此外，研究还发现，在测试时扩展中，将VL-PRM作为结果奖励模型（ORM）使用时，其性能优于传统的逐引导式步骤选择，且经过训练的3B参数规模的VL-PRM在过程错误检测能力上能够媲美甚至超越如GPT-4o这样更强大的专有模型。  **对AI从业者的主要启示**： 这项工作为AI工程师提供了提升多模态模型推理可靠性的实用见解，其核心启示是通过构建高质量的过程奖励模型，特别是加入对视觉感知步骤的监督，可以显著增强大型VLM在测试时的性能。一个关键的实践指导是，在应用测试时扩展策略时，采用评估完整解决方案的一次性搜索（类ORM方法）比逐引导的贪婪搜索更有效，这为优化推理流程和提升模型准确率提供了更高效的路径。此外，该研究证明了小规模的专用奖励模型在指导大规模基础模型方面具有巨大潜力，为资源高效的模型部署提供了新思路。

## 20.[Infusing Theory of Mind into Socially Intelligent LLM Agents](https://arxiv.org/pdf/2509.22887)
summary:**心智理论, 对话智能体, 前瞻模拟, 目标导向推理** 本研究提出了一种名为TOMA的训练框架，它通过结合心智理论（ToM）与对话前瞻模拟来训练LLM智能体，旨在生成对实现对话目标最有用的心理状态，从而提升其在社交互动中的目标达成能力和策略性。 本研究旨在解决现有LLM社交智能体因缺乏心智理论（ToM）能力而导致其在目标导向对话中效率不高的问题，其目标是开发一种能显式融合ToM的训练方法，以增强智能体的社交推理与目标达成能力。 该研究提出了TOMA框架，其核心方法是：首先，基于对话上下文，让LLM生成关于对话伙伴的多个候选心理状态假设；其次，为每个假设生成对应的候选话语；然后，通过短时程对话模拟来评估并筛选出能最大化提升目标达成得分的（心理状态，话语）对；最后，使用这些高价值数据对LLM进行微调，使其学会联合预测有效的心理状态和基于该状态的策略性话语。 实验结果表明，在Sotopia社交评估基准上，TOMA模型显著优于基线模型；与最佳基线变体相比，TOMA在Qwen2.5-3B和Qwen2.5-7B模型上的得分分别最高提升了18.9%和6.9%。 对于AI工程师和开发者而言，本研究提供了一个具体可行的框架（TOMA），用于构建更具社交智能和目标导向性的对话智能体。该工作表明，通过显式建模和训练特定认知能力（如心智理论），而非仅仅依赖通用提示工程，可以显著提升AI在复杂社交任务中的表现，为开发更高效的聊天机器人、谈判代理或虚拟助手提供了新的技术路径。

## 21.[Pay-Per-Search Models are Abstention Models](https://arxiv.org/pdf/2510.01152)
summary:**核心关键词**： 选择性寻求帮助，拒绝回答模型，强化学习，按次搜索付费  **一句话核心摘要**： 本文提出了一种名为MASH的训练框架，通过使用带按次搜索惩罚的强化学习来训练大语言模型（LLMs）进行选择性工具调用，从而在无需专门构建拒绝回答训练数据的情况下，让模型自然地学会识别其知识边界并拒绝回答未知问题。  **主要研究目标**： 该研究旨在解决大语言模型无法可靠识别其参数化知识边界，并倾向于对边界外问题产生幻觉的难题，其目标是设计一种单一的训练策略，既能让模型内在具备拒绝回答的能力，又能通过选择性地寻求外部帮助（如搜索）来扩展其可回答问题的范围。  **关键方法论**： 该论文的核心方法是MASH（Modeling Abstention via Selective Help-seeking）框架。其关键思想是将模型的外部帮助寻求行为（即工具使用）作为拒绝回答的代理。MASH使用强化学习来训练模型，并引入一个“按次搜索付费”的奖励机制：模型在获得正确答案时得到奖励，但每次调用搜索工具都会受到惩罚。这种机制激励模型仅在其内部参数化知识不足以回答问题时才进行搜索。在推理阶段，通过移除模型的搜索工具访问权限，任何搜索意图都会被直接转换为拒绝回答的决策。该框架的独特之处在于，它不需要预先确定模型的知识边界来构造训练数据。  **主要成果**： 实验结果表明，MASH框架显著优于先前的效率导向型搜索方法。具体而言，在多跳问答（multi-hop QA）数据集上，MASH将模型的答案准确率提升了7.6%。此外，该模型展现了强大的“开箱即用”的拒绝回答能力，能有效区分可回答与不可回答的问题，其表现可与专门的拒绝回答模型相媲美。  **对AI从业者的主要启示**： 此研究为AI工程师提供了一种高效且统一的训练范式，能够以“一石二鸟”的方式同时提升模型的工具使用效率和安全性。开发者无需再为拒绝回答功能设计独立的、需要预先评估模型知识边界的复杂训练流程，而是可以通过实施一个基于成本效益原则的强化学习策略，让模型在学习如何有效利用外部工具的同时，自然地获得识别自身能力边界并避免幻觉的能力，这对于构建更可靠、更值得信赖的AI应用具有直接的实践价值。

## 22.[MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive-First Text Sources](https://arxiv.org/pdf/2509.25531)
summary:**核心关键词**：**预训练语料库**，**许可优先**，**风险缓解**，**指令与推理数据**  **一句话核心总结**：该研究提出了MixtureVitae，一个通过结合优先许可文本源、低风险数据及高质量合成指令数据构建的开放预训练语料库，旨在最小化法律风险的同时实现具有竞争力的大语言模型性能。  **主要研究问题或目标**：本文旨在解决当前大语言模型开发严重依赖具有法律和伦理风险的大规模网络抓取数据的问题，探索能否构建一个基于许可优先和风险缓解策略的数据集，以训练出性能可与非许可数据集训练的模型相媲美的大语言模型。  **关键方法论**：研究采用“许可优先”的数据源策略，核心数据来自公共领域和明确许可（如CC-BY, Apache）的文本。该策略通过谨慎纳入低风险数据（如政府作品、欧盟TDM合格来源）进行补充，并大量增加由许可模型在许可种子数据上生成的合成指令与推理数据，以弥补有机许可数据中此类内容的稀缺性。整个流程包含一个透明的多阶段处理管道，用于许可证感知过滤、安全与质量筛选及领域感知混合。  **主要成果**：在1.7B参数和300B tokens的受控实验中，基于MixtureVitae训练的模型在GSM8K数学基准测试上取得了0.53的得分，显著高于其他所有对比数据集（得分范围在0.02-0.06），并在MBPP代码任务上获得0.38的得分，证明其在数学和代码推理能力上具有数量级的优势。  **对AI从业者的主要启示**：这项工作为AI工程师和数据科学家提供了一条构建高性能且法律风险更低的LLM的实用路径。它证明了通过精心策划的许可优先数据与合成指令数据的结合，可以有效替代传统的无差别网络抓取方法，从而在不牺牲模型核心竞争力（尤其是在数学和代码能力上）的前提下，显著降低商业应用中的版权侵权风险。

## 23.[JoyAgent-JDGenie: Technical Report on the GAIA](https://arxiv.org/pdf/2510.00510)
summary:**核心关键词**：通用智能体, 多智能体框架, 分层记忆系统, 系统级集成  **一句话核心总结**：本文提出了一种名为JoyAgent-JDGenie的通用智能体架构，该架构通过系统级集成融合了结合Plan-Execute与ReAct范式的多智能体协作、分层记忆系统及精炼的工具套件，在GAIA基准测试上取得了超越现有开源基线的性能，验证了该方法的鲁棒性与适应性。  **主要研究问题或目标**：该研究旨在解决现有大型语言模型（LLM）智能体因缺乏统一的、系统性的框架而导致的在处理复杂现实世界任务时鲁棒性、适应性和可复现性不足的问题。  **关键方法论**：该框架集成了三个核心组件：1) 一个异构多智能体集合，该集合融合了用于稳定规划的Plan-Execute范式和用于灵活适应的ReAct范式，并通过一个评判模型（Critic Model）进行投票协调以平衡稳定性和灵活性；2) 一个包含工作记忆（实时上下文）、语义记忆（长期知识）和程序记忆（系统提示）的分层记忆系统，以实现长时程连续性和自适应控制；3) 一个专注于搜索、代码执行和多模态解析的精炼工具套件。  **主要成果**：在GAIA基准上，该框架的性能超越了所有被评估的开源基线，并在验证集上取得了75.2的Pass@1和82.4的Pass@3分数，在测试集上达到了67.1的Pass@1，其性能接近顶尖的闭源系统。  **对AI从业者的主要启示**：该研究最重要的启示是，构建高性能通用智能体需要从系统层面进行整体设计，而非仅仅优化单个组件。对于AI工程师而言，将不同的智能体范式（如Plan-Execute与ReAct）进行集成、设计结构化的记忆系统以及整合可靠的工具接口是提升智能体鲁棒性和泛化能力的关键路径，特别是其提出的多智能体投票融合架构为复杂任务解决提供了高效且稳健的实现方案。

## 24.[BatonVoice: An Operationalist Framework for Enhancing Controllable Speech Synthesis with Linguistic Intelligence from LLMs](https://arxiv.org/pdf/2509.26514)
summary:**核心关键词**： **可控文本转语音**，**操作主义**，**解耦框架**，**声学特征文本化**  **1-Sentence Core Summary**： 为解决可控文本转语音（TTS）中大型语言模型（LLM）语言智能利用不足的问题，该研究提出了一种受“操作主义”启发的BatonVoice框架，通过解耦指令理解与语音生成，利用LLM“指挥家”生成文本化声学特征来指导TTS“管弦乐队”合成语音，从而实现了高性能的情感控制和零样本跨语言能力。  **Main Research Question or Objective**： 该研究旨在解决现有基于LLM的TTS模型未能充分利用LLM强大的指令遵循和语言理解能力的问题，这一限制阻碍了模型根据复杂文本指令进行精细化、可控语音合成的能力。  **Key Methodology**： 论文提出了BatonVoice框架，该框架将流程分为两阶段。第一阶段，“指挥家”（一个外部LLM），负责理解用户指令并生成一个文本化的声学特征“计划”（以JSON格式表示，包含音高、能量、音色等量化指标）。第二阶段，“管弦乐队”（一个专门训练的BatonTTS模型），接收原始文本和该声学特征计划，并合成最终语音。BatonTTS的训练采用三阶段流程：基础TTS预训练、基于文本化声学特征的监督微调（SFT）以及无需人工标注的偏好优化（PO），以提升模型质量和控制精度。  **Primary Results**： 实验表明，BatonVoice在英文情感语音合成任务上取得了57.6%的准确率，显著优于强大的闭源基线。该框架展现了卓越的零样本跨语言泛化能力，仅用英文数据训练，在中文情感合成基准测试中仍达到了56.2%的准确率，超越了针对中文优化的模型。此外，通过更换更强的“指挥家”LLM（从1.7B模型换为Gemini 2.5 Pro），情感合成准确率从29.8%提升至57.6%，证明了框架的可扩展性。  **Principal Implication for AI Practitioners**： 该研究为AI从业者揭示了一种有效利用现有文本LLM强大语言智能的新范式：将语音等其他模态信息“操作化”为LLM可直接理解和生成的结构化文本（即声学特征）。这种解耦的“指挥家-管弦乐队”架构具有高度模块化和可扩展性，从业者无需重新训练语音合成模型，仅通过升级“指挥家”LLM即可提升整个系统的性能。该方法显著降低了对昂贵的人工标注多模态指令数据的依赖，为开发更可控、更智能的多模态应用提供了新的思路。

## 25.[Eliciting Secret Knowledge from Language Models](https://arxiv.org/pdf/2510.01070)
summary:**核心关键词**：秘密知识引出, 模型审计, 黑盒攻击, 白盒分析  **一句核心总结**：本研究通过训练三类能应用但会否认知晓特定事实的大语言模型作为测试平台，设计并系统性地评估了多种黑盒与白盒技术以引出其隐藏的秘密知识，并最终建立了一个用于评估此类方法的公开基准。  **主要研究问题或目标**：本研究旨在解决“秘密知识引出”问题，即如何发现并验证AI模型拥有但在被直接提问时会刻意隐藏的知识，并量化评估不同引出技术的有效性。  **关键方法论**：研究首先通过监督微调构建了三类“秘密守护者”模型，使其掌握特定秘密（如用户性别或加密指令）并能在对话中应用，但同时拒绝直接承认。随后，研究设计了多种引出技术，包括黑盒方法中的“前缀填充攻击”（prefill attacks），即为模型提供一个预设前缀来引导其泄露秘密；以及白盒方法，利用logit lens和稀疏自动编码器（SAEs）来检查模型的内部激活和中间层预测。评估时，使用一个“审计LLM”根据这些技术的输出猜测秘密，以其成功率作为衡量指标。  **主要研究成果**：实验结果表明，所提出的多种引出技术均显著优于基线方法。其中，基于前缀填充的黑盒攻击最为有效，在三个测试场景中的两个取得了最佳表现，使审计模型的秘密猜测成功率超过90%。在剩余的一个场景中，基于logit lens和SAEs的白盒技术则表现更佳，证明了不同技术在特定条件下的有效性。  **对AI从业者的主要启示**：这项研究为AI安全和模型审计工程师提供了具体可行的工具和基准，证明了即使模型表面上遵循安全指令，也可能隐藏关键知识。其中，前缀填充攻击作为一种高效的黑盒技术，为从业者提供了一种成本较低且效果显著的红队测试（red-teaming）方法，可用于在模型部署前探测其潜在的欺骗行为或隐藏能力，从而加强对AI系统行为的审查和验证。

## 26.[ReSWD: ReSTIR'd, not shaken. Combining Reservoir Sampling and Sliced Wasserstein Distance for Variance Reduction](https://arxiv.org/pdf/2510.01061)
summary:**核心关键词**： **分布匹配**，**切片Wasserstein距离**，**水塘抽样**，**方差缩减**  **一句话核心摘要**： 该研究提出了一种名为ReSWD的方法，通过将加权水塘抽样集成到切片Wasserstein距离（SWD）中，自适应地保留信息丰富的投影方向，以解决SWD的蒙特卡洛估计器存在的高方差问题，从而在保持无偏性的同时产生更稳定的梯度并提升分布匹配任务的性能。  **主要研究问题或目标**： 该研究旨在解决切片Wasserstein距离（SWD）在实际应用中的一个核心问题：其蒙特卡洛估计器因随机投影而产生高方差，导致在优化过程中梯度噪声大、收敛缓慢。研究目标是开发一种能够有效降低这种方差，同时保持估计无偏性的新方法，以提高其在计算机视觉和图形学任务中的优化效率和稳定性。  **关键方法论**： 该研究的核心方法ReSWD（Reservoir SWD）将加权水塘抽样（WRS）机制与SWD相结合。在优化过程中，该方法维护一个包含K个高贡献度投影方向的“水塘”（reservoir）。在每个优化步骤，它会生成M个新的候选方向，并与水塘中已有的方向合并。然后，以各个方向上计算出的一维Wasserstein距离作为权重，利用WRS算法从中选出新的K个方向存入水塘，高权重的方向（即能更好区分两个分布的方向）有更大概率被保留。该方法还引入了时间衰减因子来逐步降低旧方向的权重，以适应动态变化的优化目标，最终通过自归一化的重要性权重计算损失，确保了梯度的稳定性和估计的无偏性。  **主要成果**： 实验结果表明，ReSWD在合成基准和真实世界任务中均一致优于标准SWD和其他方差缩减基线。在一项包含1000次匹配的1D分布匹配基准测试中，ReSWD取得了0.622 x 10⁻³的平均Wasserstein-1距离，显著优于标准SWD的0.733 x 10⁻³。此外，在色彩校正任务中，ReSWD的色彩PSNR达到24.64，高于SWD基线的24.30；在扩散模型引导任务中，基于SD3.5-large模型，ReSWD将平均Wasserstein-2距离降低至0.55 x 10⁻²，展现了其卓越性能。  **对AI从业者的主要启示**： 对于AI从业者而言，ReSWD提供了一种可直接替代标准SWD的高效、无偏估计器，它能显著降低梯度噪声并加速收敛。这意味着在开发依赖分布匹配损失的AI应用（如风格迁移、图像生成引导、色彩校正）时，工程师可以获得更稳定、更高效的训练过程。该方法尤其适用于需要处理高维数据分布的场景，能够以相近的计算成本获得更优的模型性能和更快的迭代速度，从而提升开发效率。

## 27.[CurES: From Gradient Analysis to Efficient Curriculum Learning for Reasoning LLMs](https://arxiv.org/pdf/2510.01037)
summary:**核心关键词**： **课程学习, 梯度优化, 贝叶斯后验估计, 训练效率**  **一句话核心总结**： 该研究针对推理型大语言模型训练效率低下的问题，提出了一种名为CurES的高效课程学习方法，该方法通过梯度优化分析，利用贝叶斯后验估计动态调整提示采样与计算资源分配，从而加速模型收敛并提升性能。  **主要研究问题或目标**： 本研究旨在解决现有课程学习方法在训练推理大语言模型时，因未能有效处理提示（prompt）的难度差异而导致的计算资源浪费和训练效率低下的问题，其核心目标是设计一个能够根据模型能力动态调整训练策略的高效课程学习框架。  **关键方法论**： 该方法首先从理论上分析了强化学习的梯度优化过程，揭示了提示的采样分布决定了收敛速度，而rollout数量的分配影响梯度更新的稳定性。基于此，CurES方法利用贝叶斯后验估计（具体使用Beta分布）来高效、低开销地动态评估模型对每个提示的回答准确率（即提示难度），并根据该评估结果，动态地调整提示的采样概率和分配给每个提示的rollout数量，以优先训练那些能提供最大学习信号（即准确率接近0.5）的“中等难度”提示。  **主要结果**： 实验表明，CurES在性能和收敛速度上均显著优于基线方法，在使用7B参数规模的模型上，CurES在多个推理基准测试中的平均性能比GRPO基线高出**+4.82个点**。在训练效率方面，CurES展现出更快的收敛速度，例如在MATH500数据集上，其收敛到GRPO基线的峰值性能所需训练步数减少了**5.5倍**。  **对AI从业者的主要启示**： 该研究为AI工程师提供了一个经过理论验证且行之有效的高效课程学习框架，可用于显著降低微调推理大语言模型所需的计算成本和时间。其核心启示是，通过低开销的贝叶斯方法动态评估和优先处理“中等难度”的训练样本，能够最大化每一步训练的价值，从而实现数倍的训练加速，这一动态资源分配思想对于优化任何数据密集型的模型训练流程都具有重要的实践意义。

## 28.[In-Place Feedback: A New Paradigm for Guiding LLMs in Multi-Turn Reasoning](https://arxiv.org/pdf/2510.00777)
summary:**就地反馈**, **多轮推理**, **错误修正**, **交互范式** 该研究针对大型语言模型在多轮推理中难以可靠整合反馈的问题，提出了一种“就地反馈”交互范式，即用户直接编辑模型先前的错误响应，模型基于修正后的内容生成新输出，从而有效提升了推理任务的性能与令牌效率。 本研究旨在解决大型语言模型（LLMs）在多轮推理任务中，通过传统新增消息方式提供反馈时，难以精确、可靠地整合用户修正指令的核心局限，该问题常导致模型性能提升不一致、原有错误未被修正，甚至在正确内容中引入新错误。 该研究引入了“就地反馈”（in-place feedback）范式，将反馈过程重构为直接的状态修复而非新的指令。该方法分为两步：首先，用户直接定位并编辑模型先前响应中的错误文本片段；其次，模型将此修正后的文本作为上下文，从编辑点继续生成后续内容，确保其在用户验证过的正确基础上进行推理。 实验结果表明，“就地反馈”范式在多个推理密集型基准测试中均优于传统的对话式反馈。具体而言，该方法在取得更优性能的同时，相比传统多轮反馈方法减少了79.1%的令牌使用量，并有效解决了传统反馈模式下模型无法精确定位并修正错误的问题。 对于AI工程师和开发者而言，该研究提供了一种更高效、更自然的人机协作模式来引导和修正大型语言模型的复杂推理过程。在构建需要多步推理的应用（如编程助手、科学计算）时，采用“就地反馈”界面设计，可在不增加模型训练成本的情况下，显著提升交互效率、修正精确度和最终任务成功率，从而改善用户体验和系统可靠性。

## 29.[BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration](https://arxiv.org/pdf/2510.00438)
summary:**核心关键词**：主体一致性视频生成, 多模态大语言模型, 跨模态集成  **一句话核心总结**：该研究提出了一种名为BindWeave的统一框架，通过引入MLLM-DiT架构，利用预训练的多模态大语言模型进行深度跨模态推理以生成主体感知的隐藏状态来指导扩散变换器，从而在主体一致性、自然度和文本相关性方面实现了超越现有模型的先进性能，解决了复杂提示下主体一致性视频生成的难题。  **主要研究问题或目标**：本文旨在解决现有视频生成模型在处理包含复杂空间关系、时序逻辑和多主体交互的文本提示时，难以保持主体身份和外观一致性的问题，其目标是创建一个能处理从单主体到复杂多主体场景的统一、高保真度的主体到视频（Subject-to-Video）生成框架。  **关键方法论**：BindWeave采用一种MLLM-DiT（多模态大语言模型-扩散变换器）架构。其核心是用一个预训练的MLLM（Qwen2.5-VL）作为智能指令解析器，对文本提示和参考图像的交错序列进行深度推理，以解析实体、属性和交互关系，生成富含语义的隐藏状态。随后，一个扩散变换器（DiT）生成器在两个层面被协同调节：1）在交叉注意力层，同时接收来自MLLM的隐藏状态与T5文本编码器组合的联合条件信号（`Cjoint`），以及来自CLIP的图像特征以加强身份引导；2）在输入层，将参考图像的VAE特征拼接到噪声视频潜变量的时间维度上，以保留精细的外观细节。  **主要成果**：在OpenS2V基准测试上的实验表明，BindWeave在多项指标上均达到当前最佳水平。该方法在综合评估的总分上取得了**57.61%**的成绩，优于所有对比的开源及商业模型。尤其在衡量主体一致性的关键指标NexusScore上达到了**46.84%**，展示了其在核心任务上的显著优势。消融实验也证实，引入MLLM模块使模型性能得到显著提升。  **对AI从业者的主要启示**：该研究为可控内容生成领域提供了一个重要范式，即利用预训练的多模态大语言模型作为统一的前端指令解析器，替代传统的浅层特征融合。这种方法能更深刻地理解和推理多模态输入间的复杂语义关系，为生成模型提供更鲁棒、更具语义基础的条件信号。对于AI工程师和研究者而言，最有价值的启示是将大型推理模型集成到生成模型的条件控制流中，是解决细粒度控制、多主体一致性和复杂指令遵循等挑战的一个极具前景的技术路径。

## 30.[VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs](https://arxiv.org/pdf/2509.25916)
summary:**核心关键词**： 视觉语言模型，细粒度感知，特征检索，混合细粒度区域编码器  **1-Sentence Core Summary**： 该研究提出了一种名为VLM-FO1的新框架，通过一个采用混合细粒度区域编码器（HFRE）的即插即用模块，将目标感知的坐标生成问题重构为特征检索任务，从而在不损害模型通用视觉理解能力的前提下，显著提升了预训练视觉语言模型（VLM）的细粒度感知与定位能力。  **Main Research Question or Objective**： 该研究旨在解决现有视觉语言模型（VLM）在需要精确定位的细粒度感知任务（如目标检测和 grounding）上表现不佳的核心问题。该问题源于以语言为中心的VLM架构在直接生成精确数值坐标方面的固有困难和脆弱性。  **Key Methodology**： VLM-FO1的核心方法是将其设计为一个与任何预训练VLM集成的即插即用模块。该模块引入了一个混合细粒度区域编码器（HFRE），其特点是包含一个双视觉编码器结构，结合了VLM原有的富含语义的编码器和新增的专注于高分辨率细节的编码器。该系统将外部检测器生成的候选区域框处理成富含语义与空间细节的“区域令牌（region tokens）”，然后通过一个基于令牌的引用系统将这些区域令牌与文本一起输入到大型语言模型中，从而将定位问题从不稳定的坐标生成转变为鲁棒的特征检索与引用任务。  **Primary Results**： 实验结果表明，VLM-FO1在多个基准测试中达到了SOTA性能。具体而言，在COCO目标检测基准上，该方法的轻量级3B模型取得了44.4的mAP，相较于如Qwen2.5-VL等基线模型实现了超过20个点的提升，达到了与专用检测模型相当的水平。此结果验证了该框架在对象定位、区域理解和视觉推理任务上的有效性。  **Principal Implication for AI Practitioners**： 对于AI从业者而言，该研究提供了一种高效且灵活的模块化方法，用于增强现有预训练VLM的细粒度感知能力，而无需对基础模型进行大规模的重新训练或架构修改。这种即插即用的设计和两阶段训练策略，使得开发者可以方便地将精确的视觉定位功能集成到各类VLM应用中，例如机器人、详细图像分析和人机交互，同时保留模型原有的高级推理能力，显著降低了开发具备高级感知能力的VLM的门槛和成本。

## 31.[Boolean Satisfiability via Imitation Learning](https://arxiv.org/pdf/2509.25411)
summary:**核心关键词**： **布尔可满足性 (SAT)**, **模仿学习**, **CDCL求解器**, **分支策略**  **一句话核心总结**： 该研究针对布尔可满足性（SAT）问题，提出了一种名为ImitSAT的、基于模仿学习的CDCL求解器分支策略，它通过学习从完整运行中提取的无冲突专家决策序列（KeyTrace），直接减少作为求解时间主要瓶颈的传播操作，从而显著提升求解效率。  **主要研究问题或目标**： 本研究旨在解决传统CDCL求解器中分支策略效率低下的问题，通过机器学习方法直接、稳定地生成高质量的分支决策，以减少作为求解时间主要贡献者的单元传播（unit propagation）数量。  **关键方法论**： 该方法的核心是“专家关键轨迹（KeyTrace）”提取与模仿学习。首先，通过分析求解器的完整运行过程，剔除所有因冲突而回溯的无效决策，从而构建出一个仅包含最终有效决策的紧凑序列作为专家演示。然后，将分支过程建模为一个自回归序列预测任务，使用基于Transformer的架构学习在给定当前问题和决策前缀的条件下，预测下一个最优决策。  **主要成果**： 实验结果表明，ImitSAT在多个基准测试中优于现有的学习方法，有效减少了传播次数和端到端求解时间。例如，在包含50个变量的随机3-SAT测试集上，ImitSAT将中位相对传播数（MRPP）降低至0.74，且模型表现出强大的泛化能力，能有效迁移到其他未经训练的结构化SAT问题族。  **对AI从业者的主要启示**： 该研究为AI从业者展示了一种将模仿学习应用于组合优化问题的有效范式，其核心启示在于：通过设计精巧的“专家轨迹”提取方法，可以将传统算法复杂的搜索过程提炼为干净的监督信号，从而能够直接利用强大的序列模型（如Transformer）来指导和加速经典求解器，这一思路为在其他规划、验证等领域融合深度学习与传统算法提供了高效的实现路径。

## 32.[Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures](https://arxiv.org/pdf/2509.25045)
summary:**核心关键词**：LLM可解释性，向量符号架构，超维探针，信息解码  **一句话核心摘要**：本文提出一种名为“超维探针”的新范式，通过结合符号表征和神经探测思想，利用向量符号架构（VSA）将大语言模型（LLM）的内部残差流投影到可解释的概念空间，从而实现对LLM内部表征更具信息量和结构化的解码。  **主要研究问题或目标**：该研究旨在解决大语言模型（LLM）内部表征不透明的问题，特别是为了克服现有可解释性方法（如直接对数归因DLA和稀疏自动编码器SAE）在依赖模型输出词表或特征命名模糊等方面的局限性，从而开发一种能从模型内部向量空间中解码出人类可读、结构化概念的新范式。  **关键方法**：该方法首先通过k-均值聚类和和池化对LLM多个中间层的残差流嵌入进行压缩；其次，训练一个浅层监督式神经网络编码器，将压缩后的LLM嵌入映射到一个由向量符号架构（VSA）构成的、预先定义好的代理概念空间中；最后，在推理阶段，利用超向量代数的“解绑”（unbinding）操作，从编码器输出的VSA表征中查询并提取出具体的、可解释的原子概念。  **主要成果**：实验表明，该探针能可靠地从LLM的内部状态中提取目标概念，即便在LLM未能生成正确答案时依然有效。在类比推理任务中，该方法在所有测试模型上的平均`probing@1`精度达到83%，远高于模型自身平均31%的下一词元预测`precision@1`。  **对AI从业者的主要启示**：该研究为AI从业者提供了一种新颖且计算高效的LLM可解释性工具，它允许使用预定义的、结构化的概念框架直接查询模型的内部状态，而无需依赖模型词表或进行复杂的特征命名。这可以直接应用于调试模型故障，通过分析模型表征中编码了哪些概念来理解其推理过程，尤其能够诊断出模型是将正确概念编码在内部（表征成功）但未能生成（生成失败）的情况，为模型开发和故障分析提供了更深层次的洞察。

## 33.[An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications](https://arxiv.org/pdf/2509.19185)
summary:**核心关键词** AI智能体, 测试实践, 实证研究, 非确定性  **一句话核心摘要** 本研究通过对39个开源智能体框架和439个智能体应用进行大规模实证分析，首次揭示了AI智能体生态系统中的测试实践，发现测试精力被颠倒地投入到确定性组件上（超过70%），而关键的提示词（Trigger）组件则被严重忽视（仅约1%的测试覆盖）。  **主要研究问题或目标** 本研究旨在系统性地实证调查开源社区的开发者如何测试基于基础模型（FM）的AI智能体，以应对其固有的非确定性和不可复现性所带来的内部正确性验证挑战。  **关键方法论** 该研究通过对39个开源AI智能体框架和439个智能体应用的代码库进行大规模分析，首先识别出10种不同的测试模式，然后将这些模式映射到智能体的经典架构组件上，从而系统地量化测试精力的分布。  **主要成果** 研究发现存在显著的“测试精力倒置”现象：超过70%的测试工作集中在工具和工作流等确定性组件上，而基于基础模型的核心“规划体”（Plan Body）组件的测试占比不足5%。最关键的发现是，作为智能体行为起点的“触发器”（Trigger）即提示词组件，在所有测试中仅出现约1%，构成了一个严重的测试盲点。  **对AI从业者的主要启示** 对于AI工程师而言，本研究最直接的启示是必须建立系统的提示词回归测试机制，以应对底层基础模型更新导致的“静默失败”和性能衰退风险。开发者应将测试重点从重复验证框架的确定性功能转移到验证应用层独特的业务逻辑、工具集成和提示词的健壮性上，从而构建更可靠的AI智能体。

## 34.[TGPO: Temporal Grounded Policy Optimization for Signal Temporal Logic Tasks](https://arxiv.org/pdf/2510.00225)
summary:**核心关键词**：信号时序逻辑, 分层强化学习, 时序接地, 策略优化  **一句话核心总结**：为解决通用信号时序逻辑（STL）任务，该研究提出了一种名为TGPO的分层强化学习框架，它通过将STL分解为时序子目标和约束，并利用评论家引导的时间分配和密集奖励信号进行策略学习，从而显著提升了在复杂机器人控制任务中的性能。  **主要研究问题或目标**：旨在解决标准强化学习算法因信号时序逻辑（STL）的非马尔可夫性和固有稀疏奖励问题，而难以有效求解通用、长时域、复杂动态系统控制任务的难题。  **关键方法**：该研究提出的TGPO框架首先将复杂的STL公式分解为一系列带时间变量的“可达性”子目标和“不变性”约束；然后，采用一个双层结构，其中高层组件负责为子目标分配具体时间值，而低层时序条件策略则利用分解后产生的密集阶段性奖励信号来学习实现这些有序子目标；为高效搜索时间分配，该方法利用学习到的评论家（critic）网络通过梅特罗波利斯-黑斯廷斯采样指导高层搜索，使其专注于时序上可行的方案。  **主要成果**：在五个从低维导航到高维机器人运动的实验环境中，TGPO框架的性能显著优于现有基线方法，特别是在高维和长时域任务中，其任务成功率相比最佳基线平均提升了31.6%。  **对AI从业者的主要启示**：该研究为AI从业者提供了一种将复杂的、非马尔可夫性的稀疏奖励控制问题（如由时序逻辑定义的机器人任务）转化为可解的、具有密集奖励信号的分层强化学习问题的新范式，其通过STL分解和时序接地，有效解决了长时域任务规划与底层控制相结合的难题，为开发能处理复杂指令的自主系统提供了具体技术路径。

## 35.[Aligning Visual Foundation Encoders to Tokenizers for Diffusion Models](https://arxiv.org/pdf/2509.25162)
summary:**核心关键词**：视觉编码器对齐, 扩散模型, 图像分词器, 语义潜空间  **一句话核心总结**：该研究提出一种通过三阶段对齐策略将预训练视觉编码器调整为扩散模型分词器的方法，旨在利用基础编码器丰富的语义结构建立一个更优的潜空间，以加速模型收敛并提升图像生成质量。  **主要研究问题或目标**：本研究旨在解决传统从零开始训练的VAE（变分自编码器）分词器主要关注低级像素细节，导致其潜空间语义结构较弱，进而影响潜在扩散模型训练效率和生成质量的问题。  **关键方法论**：该方法采用一个三阶段对齐策略：首先，冻结预训练的视觉编码器，仅训练一个轻量级适配器和解码器以建立初始的语义潜空间；其次，引入语义保持损失，联合优化所有组件，使编码器在学习感知细节的同时保留高层语义；最后，固定编码器和适配器，单独微调解码器以提升最终的重建质量。  **主要成果**：实验表明，该方法设计的图像分词器显著提升了扩散模型的性能。在ImageNet 256×256数据集上，模型仅用64个周期（epochs）的训练就达到了1.90的生成FID（gFID），加速了收敛。在LAION数据集上的大规模文生图任务中，使用该分词器的20亿参数模型在同等训练步数下持续优于使用FLUX VAE的模型。  **对AI从业者的主要启示**：该研究为AI从业者提供了一种高效构建生成模型潜空间的新范式：与其从零开始训练分词器，不如通过对齐强大的预训练视觉基础模型来构建，以继承其丰富的语义表征。这一方法最显著的价值在于能够大幅加速扩散模型的收敛速度，直接帮助开发者降低大规模生成模型训练的计算成本和时间，加快研发迭代周期。

